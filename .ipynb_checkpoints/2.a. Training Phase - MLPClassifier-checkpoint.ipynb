{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad5b23e-285d-42f2-ba98-06e46c7b56b2",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction Kaggle Competition\n",
    "## October 28, 2024\n",
    "DICHOSO, Aaron Gabrielle C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4338a-494e-4783-a4d1-e99f9cef1cd7",
   "metadata": {},
   "source": [
    "This Notebook is part of a series of notebooks that will contain documentation and methods used for training a Multilayer Perceptron (MLP) used in the <a href=\"https://www.kaggle.com/competitions/playground-series-s4e10/\"><b>2024 Loan Approval Prediction Kaggle Playground Series</b></a>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525782c9-87a3-42d4-b71e-807c03b483c0",
   "metadata": {},
   "source": [
    "For this notebook, I will focus on the methods that I utilized for model training and hyperparameter training.\n",
    "\n",
    "To view the data cleaning itself, feel free to visit the following notebook: \n",
    "\n",
    "<ul>\n",
    "    <li>1. Data Exploration, Cleaning, and Transformations</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64118b2-37b7-4d42-b31c-b7640af62602",
   "metadata": {},
   "source": [
    "I chose to use MLPs due to their ability to handle high-dimensional and non-linearly separable data. These characteristics are due to its usage of non-linear activation functions for each node in the MLP. Additionally, how I preprocessed the data made it so that all the features were relatively the same scale. My hope in using an MLP is that the model will be able to learn the importance of each feature through its hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3e475-bc3f-4e05-9d20-29e3f587adbe",
   "metadata": {},
   "source": [
    "# 1. Import Cleaned Datasets\n",
    "\n",
    "We first import the cleaned datasets from the previous notebook first. In this repository, the cleaned datasets are saved in the <b>./output</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b42ef0-d8b5-4504-b913-bc514293d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61015a-1245-4d62-9e1e-580667a80eaa",
   "metadata": {},
   "source": [
    "As observed, there are two kinds of training datasets used, the train set without oversampling, and the dataset that underwent ADASYN oversampling. I wish to test the performance of the model comparing these two methods during the hyperparameter tuning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b701978-d1eb-49fe-90be-05b1af92ee93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.569797</td>\n",
       "      <td>-1.081318</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.390423</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>1.719062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>-0.973222</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240977</td>\n",
       "      <td>-1.508084</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.185873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407079</td>\n",
       "      <td>0.435878</td>\n",
       "      <td>1.579649</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.277050</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-1.318902</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0    1.569797      -1.081318          -1.896898  -0.578305       0.390423   \n",
       "1   -0.921741      -0.052550           0.601227  -0.937769       0.896212   \n",
       "2    0.240977      -1.508084           0.923860  -0.578305      -0.470628   \n",
       "3    0.407079       0.435878           1.579649   0.500086       0.277050   \n",
       "4   -0.921741       0.098465          -0.486519  -0.578305      -1.318902   \n",
       "\n",
       "   loan_percent_income  cb_person_default_on_file  cb_person_cred_hist_length  \\\n",
       "0             0.117380                          0                    1.719062   \n",
       "1            -0.973222                          0                   -1.364513   \n",
       "2             0.553620                          0                    1.185873   \n",
       "3             0.117380                          0                    0.087481   \n",
       "4            -0.646041                          0                   -0.721995   \n",
       "\n",
       "   loan_status  PERSON_HOME_OWNERSHIP_MORTGAGE  ...  LOAN_GRADE_B  \\\n",
       "0            0                               0  ...             1   \n",
       "1            0                               0  ...             0   \n",
       "2            0                               0  ...             0   \n",
       "3            0                               0  ...             1   \n",
       "4            0                               0  ...             0   \n",
       "\n",
       "   LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0             0             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                 1                                    0   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                0                                   0  \n",
       "1                                0                                   1  \n",
       "2                                1                                   0  \n",
       "3                                1                                   0  \n",
       "4                                0                                   1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import Training Dataset\n",
    "loans_train_df = pd.read_csv('./outputs/cleaned_loans_train.csv')\n",
    "loans_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e1230d-9cb0-49c9-a2da-ef18bcc8c032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.569797</td>\n",
       "      <td>-1.081318</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.390423</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>1.719062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>-0.973222</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240977</td>\n",
       "      <td>-1.508084</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.185873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407079</td>\n",
       "      <td>0.435878</td>\n",
       "      <td>1.579649</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.277050</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-1.318902</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0    1.569797      -1.081318          -1.896898  -0.578305       0.390423   \n",
       "1   -0.921741      -0.052550           0.601227  -0.937769       0.896212   \n",
       "2    0.240977      -1.508084           0.923860  -0.578305      -0.470628   \n",
       "3    0.407079       0.435878           1.579649   0.500086       0.277050   \n",
       "4   -0.921741       0.098465          -0.486519  -0.578305      -1.318902   \n",
       "\n",
       "   loan_percent_income  cb_person_default_on_file  cb_person_cred_hist_length  \\\n",
       "0             0.117380                          0                    1.719062   \n",
       "1            -0.973222                          0                   -1.364513   \n",
       "2             0.553620                          0                    1.185873   \n",
       "3             0.117380                          0                    0.087481   \n",
       "4            -0.646041                          0                   -0.721995   \n",
       "\n",
       "   PERSON_HOME_OWNERSHIP_MORTGAGE  PERSON_HOME_OWNERSHIP_OTHER  ...  \\\n",
       "0                               0                            0  ...   \n",
       "1                               0                            0  ...   \n",
       "2                               0                            0  ...   \n",
       "3                               0                            0  ...   \n",
       "4                               0                            0  ...   \n",
       "\n",
       "   LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0             0             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                 1                                    0   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \\\n",
       "0                                0                                   0   \n",
       "1                                0                                   1   \n",
       "2                                1                                   0   \n",
       "3                                1                                   0   \n",
       "4                                0                                   1   \n",
       "\n",
       "   loan_status  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_train_ada_df = pd.read_csv('./outputs/cleaned_loans_train_ada.csv')\n",
    "loans_train_ada_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862d9cb-039b-48bd-bf65-63d18b91d1f9",
   "metadata": {},
   "source": [
    "# 2. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce364233-0253-4d6a-95c3-3af20cb58eec",
   "metadata": {},
   "source": [
    "The MLP has several hyperparameters that should be tuned to maximize its performance.\n",
    "\n",
    "In this notebook, I will focus on tuning the following <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html\">hyperparameters of the MLP</a> for their explainability, namely:\n",
    "\n",
    "<ol>\n",
    "    <li><b>activation:</b> the activation function used for the hidden layers of the MLP</li>\n",
    "    <li><b>tol:</b> the learning rate tolerance of the MLP, which is used to determine model convergence</li>\n",
    "    <li><b>beta_1:</b> The exponential decay rate for the first moment estimate of the adam classifier</li>\n",
    "    <li><b>beta_2:</b> The exponential decay rate for the second moment estimate of the adam classifier</li>\n",
    "    <li><b>epsilon:</b> The value for numerical stability for adam</li>\n",
    "    <li><b>oversampling_method:</b> The type of oversampling done in the dataset used.</li>\n",
    "</ol>\n",
    "\n",
    "Additionally, I will not be tuning the shape or size of the hidden layers due to its high resource demand for my system. I will be using the default parameters given by scikit-learn for the MLP's hidden layers for this reason.\n",
    "\n",
    "The range of follows I chose for these hyperparameters are as follows:\n",
    "<ol>\n",
    "    <li><b>activation:</b> \n",
    "        <ol>\n",
    "            <li>the identity function (f(x) = x) </li>\n",
    "            <li>the rectified linear unit (relu) function (f(x) = max(0,x))</li>\n",
    "            <li>the logistic sigmoid function (f(x) = 1 / (1 + exp(-x)))</li>\n",
    "            <li>the hyperbolic tangent function (f(x) = tanh(x))</li>\n",
    "        </ol>\n",
    "    <li><b>tol:</b> [0.0001, 0.1]</li>\n",
    "    <li><b>beta_1:</b> [0, 0.9999]</li>\n",
    "    <li><b>beta_2:</b> [0, 0.9999]</li>\n",
    "    <li><b>epsilon:</b> [1e-10, 0.1]</li>\n",
    "    <li><b>oversampling_method:</b> Either using the ADASYN oversampled data set or the unbalanced labels dataset.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a1e085-ab0e-45a9-b731-8b39629b8499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>tol</th>\n",
       "      <th>beta_1</th>\n",
       "      <th>beta_2</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>oversampling_method</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [activation, tol, beta_1, beta_2, epsilon, oversampling_method, roc_auc]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyper_tuning = pd.DataFrame(columns=['activation', 'tol', 'beta_1', 'beta_2', 'epsilon', 'oversampling_method', 'roc_auc'])\n",
    "df_hyper_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afcacfd-a667-447e-81e1-9cdb8eba50e1",
   "metadata": {},
   "source": [
    "For the specific method of hyperparameter tuning, I chose to perform bayesian optimization, which is a hyperparameter tuning method that involves observing the past iterations of the tuning process to influence the configurations to test later on. I chose this method over GridSearch because of the numerous possible configurations that I would need to search through not being a feasible method for my system. Additionally, I chose it over RandomSearch because bayesian optimization would be able to utilize my system resources more effectively by searching in areas with higher probabilities of giving me high performances as opposed to randomly testing configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8b935-c880-4e6d-b448-b75d2d57550a",
   "metadata": {},
   "source": [
    "To perform bayesian optimization, I utilized the <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html\">gp_minimize()</a> function provided by the scikit-optimize library.\n",
    "\n",
    "Following the instructions found in the documentations, I first initialized the search space to be used in the optimization process. This involved creating an array that pertains to the hyperparameters to tune, the data type of the hyperparameters, and the range of possible values to test in the hyperparameter tuning process.\n",
    "\n",
    "Afterwards, I created the objective function that the gp_minimize() function will execute. The objective function will use the search space defined earlier and test different values for the hyperparameters. It will then return the negative value of the Area Under the ROC (AUC) obtained from 3-fold cross validation, as this value will be minimized by the gp_minimize() function. Invalid configurations during hyperparameter tuning process will be given a positive value, allowing the bayesian optimization process to avoid such configurations.\n",
    "\n",
    "I also limit the number of calls performed by the function due to my limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23925d52-e297-48f0-82ca-2f510b643f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.016496541669711085\n",
      "Beta 1: 0.83686881851584\n",
      "Beta 2: 0.6243229026863576\n",
      "Epsilon: 0.0643763560005888\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9024631800084547\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.07188842519043259\n",
      "Beta 1: 0.25239852068545804\n",
      "Beta 2: 0.06331496979783176\n",
      "Epsilon: 0.0827951596286516\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8451398007840879\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.058689947274921564\n",
      "Beta 1: 0.30429335879779357\n",
      "Beta 2: 0.050713741188076936\n",
      "Epsilon: 0.009688964315612373\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8975646951036431\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.08486644361963162\n",
      "Beta 1: 0.5649601635734669\n",
      "Beta 2: 0.6937460784681164\n",
      "Epsilon: 0.07780812750903812\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8065784961475219\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.09751733893305348\n",
      "Beta 1: 0.2303872406234658\n",
      "Beta 2: 0.5838965982413987\n",
      "Epsilon: 0.041593962401649996\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9018987925446763\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.07617323657913525\n",
      "Beta 1: 0.37209594113587824\n",
      "Beta 2: 0.3619486541064976\n",
      "Epsilon: 0.08416547351629484\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8966021423214053\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.09407366588137311\n",
      "Beta 1: 0.9690493107790962\n",
      "Beta 2: 0.36303718913486577\n",
      "Epsilon: 0.05258379881016876\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8995937086236486\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.02543292622295672\n",
      "Beta 1: 0.28217254656753643\n",
      "Beta 2: 0.1905923559738276\n",
      "Epsilon: 0.061132040647492995\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9039180246932323\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.05389789837591881\n",
      "Beta 1: 0.7068410287212618\n",
      "Beta 2: 0.8209353890906996\n",
      "Epsilon: 0.04421675720063663\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8723103742187407\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.08422300104632201\n",
      "Beta 1: 0.023888470817542334\n",
      "Beta 2: 0.8105425380090512\n",
      "Epsilon: 0.0033429842050095595\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9024214447370715\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.042552465295848094\n",
      "Beta 1: 0.7699808789930831\n",
      "Beta 2: 0.9621417688992464\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9188695307974065\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9062092808527792\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.00985276879430457\n",
      "Beta 1: 0.6977682491057123\n",
      "Beta 2: 0.8736469268920373\n",
      "Epsilon: 1.9726330426164204e-05\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8953972245424131\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.06211231852079711\n",
      "Beta 1: 0.1532077640203569\n",
      "Beta 2: 0.19027196936797847\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9035962169931113\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.917532171740627\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.1996847769888372\n",
      "Beta 2: 0.7390073382030646\n",
      "Epsilon: 0.02284897436625481\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9271317794773674\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.02590943341910726\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8402881537335666\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8173919863661873\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.4267463997498231\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.01601635407974281\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9009823214545589\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.08913276065361019\n",
      "Beta 1: 0.9993114596691666\n",
      "Beta 2: 0.7224796793602797\n",
      "Epsilon: 0.09146605608333426\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8910745586532721\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.08764702246942277\n",
      "Beta 1: 0.07113977433756913\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.0668677277668728\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8881780675298888\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9270955880667762\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.15566671762896758\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9289279909982223\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.008622871151241131\n",
      "Beta 1: 0.21316382756117866\n",
      "Beta 2: 0.6236621537514756\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9118164912628144\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9295684023235907\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.005892878650048654\n",
      "Beta 1: 0.02412971389677896\n",
      "Beta 2: 0.8753771688768578\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9000135749846506\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.7982762796606485\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9289600034055906\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.059619039369720774\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8825618020632425\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9260882818303323\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.29610416037005033\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9278665477044928\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.030663984219471647\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9100212180146302\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.5954146726084351\n",
      "Beta 2: 0.8660174468673751\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.933098265450662\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.9275038236267017\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.00012311156366385894\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9322555933695185\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.30993297732485975\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9319013360653886\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.8500452746259589\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9321899556535405\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.05687258514855831\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.930593651449367\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.18711018917457112\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9300837320970171\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.030855148470857734\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9233786187470024\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.2951603306487375\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.93028661421206\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.8866921956456646\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1.0000001110223023e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9318768960981991\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.9245296002923471\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8828816944092496\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9134738876810988\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.917907417380963\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['relu', 0.0001, 0.9999, 0.9999, 1e-10, 'ada'] before, using random point ['logistic', 0.049497968360230196, 0.285741592739779, 0.49973194124452847, 0.07091366665840661, 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.049497968360230196\n",
      "Beta 1: 0.285741592739779\n",
      "Beta 2: 0.49973194124452847\n",
      "Epsilon: 0.07091366665840661\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.805971388286101\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.06285582725087735\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9265416985457074\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['relu', 0.0001, 0.9999, 0.9999, 1e-10, 'ada'] before, using random point ['logistic', 0.024931787241026116, 0.23835818181291782, 0.9988835719491237, 0.062215712669068585, 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.024931787241026116\n",
      "Beta 1: 0.23835818181291782\n",
      "Beta 2: 0.9988835719491237\n",
      "Epsilon: 0.062215712669068585\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8097573475838056\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.10805971169590194\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9012497376757782\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.2370082136662886\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.013195810279745106\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9276822102917807\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.42269310422491996\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.012820548227350292\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9258833374881613\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.8420323167823088\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8815848797294222\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.45637100115181783\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.012607506398842176\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9273185564203336\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.23508587097764533\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.02463160521958973\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9017561836687076\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.03760154915339851\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.01303028772650066\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9275257592962225\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.060041253822460984\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.013245593298161284\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9170043434985865\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9650399101730573\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9337280814584323\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.36749550495076416\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9316330961581203\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.5512321788510759\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9328025661165391\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['relu', 0.0001, 0.9999, 0.9999, 1e-10, 'ada'] before, using random point ['tanh', 0.035185055582388595, 0.2986592493566309, 0.6850367320899565, 0.07172452477466389, 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.035185055582388595\n",
      "Beta 1: 0.2986592493566309\n",
      "Beta 2: 0.6850367320899565\n",
      "Epsilon: 0.07172452477466389\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8483532713601029\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.694458518028561\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9307152202129779\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.6732914682888953\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9309694620892559\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.6451557870336069\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9318542689964855\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9836126240967187\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.933023823114381\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9899181240954202\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9327305682560176\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['relu', 0.0001, 0.9999, 0.9999, 1e-10, 'ada'] before, using random point ['logistic', 0.04577552951391579, 0.37580226197446515, 0.7589280346682357, 0.019065101988103766, 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.04577552951391579\n",
      "Beta 1: 0.37580226197446515\n",
      "Beta 2: 0.7589280346682357\n",
      "Epsilon: 0.019065101988103766\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8308208064964265\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.7978260285734755\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.663329260068367\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9033197458181726\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8979024312379127\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.04459538221928184\n",
      "Beta 1: 0.46847605110769813\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5559741939718994\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.09680195221800815\n",
      "Beta 1: 0.17964172929047517\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.011019207762884934\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9002513732499375\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.015125497557427427\n",
      "Beta 2: 0.007834780464730937\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.913290157739613\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.35473672932580796\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.037568309894604385\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9255004681182295\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.015367727476517828\n",
      "Beta 1: 0.5868748310602345\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9188818062544316\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.18737105725171482\n",
      "Beta 2: 0.013832077226734624\n",
      "Epsilon: 0.03531655068886721\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.901966816313974\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.614873533425998\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8844632785141376\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.005803066611739827\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.00516253261299683\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.899263867371538\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.024881322318551058\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.015807945393220335\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8462173519219007\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: identity\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.21127647131662586\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.07199782590439283\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9018228475255651\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.05092193003477266\n",
      "Beta 1: 0.6091113231328343\n",
      "Beta 2: 0.9221917657488232\n",
      "Epsilon: 0.02099814849898803\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8910766805411631\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.04750574496552554\n",
      "Beta 1: 0.7629287048115185\n",
      "Beta 2: 0.09743230010276759\n",
      "Epsilon: 0.08161649235157173\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8992803067253615\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.09324040505942868\n",
      "Beta 1: 0.12783475559595123\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.04805273744153546\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9056336670031154\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.06636283719632373\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.07374187603951203\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8622617774389708\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0776586150713152\n",
      "Beta 1: 0.4155428101880503\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.021176159698890223\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8867945874503604\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.029185576847805067\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.041361780773309265\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8942311270129526\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.06503828078884105\n",
      "Beta 1: 0.9835822999134618\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.026257904416329095\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9039840711369075\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.5589671431681726\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9171629350952154\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.0808206724321611\n",
      "Beta 1: 0.5237460541463916\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.00356713904035321\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8918245723754437\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.03995634682193667\n",
      "Beta 1: 0.07003550346380945\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.02406852149061989\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9123954351790831\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.09990458694325384\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8548526689153814\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: logistic\n",
      "Tolerance: 0.05979702045466868\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.6904285041164391\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.4661171326093605\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9299669978612014\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.41146465845050373\n",
      "Beta 2: 0.6959704528661759\n",
      "Epsilon: 0.020443312847684043\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9228654718398409\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.17466327080542582\n",
      "Beta 2: 0.9999\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.9300812681759641\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.084609736879789\n",
      "Beta 1: 0.0\n",
      "Beta 2: 0.0\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.926008793253974\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.8872455869058004\n",
      "Beta 2: 0.7993197275032184\n",
      "Epsilon: 0.013002605025819272\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9273484357936814\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: tanh\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.3976954487071893\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.9181037407618508\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.5280115285024548\n",
      "Beta 2: 0.6154545723243792\n",
      "Epsilon: 1e-10\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.927469181148301\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.9999\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.1\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.906046560975141\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.0001\n",
      "Beta 1: 0.7499506843073827\n",
      "Beta 2: 0.0\n",
      "Epsilon: 0.037478595683078784\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9261838947449257\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Activation: relu\n",
      "Tolerance: 0.1\n",
      "Beta 1: 0.03207935994167879\n",
      "Beta 2: 0.49294782062744513\n",
      "Epsilon: 0.019164909806257912\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9157959403333694\n",
      "================\n",
      "Best parameters: ['relu', 0.0001, 0.9650399101730573, 0.9999, 1e-10, 'ada']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "# Define the search space\n",
    "search_space = [\n",
    "    Categorical(['identity', 'logistic', 'tanh', 'relu'], name='activation'),\n",
    "    Real(0.0001, 0.1, name='tol'),\n",
    "    Real(0, 0.9999, name='beta_1'),\n",
    "    Real(0, 0.9999, name='beta_2'),\n",
    "    Real(1e-10, 0.1, name='epsilon'),\n",
    "    Categorical(['none', 'ada'], name='oversampling_method')\n",
    "]\n",
    "\n",
    "# Define your objective function (e.g., maximizing accuracy)\n",
    "@use_named_args(search_space)\n",
    "def objective_function(activation, tol, beta_1, beta_2, epsilon, oversampling_method):\n",
    "    print(\"================\")\n",
    "    print(\"Configuration:\")\n",
    "    print(\"Activation:\", activation)\n",
    "    print(\"Tolerance:\", tol)\n",
    "    print(\"Beta 1:\", beta_1)\n",
    "    print(\"Beta 2:\", beta_2)\n",
    "    print(\"Epsilon:\", epsilon)\n",
    "    print(\"Oversampling Method:\", oversampling_method)\n",
    "    print(\"----------------\")\n",
    "    try:\n",
    "        if oversampling_method == 'none':\n",
    "            X = loans_train_df.loc[:, loans_train_df.columns != \"loan_status\"]\n",
    "            y = loans_train_df[\"loan_status\"]\n",
    "        elif oversampling_method == 'ada':\n",
    "            X = loans_train_ada_df.loc[:, loans_train_ada_df.columns != \"loan_status\"]\n",
    "            y = loans_train_ada_df[\"loan_status\"]\n",
    "\n",
    "        model = MLPClassifier(activation=activation, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, early_stopping=True, tol=tol)\n",
    "        roc_auc = cross_val_score(model, X, y, cv=3, scoring='roc_auc').mean()\n",
    "\n",
    "        print(\"Results:\", -roc_auc)\n",
    "        df_hyper_tuning.loc[len(df_hyper_tuning.index)] = [activation, tol, beta_1, beta_2, epsilon, oversampling_method, roc_auc] \n",
    "        print(\"================\")\n",
    "        return -roc_auc\n",
    "    except:\n",
    "        print(\"Invalid Config\")\n",
    "        return 100000\n",
    "        \n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "res = gp_minimize(objective_function, search_space, n_calls=100)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e92b4-1928-4cfe-ad12-2c09b2c5bf37",
   "metadata": {},
   "source": [
    "These tuning results are saved in a dataframe. The hyperparameter tuning results for all models can be viewed in the <b>./hyper_tuning</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "427c5a7e-4338-439c-baf9-48c49cd98d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>tol</th>\n",
       "      <th>beta_1</th>\n",
       "      <th>beta_2</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>oversampling_method</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.965040</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.933728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.595415</td>\n",
       "      <td>0.866017</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.933098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.983613</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.933024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.551232</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.932803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.989918</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.932731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.084866</td>\n",
       "      <td>0.564960</td>\n",
       "      <td>0.693746</td>\n",
       "      <td>7.780813e-02</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.806578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.049498</td>\n",
       "      <td>0.285742</td>\n",
       "      <td>0.499732</td>\n",
       "      <td>7.091367e-02</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.805971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>none</td>\n",
       "      <td>0.690429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>identity</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.797826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.663329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>identity</td>\n",
       "      <td>0.044595</td>\n",
       "      <td>0.468476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>none</td>\n",
       "      <td>0.555974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation       tol    beta_1    beta_2       epsilon oversampling_method  \\\n",
       "55       relu  0.000100  0.965040  0.999900  1.000000e-10                 ada   \n",
       "31       relu  0.000100  0.595415  0.866017  1.000000e-10                 ada   \n",
       "62       relu  0.000100  0.983613  0.999900  1.000000e-10                 ada   \n",
       "57       relu  0.000100  0.551232  0.999900  1.000000e-10                 ada   \n",
       "63       relu  0.000100  0.989918  0.999900  1.000000e-10                 ada   \n",
       "..        ...       ...       ...       ...           ...                 ...   \n",
       "3    logistic  0.084866  0.564960  0.693746  7.780813e-02                 ada   \n",
       "44   logistic  0.049498  0.285742  0.499732  7.091367e-02                 ada   \n",
       "89   logistic  0.059797  0.999900  0.999900  1.000000e-01                none   \n",
       "65   identity  0.100000  0.797826  0.000000  1.000000e-10                 ada   \n",
       "68   identity  0.044595  0.468476  0.000000  1.000000e-10                none   \n",
       "\n",
       "     roc_auc  \n",
       "55  0.933728  \n",
       "31  0.933098  \n",
       "62  0.933024  \n",
       "57  0.932803  \n",
       "63  0.932731  \n",
       "..       ...  \n",
       "3   0.806578  \n",
       "44  0.805971  \n",
       "89  0.690429  \n",
       "65  0.663329  \n",
       "68  0.555974  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyper_tuning.sort_values(by=['roc_auc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62705767-e03c-4151-90d1-b48816a82017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_tuning.to_csv('hyper_tuning/mlp_hyper_tuning.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171ab65-8366-469c-98e4-adb03c036f87",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f9fbc-b97a-46a6-987e-183d29f2d38e",
   "metadata": {},
   "source": [
    "The results showed that the following configuration produced the best results:\n",
    "\n",
    "<ol>\n",
    "    <li><b>activation:</b> relu</li>\n",
    "    <li><b>tol:</b> 0.0001</li>\n",
    "    <li><b>beta_1:</b> 0.965040</li>\n",
    "    <li><b>beta_2:</b> 0.9999</li>\n",
    "    <li><b>epsilon:</b> 1e-10</li>\n",
    "    <li><b>oversampling_method: </b> ADASYN Oversampling</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb02a78-005e-4052-b288-fbbacff6b588",
   "metadata": {},
   "source": [
    "The relu activation function being present in the configuration that produced the best results is highly favorable, as it is significantly less expensive in terms of computational costs compared to the tanh and logistic sigmoid functions. \n",
    "\n",
    "Additionally, the utilization of the dataset that underwent ADASYN oversampling indicates that the oversampling method was useful for MLPs, allowing the model to better classify the two classes. This may be due to how ADASYN oversamples the dataset, creating samples for the minority class that are harder to differentiate from the majority class. Thus, during training, the MLP may have been able to better create the distinction between the two classes due to the synthetically generated edge cases.\n",
    "\n",
    "Finally, the tolerance, beta_1, beta_2, and epsilon hyperparameters all show how the MLP underwent training and eventual convergence. \n",
    "\n",
    "For epsilon, we can see that it chose the lowest possible value of 1e-10, which may be due to how it is only used for numerical stability by preventing the divison by 0 during parameter updating. Higher values may cause instability and improper adjustment of weights for the MLP.\n",
    "\n",
    "The tolerance as well was chosen to be at the lowest pssible value of 1e-4. This is expected as higher values of the tolerance may cause the MLP to preemptively stop with its training, thinking that it may have already converged even though it might still be able to learn more.\n",
    "\n",
    "For beta_1 and beta_2, these are hyperparameters that the adam optimizer uses for weight optimization. These values, when closer to 1, mean that the adam optimizer is less biased towards 0 during the start of the training process and that the correction process is much more smoother. This attribute is important as we do not want to overcorrect the weights of the MLP. Rather, it is more favorable to slowly update the weights to eventually reach a minimum for the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b7235-c8b4-44f8-9f2f-110eeb93d5fc",
   "metadata": {},
   "source": [
    "# 4. Exporting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cea395-526c-45cd-b504-baa1c11959a3",
   "metadata": {},
   "source": [
    "The model with the best configuration found during the hyperparameter tuning process is saved in the <b>./outputs</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d909bfcd-9e27-48d5-a878-b420df7e265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(activation=res.x[0],early_stopping=True, tol=res.x[1], beta_1=res.x[2], beta_2=res.x[3], epsilon=res.x[4],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ce9994-121f-469c-bf24-dba44eea86bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48024405\n",
      "Validation score: 0.799268\n",
      "Iteration 2, loss = 0.40816724\n",
      "Validation score: 0.822308\n",
      "Iteration 3, loss = 0.38038027\n",
      "Validation score: 0.830021\n",
      "Iteration 4, loss = 0.36580037\n",
      "Validation score: 0.839612\n",
      "Iteration 5, loss = 0.35647445\n",
      "Validation score: 0.840206\n",
      "Iteration 6, loss = 0.34961774\n",
      "Validation score: 0.839415\n",
      "Iteration 7, loss = 0.34373210\n",
      "Validation score: 0.847325\n",
      "Iteration 8, loss = 0.33976193\n",
      "Validation score: 0.845743\n",
      "Iteration 9, loss = 0.33565637\n",
      "Validation score: 0.844062\n",
      "Iteration 10, loss = 0.33252108\n",
      "Validation score: 0.850193\n",
      "Iteration 11, loss = 0.32893781\n",
      "Validation score: 0.853159\n",
      "Iteration 12, loss = 0.32651435\n",
      "Validation score: 0.853555\n",
      "Iteration 13, loss = 0.32436091\n",
      "Validation score: 0.851775\n",
      "Iteration 14, loss = 0.32148650\n",
      "Validation score: 0.856818\n",
      "Iteration 15, loss = 0.32017166\n",
      "Validation score: 0.853950\n",
      "Iteration 16, loss = 0.31754698\n",
      "Validation score: 0.857510\n",
      "Iteration 17, loss = 0.31612302\n",
      "Validation score: 0.857807\n",
      "Iteration 18, loss = 0.31415863\n",
      "Validation score: 0.858598\n",
      "Iteration 19, loss = 0.31254868\n",
      "Validation score: 0.858598\n",
      "Iteration 20, loss = 0.31146433\n",
      "Validation score: 0.858400\n",
      "Iteration 21, loss = 0.30964742\n",
      "Validation score: 0.858103\n",
      "Iteration 22, loss = 0.30851287\n",
      "Validation score: 0.856719\n",
      "Iteration 23, loss = 0.30720788\n",
      "Validation score: 0.858499\n",
      "Iteration 24, loss = 0.30521443\n",
      "Validation score: 0.861762\n",
      "Iteration 25, loss = 0.30475471\n",
      "Validation score: 0.863641\n",
      "Iteration 26, loss = 0.30314509\n",
      "Validation score: 0.862652\n",
      "Iteration 27, loss = 0.30264737\n",
      "Validation score: 0.863740\n",
      "Iteration 28, loss = 0.30181142\n",
      "Validation score: 0.867794\n",
      "Iteration 29, loss = 0.30017505\n",
      "Validation score: 0.862949\n",
      "Iteration 30, loss = 0.30028726\n",
      "Validation score: 0.861169\n",
      "Iteration 31, loss = 0.29833587\n",
      "Validation score: 0.865915\n",
      "Iteration 32, loss = 0.29752731\n",
      "Validation score: 0.868189\n",
      "Iteration 33, loss = 0.29648220\n",
      "Validation score: 0.865322\n",
      "Iteration 34, loss = 0.29638194\n",
      "Validation score: 0.862454\n",
      "Iteration 35, loss = 0.29481847\n",
      "Validation score: 0.867893\n",
      "Iteration 36, loss = 0.29449544\n",
      "Validation score: 0.861663\n",
      "Iteration 37, loss = 0.29330513\n",
      "Validation score: 0.869376\n",
      "Iteration 38, loss = 0.29271953\n",
      "Validation score: 0.867300\n",
      "Iteration 39, loss = 0.29136695\n",
      "Validation score: 0.866706\n",
      "Iteration 40, loss = 0.29214692\n",
      "Validation score: 0.868585\n",
      "Iteration 41, loss = 0.29025578\n",
      "Validation score: 0.866607\n",
      "Iteration 42, loss = 0.29033913\n",
      "Validation score: 0.869673\n",
      "Iteration 43, loss = 0.28911991\n",
      "Validation score: 0.867300\n",
      "Iteration 44, loss = 0.28833886\n",
      "Validation score: 0.868684\n",
      "Iteration 45, loss = 0.28821245\n",
      "Validation score: 0.872639\n",
      "Iteration 46, loss = 0.28717534\n",
      "Validation score: 0.867893\n",
      "Iteration 47, loss = 0.28682086\n",
      "Validation score: 0.868288\n",
      "Iteration 48, loss = 0.28615394\n",
      "Validation score: 0.870266\n",
      "Iteration 49, loss = 0.28593777\n",
      "Validation score: 0.869376\n",
      "Iteration 50, loss = 0.28454905\n",
      "Validation score: 0.868684\n",
      "Iteration 51, loss = 0.28480611\n",
      "Validation score: 0.869969\n",
      "Iteration 52, loss = 0.28388773\n",
      "Validation score: 0.869178\n",
      "Iteration 53, loss = 0.28389511\n",
      "Validation score: 0.868684\n",
      "Iteration 54, loss = 0.28368423\n",
      "Validation score: 0.862257\n",
      "Iteration 55, loss = 0.28282469\n",
      "Validation score: 0.866311\n",
      "Iteration 56, loss = 0.28126480\n",
      "Validation score: 0.871551\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51868817\n",
      "Validation score: 0.783892\n",
      "Iteration 2, loss = 0.43999428\n",
      "Validation score: 0.809849\n",
      "Iteration 3, loss = 0.40446419\n",
      "Validation score: 0.822605\n",
      "Iteration 4, loss = 0.38396461\n",
      "Validation score: 0.832542\n",
      "Iteration 5, loss = 0.37117737\n",
      "Validation score: 0.838475\n",
      "Iteration 6, loss = 0.36234503\n",
      "Validation score: 0.835805\n",
      "Iteration 7, loss = 0.35558136\n",
      "Validation score: 0.841590\n",
      "Iteration 8, loss = 0.34935542\n",
      "Validation score: 0.844408\n",
      "Iteration 9, loss = 0.34449899\n",
      "Validation score: 0.843222\n",
      "Iteration 10, loss = 0.34082307\n",
      "Validation score: 0.847078\n",
      "Iteration 11, loss = 0.33623447\n",
      "Validation score: 0.842183\n",
      "Iteration 12, loss = 0.33275393\n",
      "Validation score: 0.845595\n",
      "Iteration 13, loss = 0.33017995\n",
      "Validation score: 0.846781\n",
      "Iteration 14, loss = 0.32633731\n",
      "Validation score: 0.849006\n",
      "Iteration 15, loss = 0.32359452\n",
      "Validation score: 0.853604\n",
      "Iteration 16, loss = 0.32162994\n",
      "Validation score: 0.850934\n",
      "Iteration 17, loss = 0.31879251\n",
      "Validation score: 0.851973\n",
      "Iteration 18, loss = 0.31777979\n",
      "Validation score: 0.852566\n",
      "Iteration 19, loss = 0.31501022\n",
      "Validation score: 0.855681\n",
      "Iteration 20, loss = 0.31232724\n",
      "Validation score: 0.855977\n",
      "Iteration 21, loss = 0.31082017\n",
      "Validation score: 0.853308\n",
      "Iteration 22, loss = 0.30897872\n",
      "Validation score: 0.856867\n",
      "Iteration 23, loss = 0.30763622\n",
      "Validation score: 0.858351\n",
      "Iteration 24, loss = 0.30518005\n",
      "Validation score: 0.857906\n",
      "Iteration 25, loss = 0.30360650\n",
      "Validation score: 0.857016\n",
      "Iteration 26, loss = 0.30192589\n",
      "Validation score: 0.858202\n",
      "Iteration 27, loss = 0.30120111\n",
      "Validation score: 0.859686\n",
      "Iteration 28, loss = 0.29991951\n",
      "Validation score: 0.862059\n",
      "Iteration 29, loss = 0.29756222\n",
      "Validation score: 0.859834\n",
      "Iteration 30, loss = 0.29662140\n",
      "Validation score: 0.858647\n",
      "Iteration 31, loss = 0.29561793\n",
      "Validation score: 0.864284\n",
      "Iteration 32, loss = 0.29539174\n",
      "Validation score: 0.861762\n",
      "Iteration 33, loss = 0.29332907\n",
      "Validation score: 0.862355\n",
      "Iteration 34, loss = 0.29224583\n",
      "Validation score: 0.861614\n",
      "Iteration 35, loss = 0.29034039\n",
      "Validation score: 0.864284\n",
      "Iteration 36, loss = 0.29020203\n",
      "Validation score: 0.864432\n",
      "Iteration 37, loss = 0.28948790\n",
      "Validation score: 0.863097\n",
      "Iteration 38, loss = 0.28770403\n",
      "Validation score: 0.865025\n",
      "Iteration 39, loss = 0.28748006\n",
      "Validation score: 0.864877\n",
      "Iteration 40, loss = 0.28642110\n",
      "Validation score: 0.864580\n",
      "Iteration 41, loss = 0.28474499\n",
      "Validation score: 0.869327\n",
      "Iteration 42, loss = 0.28371900\n",
      "Validation score: 0.862652\n",
      "Iteration 43, loss = 0.28393462\n",
      "Validation score: 0.865619\n",
      "Iteration 44, loss = 0.28364894\n",
      "Validation score: 0.867102\n",
      "Iteration 45, loss = 0.28265599\n",
      "Validation score: 0.864729\n",
      "Iteration 46, loss = 0.28092023\n",
      "Validation score: 0.866212\n",
      "Iteration 47, loss = 0.28094530\n",
      "Validation score: 0.867250\n",
      "Iteration 48, loss = 0.27929183\n",
      "Validation score: 0.870217\n",
      "Iteration 49, loss = 0.27876855\n",
      "Validation score: 0.865767\n",
      "Iteration 50, loss = 0.27894140\n",
      "Validation score: 0.867695\n",
      "Iteration 51, loss = 0.27789529\n",
      "Validation score: 0.868585\n",
      "Iteration 52, loss = 0.27687070\n",
      "Validation score: 0.870365\n",
      "Iteration 53, loss = 0.27660843\n",
      "Validation score: 0.869475\n",
      "Iteration 54, loss = 0.27588810\n",
      "Validation score: 0.866508\n",
      "Iteration 55, loss = 0.27449086\n",
      "Validation score: 0.870068\n",
      "Iteration 56, loss = 0.27571330\n",
      "Validation score: 0.868882\n",
      "Iteration 57, loss = 0.27337025\n",
      "Validation score: 0.871848\n",
      "Iteration 58, loss = 0.27336511\n",
      "Validation score: 0.871700\n",
      "Iteration 59, loss = 0.27282526\n",
      "Validation score: 0.868585\n",
      "Iteration 60, loss = 0.27365532\n",
      "Validation score: 0.872293\n",
      "Iteration 61, loss = 0.27278862\n",
      "Validation score: 0.871403\n",
      "Iteration 62, loss = 0.27083351\n",
      "Validation score: 0.871551\n",
      "Iteration 63, loss = 0.27073961\n",
      "Validation score: 0.872738\n",
      "Iteration 64, loss = 0.27092271\n",
      "Validation score: 0.874370\n",
      "Iteration 65, loss = 0.27034658\n",
      "Validation score: 0.871106\n",
      "Iteration 66, loss = 0.27119656\n",
      "Validation score: 0.871996\n",
      "Iteration 67, loss = 0.26994224\n",
      "Validation score: 0.874666\n",
      "Iteration 68, loss = 0.26817997\n",
      "Validation score: 0.873480\n",
      "Iteration 69, loss = 0.26829189\n",
      "Validation score: 0.871106\n",
      "Iteration 70, loss = 0.26737570\n",
      "Validation score: 0.868288\n",
      "Iteration 71, loss = 0.26783468\n",
      "Validation score: 0.871848\n",
      "Iteration 72, loss = 0.26638215\n",
      "Validation score: 0.876891\n",
      "Iteration 73, loss = 0.26512183\n",
      "Validation score: 0.875111\n",
      "Iteration 74, loss = 0.26697030\n",
      "Validation score: 0.873183\n",
      "Iteration 75, loss = 0.26453966\n",
      "Validation score: 0.867843\n",
      "Iteration 76, loss = 0.26540860\n",
      "Validation score: 0.869475\n",
      "Iteration 77, loss = 0.26444576\n",
      "Validation score: 0.874963\n",
      "Iteration 78, loss = 0.26338861\n",
      "Validation score: 0.874370\n",
      "Iteration 79, loss = 0.26423668\n",
      "Validation score: 0.875408\n",
      "Iteration 80, loss = 0.26338007\n",
      "Validation score: 0.872738\n",
      "Iteration 81, loss = 0.26258861\n",
      "Validation score: 0.877188\n",
      "Iteration 82, loss = 0.26183793\n",
      "Validation score: 0.872886\n",
      "Iteration 83, loss = 0.26256105\n",
      "Validation score: 0.874666\n",
      "Iteration 84, loss = 0.26106621\n",
      "Validation score: 0.877336\n",
      "Iteration 85, loss = 0.26142124\n",
      "Validation score: 0.875853\n",
      "Iteration 86, loss = 0.25962951\n",
      "Validation score: 0.874815\n",
      "Iteration 87, loss = 0.26101916\n",
      "Validation score: 0.877781\n",
      "Iteration 88, loss = 0.25894771\n",
      "Validation score: 0.873776\n",
      "Iteration 89, loss = 0.25895546\n",
      "Validation score: 0.876446\n",
      "Iteration 90, loss = 0.25810639\n",
      "Validation score: 0.878671\n",
      "Iteration 91, loss = 0.25851301\n",
      "Validation score: 0.878078\n",
      "Iteration 92, loss = 0.25793378\n",
      "Validation score: 0.877484\n",
      "Iteration 93, loss = 0.25704970\n",
      "Validation score: 0.874666\n",
      "Iteration 94, loss = 0.25797800\n",
      "Validation score: 0.875556\n",
      "Iteration 95, loss = 0.25729376\n",
      "Validation score: 0.880006\n",
      "Iteration 96, loss = 0.25688082\n",
      "Validation score: 0.876446\n",
      "Iteration 97, loss = 0.25645396\n",
      "Validation score: 0.879264\n",
      "Iteration 98, loss = 0.25654423\n",
      "Validation score: 0.879709\n",
      "Iteration 99, loss = 0.25659613\n",
      "Validation score: 0.878523\n",
      "Iteration 100, loss = 0.25577922\n",
      "Validation score: 0.874370\n",
      "Iteration 101, loss = 0.25614118\n",
      "Validation score: 0.879116\n",
      "Iteration 102, loss = 0.25544047\n",
      "Validation score: 0.879413\n",
      "Iteration 103, loss = 0.25458881\n",
      "Validation score: 0.876001\n",
      "Iteration 104, loss = 0.25515055\n",
      "Validation score: 0.880154\n",
      "Iteration 105, loss = 0.25452774\n",
      "Validation score: 0.878374\n",
      "Iteration 106, loss = 0.25390344\n",
      "Validation score: 0.876446\n",
      "Iteration 107, loss = 0.25392240\n",
      "Validation score: 0.879858\n",
      "Iteration 108, loss = 0.25297482\n",
      "Validation score: 0.877336\n",
      "Iteration 109, loss = 0.25480293\n",
      "Validation score: 0.879709\n",
      "Iteration 110, loss = 0.25191360\n",
      "Validation score: 0.881786\n",
      "Iteration 111, loss = 0.25264671\n",
      "Validation score: 0.880154\n",
      "Iteration 112, loss = 0.25258540\n",
      "Validation score: 0.877336\n",
      "Iteration 113, loss = 0.25151386\n",
      "Validation score: 0.881637\n",
      "Iteration 114, loss = 0.25075766\n",
      "Validation score: 0.877188\n",
      "Iteration 115, loss = 0.25139898\n",
      "Validation score: 0.882824\n",
      "Iteration 116, loss = 0.25151963\n",
      "Validation score: 0.880303\n",
      "Iteration 117, loss = 0.25032907\n",
      "Validation score: 0.878523\n",
      "Iteration 118, loss = 0.25036033\n",
      "Validation score: 0.881489\n",
      "Iteration 119, loss = 0.25026050\n",
      "Validation score: 0.881193\n",
      "Iteration 120, loss = 0.25039247\n",
      "Validation score: 0.879413\n",
      "Iteration 121, loss = 0.25009663\n",
      "Validation score: 0.878078\n",
      "Iteration 122, loss = 0.24986565\n",
      "Validation score: 0.878968\n",
      "Iteration 123, loss = 0.24803219\n",
      "Validation score: 0.878671\n",
      "Iteration 124, loss = 0.25044617\n",
      "Validation score: 0.880303\n",
      "Iteration 125, loss = 0.24871942\n",
      "Validation score: 0.881341\n",
      "Iteration 126, loss = 0.24950780\n",
      "Validation score: 0.879709\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49688364\n",
      "Validation score: 0.785672\n",
      "Iteration 2, loss = 0.42463687\n",
      "Validation score: 0.811925\n",
      "Iteration 3, loss = 0.39531899\n",
      "Validation score: 0.822456\n",
      "Iteration 4, loss = 0.37803141\n",
      "Validation score: 0.828834\n",
      "Iteration 5, loss = 0.36717397\n",
      "Validation score: 0.834470\n",
      "Iteration 6, loss = 0.35943126\n",
      "Validation score: 0.834322\n",
      "Iteration 7, loss = 0.35418279\n",
      "Validation score: 0.834026\n",
      "Iteration 8, loss = 0.34846158\n",
      "Validation score: 0.843222\n",
      "Iteration 9, loss = 0.34359800\n",
      "Validation score: 0.842183\n",
      "Iteration 10, loss = 0.34046795\n",
      "Validation score: 0.846781\n",
      "Iteration 11, loss = 0.33640444\n",
      "Validation score: 0.844705\n",
      "Iteration 12, loss = 0.33365523\n",
      "Validation score: 0.844112\n",
      "Iteration 13, loss = 0.33181586\n",
      "Validation score: 0.848413\n",
      "Iteration 14, loss = 0.32864588\n",
      "Validation score: 0.847226\n",
      "Iteration 15, loss = 0.32718928\n",
      "Validation score: 0.848265\n",
      "Iteration 16, loss = 0.32453923\n",
      "Validation score: 0.846930\n",
      "Iteration 17, loss = 0.32343070\n",
      "Validation score: 0.851231\n",
      "Iteration 18, loss = 0.32049115\n",
      "Validation score: 0.849896\n",
      "Iteration 19, loss = 0.31907682\n",
      "Validation score: 0.852269\n",
      "Iteration 20, loss = 0.31778792\n",
      "Validation score: 0.852863\n",
      "Iteration 21, loss = 0.31643358\n",
      "Validation score: 0.854198\n",
      "Iteration 22, loss = 0.31470915\n",
      "Validation score: 0.854346\n",
      "Iteration 23, loss = 0.31373079\n",
      "Validation score: 0.851676\n",
      "Iteration 24, loss = 0.31251925\n",
      "Validation score: 0.856422\n",
      "Iteration 25, loss = 0.31078301\n",
      "Validation score: 0.856719\n",
      "Iteration 26, loss = 0.30933086\n",
      "Validation score: 0.857757\n",
      "Iteration 27, loss = 0.30843484\n",
      "Validation score: 0.860279\n",
      "Iteration 28, loss = 0.30720410\n",
      "Validation score: 0.859241\n",
      "Iteration 29, loss = 0.30630529\n",
      "Validation score: 0.862059\n",
      "Iteration 30, loss = 0.30540981\n",
      "Validation score: 0.862059\n",
      "Iteration 31, loss = 0.30322160\n",
      "Validation score: 0.860724\n",
      "Iteration 32, loss = 0.30284102\n",
      "Validation score: 0.859686\n",
      "Iteration 33, loss = 0.30242584\n",
      "Validation score: 0.857461\n",
      "Iteration 34, loss = 0.30122125\n",
      "Validation score: 0.862949\n",
      "Iteration 35, loss = 0.30027102\n",
      "Validation score: 0.862949\n",
      "Iteration 36, loss = 0.29869756\n",
      "Validation score: 0.866805\n",
      "Iteration 37, loss = 0.29830483\n",
      "Validation score: 0.857312\n",
      "Iteration 38, loss = 0.29732986\n",
      "Validation score: 0.864580\n",
      "Iteration 39, loss = 0.29598724\n",
      "Validation score: 0.865915\n",
      "Iteration 40, loss = 0.29516417\n",
      "Validation score: 0.862949\n",
      "Iteration 41, loss = 0.29501086\n",
      "Validation score: 0.863097\n",
      "Iteration 42, loss = 0.29386451\n",
      "Validation score: 0.866657\n",
      "Iteration 43, loss = 0.29352054\n",
      "Validation score: 0.861910\n",
      "Iteration 44, loss = 0.29318762\n",
      "Validation score: 0.867250\n",
      "Iteration 45, loss = 0.29212148\n",
      "Validation score: 0.865915\n",
      "Iteration 46, loss = 0.29149130\n",
      "Validation score: 0.868437\n",
      "Iteration 47, loss = 0.29109656\n",
      "Validation score: 0.863542\n",
      "Iteration 48, loss = 0.28980777\n",
      "Validation score: 0.865470\n",
      "Iteration 49, loss = 0.28966283\n",
      "Validation score: 0.865174\n",
      "Iteration 50, loss = 0.28888354\n",
      "Validation score: 0.866212\n",
      "Iteration 51, loss = 0.28746437\n",
      "Validation score: 0.869772\n",
      "Iteration 52, loss = 0.28755852\n",
      "Validation score: 0.866953\n",
      "Iteration 53, loss = 0.28655986\n",
      "Validation score: 0.867992\n",
      "Iteration 54, loss = 0.28699476\n",
      "Validation score: 0.867547\n",
      "Iteration 55, loss = 0.28547904\n",
      "Validation score: 0.867843\n",
      "Iteration 56, loss = 0.28554180\n",
      "Validation score: 0.866360\n",
      "Iteration 57, loss = 0.28573894\n",
      "Validation score: 0.870662\n",
      "Iteration 58, loss = 0.28559797\n",
      "Validation score: 0.868288\n",
      "Iteration 59, loss = 0.28302018\n",
      "Validation score: 0.870662\n",
      "Iteration 60, loss = 0.28285607\n",
      "Validation score: 0.871996\n",
      "Iteration 61, loss = 0.28222983\n",
      "Validation score: 0.871255\n",
      "Iteration 62, loss = 0.28196643\n",
      "Validation score: 0.872886\n",
      "Iteration 63, loss = 0.28141906\n",
      "Validation score: 0.866953\n",
      "Iteration 64, loss = 0.28086625\n",
      "Validation score: 0.865915\n",
      "Iteration 65, loss = 0.28157588\n",
      "Validation score: 0.872145\n",
      "Iteration 66, loss = 0.27952046\n",
      "Validation score: 0.872738\n",
      "Iteration 67, loss = 0.27985654\n",
      "Validation score: 0.871255\n",
      "Iteration 68, loss = 0.28036868\n",
      "Validation score: 0.871700\n",
      "Iteration 69, loss = 0.27969449\n",
      "Validation score: 0.871106\n",
      "Iteration 70, loss = 0.27866455\n",
      "Validation score: 0.870068\n",
      "Iteration 71, loss = 0.27908270\n",
      "Validation score: 0.872145\n",
      "Iteration 72, loss = 0.27768241\n",
      "Validation score: 0.870217\n",
      "Iteration 73, loss = 0.27751033\n",
      "Validation score: 0.875556\n",
      "Iteration 74, loss = 0.27655582\n",
      "Validation score: 0.873331\n",
      "Iteration 75, loss = 0.27664456\n",
      "Validation score: 0.875408\n",
      "Iteration 76, loss = 0.27598159\n",
      "Validation score: 0.871106\n",
      "Iteration 77, loss = 0.27540274\n",
      "Validation score: 0.877633\n",
      "Iteration 78, loss = 0.27599017\n",
      "Validation score: 0.874518\n",
      "Iteration 79, loss = 0.27499673\n",
      "Validation score: 0.873776\n",
      "Iteration 80, loss = 0.27346361\n",
      "Validation score: 0.872441\n",
      "Iteration 81, loss = 0.27452575\n",
      "Validation score: 0.873183\n",
      "Iteration 82, loss = 0.27396746\n",
      "Validation score: 0.870513\n",
      "Iteration 83, loss = 0.27391335\n",
      "Validation score: 0.867695\n",
      "Iteration 84, loss = 0.27321278\n",
      "Validation score: 0.873035\n",
      "Iteration 85, loss = 0.27305525\n",
      "Validation score: 0.871848\n",
      "Iteration 86, loss = 0.27301675\n",
      "Validation score: 0.874221\n",
      "Iteration 87, loss = 0.27186606\n",
      "Validation score: 0.875260\n",
      "Iteration 88, loss = 0.27261654\n",
      "Validation score: 0.871996\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49931883\n",
      "Validation score: 0.786858\n",
      "Iteration 2, loss = 0.42869746\n",
      "Validation score: 0.811035\n",
      "Iteration 3, loss = 0.40104865\n",
      "Validation score: 0.822011\n",
      "Iteration 4, loss = 0.38276463\n",
      "Validation score: 0.831652\n",
      "Iteration 5, loss = 0.37122442\n",
      "Validation score: 0.835509\n",
      "Iteration 6, loss = 0.36281202\n",
      "Validation score: 0.837585\n",
      "Iteration 7, loss = 0.35664237\n",
      "Validation score: 0.838475\n",
      "Iteration 8, loss = 0.35124094\n",
      "Validation score: 0.844705\n",
      "Iteration 9, loss = 0.34689799\n",
      "Validation score: 0.843815\n",
      "Iteration 10, loss = 0.34417181\n",
      "Validation score: 0.847078\n",
      "Iteration 11, loss = 0.34149595\n",
      "Validation score: 0.846930\n",
      "Iteration 12, loss = 0.33793453\n",
      "Validation score: 0.848858\n",
      "Iteration 13, loss = 0.33529154\n",
      "Validation score: 0.850786\n",
      "Iteration 14, loss = 0.33374728\n",
      "Validation score: 0.847226\n",
      "Iteration 15, loss = 0.33153618\n",
      "Validation score: 0.854198\n",
      "Iteration 16, loss = 0.33040868\n",
      "Validation score: 0.850638\n",
      "Iteration 17, loss = 0.32815020\n",
      "Validation score: 0.850341\n",
      "Iteration 18, loss = 0.32643353\n",
      "Validation score: 0.855977\n",
      "Iteration 19, loss = 0.32513352\n",
      "Validation score: 0.853159\n",
      "Iteration 20, loss = 0.32289630\n",
      "Validation score: 0.857757\n",
      "Iteration 21, loss = 0.32137881\n",
      "Validation score: 0.857609\n",
      "Iteration 22, loss = 0.31968152\n",
      "Validation score: 0.855532\n",
      "Iteration 23, loss = 0.31696344\n",
      "Validation score: 0.858796\n",
      "Iteration 24, loss = 0.31671027\n",
      "Validation score: 0.853901\n",
      "Iteration 25, loss = 0.31543273\n",
      "Validation score: 0.859241\n",
      "Iteration 26, loss = 0.31444878\n",
      "Validation score: 0.862949\n",
      "Iteration 27, loss = 0.31246601\n",
      "Validation score: 0.867250\n",
      "Iteration 28, loss = 0.31208539\n",
      "Validation score: 0.858499\n",
      "Iteration 29, loss = 0.31008518\n",
      "Validation score: 0.863394\n",
      "Iteration 30, loss = 0.30972130\n",
      "Validation score: 0.862355\n",
      "Iteration 31, loss = 0.30772582\n",
      "Validation score: 0.865025\n",
      "Iteration 32, loss = 0.30733523\n",
      "Validation score: 0.864432\n",
      "Iteration 33, loss = 0.30572412\n",
      "Validation score: 0.867695\n",
      "Iteration 34, loss = 0.30551723\n",
      "Validation score: 0.860872\n",
      "Iteration 35, loss = 0.30515483\n",
      "Validation score: 0.868585\n",
      "Iteration 36, loss = 0.30341683\n",
      "Validation score: 0.862949\n",
      "Iteration 37, loss = 0.30292551\n",
      "Validation score: 0.865174\n",
      "Iteration 38, loss = 0.30205388\n",
      "Validation score: 0.864284\n",
      "Iteration 39, loss = 0.30060738\n",
      "Validation score: 0.865767\n",
      "Iteration 40, loss = 0.30115432\n",
      "Validation score: 0.867250\n",
      "Iteration 41, loss = 0.29925271\n",
      "Validation score: 0.866805\n",
      "Iteration 42, loss = 0.29879234\n",
      "Validation score: 0.858647\n",
      "Iteration 43, loss = 0.29930542\n",
      "Validation score: 0.864877\n",
      "Iteration 44, loss = 0.29653608\n",
      "Validation score: 0.870958\n",
      "Iteration 45, loss = 0.29627630\n",
      "Validation score: 0.868288\n",
      "Iteration 46, loss = 0.29616003\n",
      "Validation score: 0.866508\n",
      "Iteration 47, loss = 0.29433672\n",
      "Validation score: 0.866953\n",
      "Iteration 48, loss = 0.29445643\n",
      "Validation score: 0.869623\n",
      "Iteration 49, loss = 0.29249184\n",
      "Validation score: 0.867992\n",
      "Iteration 50, loss = 0.29231233\n",
      "Validation score: 0.868288\n",
      "Iteration 51, loss = 0.29189431\n",
      "Validation score: 0.866508\n",
      "Iteration 52, loss = 0.29199060\n",
      "Validation score: 0.873183\n",
      "Iteration 53, loss = 0.29096320\n",
      "Validation score: 0.869920\n",
      "Iteration 54, loss = 0.28992294\n",
      "Validation score: 0.871255\n",
      "Iteration 55, loss = 0.29021325\n",
      "Validation score: 0.868437\n",
      "Iteration 56, loss = 0.28979896\n",
      "Validation score: 0.872441\n",
      "Iteration 57, loss = 0.29029876\n",
      "Validation score: 0.873183\n",
      "Iteration 58, loss = 0.28972658\n",
      "Validation score: 0.869623\n",
      "Iteration 59, loss = 0.28786106\n",
      "Validation score: 0.874666\n",
      "Iteration 60, loss = 0.28788919\n",
      "Validation score: 0.874370\n",
      "Iteration 61, loss = 0.28796463\n",
      "Validation score: 0.873628\n",
      "Iteration 62, loss = 0.28760611\n",
      "Validation score: 0.874815\n",
      "Iteration 63, loss = 0.28644903\n",
      "Validation score: 0.871996\n",
      "Iteration 64, loss = 0.28537829\n",
      "Validation score: 0.873628\n",
      "Iteration 65, loss = 0.28604265\n",
      "Validation score: 0.876001\n",
      "Iteration 66, loss = 0.28469008\n",
      "Validation score: 0.871106\n",
      "Iteration 67, loss = 0.28453547\n",
      "Validation score: 0.874963\n",
      "Iteration 68, loss = 0.28408227\n",
      "Validation score: 0.874370\n",
      "Iteration 69, loss = 0.28282650\n",
      "Validation score: 0.872145\n",
      "Iteration 70, loss = 0.28376647\n",
      "Validation score: 0.874666\n",
      "Iteration 71, loss = 0.28286874\n",
      "Validation score: 0.871996\n",
      "Iteration 72, loss = 0.28252457\n",
      "Validation score: 0.871848\n",
      "Iteration 73, loss = 0.28226596\n",
      "Validation score: 0.876001\n",
      "Iteration 74, loss = 0.28244511\n",
      "Validation score: 0.873776\n",
      "Iteration 75, loss = 0.28102239\n",
      "Validation score: 0.875853\n",
      "Iteration 76, loss = 0.28183920\n",
      "Validation score: 0.874518\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Validation AUC: 0.9325054374797473\n"
     ]
    }
   ],
   "source": [
    "if res.x[5] == 'none':\n",
    "    X = loans_train_df.loc[:, loans_train_df.columns != \"loan_status\"]\n",
    "    y = loans_train_df[\"loan_status\"]\n",
    "elif res.x[5] == 'ada':\n",
    "    X = loans_train_ada_df.loc[:, loans_train_ada_df.columns != \"loan_status\"]\n",
    "    y = loans_train_ada_df[\"loan_status\"]\n",
    "    \n",
    "clf.fit(X,y)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = cross_val_score(clf, X, y, cv=3, scoring='roc_auc').mean()\n",
    "print(\"Validation AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfbe4c-8179-4920-a055-673bfc5ce9f8",
   "metadata": {},
   "source": [
    "Validation AUC from 3-fold cross validation: 0.9325054374797473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45645931-70d9-4f35-baf5-5c6a1bf13aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./outputs/mlp_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(clf, './outputs/mlp_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f878d11-3b71-4734-8af5-c5fe0990ee03",
   "metadata": {},
   "source": [
    "# 5. Fitting into Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d001150-7548-4344-a6fd-ad5ea80c66e8",
   "metadata": {},
   "source": [
    "Finally, we can now generate the predictions made by the DTC on the test data. This is done by isolating the features of the test samples, forwarding it to the DTC for prediction, and appending the predicted class labels with the corresponding IDs of the test data. Predictions for the models can be found in the <b>./predictions</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f8eb55-bb21-4724-8c6a-ec5d7fe17169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58645</td>\n",
       "      <td>-0.755638</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>-0.117198</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>1.455666</td>\n",
       "      <td>2.189522</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58646</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.127233</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58647</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>-1.418731</td>\n",
       "      <td>0.403331</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>1.748450</td>\n",
       "      <td>-0.318861</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58648</td>\n",
       "      <td>0.905387</td>\n",
       "      <td>-0.300610</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620670</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58649</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.259932</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>1.573370</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>97738</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.332883</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-1.117500</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>97739</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.389963</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-1.782989</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>97740</td>\n",
       "      <td>3.895232</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>-1.043084</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0</td>\n",
       "      <td>2.637868</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>97741</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.019656</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>2.516703</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>97742</td>\n",
       "      <td>0.573182</td>\n",
       "      <td>-0.531228</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>1.839088</td>\n",
       "      <td>-0.108264</td>\n",
       "      <td>3.062003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.018914</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  person_age  person_income  person_emp_length  loan_amnt  \\\n",
       "0      58645   -0.755638       0.404383          -0.117198   2.836600   \n",
       "1      58646   -0.257331       1.127233           0.601227   0.140622   \n",
       "2      58647   -0.257331      -1.418731           0.403331  -0.937769   \n",
       "3      58648    0.905387      -0.300610           0.169270  -0.398573   \n",
       "4      58649   -0.257331       1.259932           0.923860   1.039281   \n",
       "...      ...         ...            ...                ...        ...   \n",
       "39093  97738   -0.921741      -1.332883          -0.486519  -1.117500   \n",
       "39094  97739   -0.921741      -0.389963           0.601227  -0.398573   \n",
       "39095  97740    3.895232       0.098465          -1.896898   1.039281   \n",
       "39096  97741   -0.921741      -1.019656           0.169270   0.859550   \n",
       "39097  97742    0.573182      -0.531228           0.601227   1.839088   \n",
       "\n",
       "       loan_int_rate  loan_percent_income  cb_person_default_on_file  \\\n",
       "0           1.455666             2.189522                          0   \n",
       "1           0.722635            -0.646041                          1   \n",
       "2           1.748450            -0.318861                          1   \n",
       "3          -0.470628            -0.209801                          0   \n",
       "4           1.573370            -0.100741                          1   \n",
       "...              ...                  ...                        ...   \n",
       "39093       0.044689            -0.646041                          0   \n",
       "39094      -1.782989            -0.100741                          0   \n",
       "39095      -1.043084             0.989861                          0   \n",
       "39096       1.425586             2.516703                          1   \n",
       "39097      -0.108264             3.062003                          0   \n",
       "\n",
       "       cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  ...  \\\n",
       "0                       -1.364513                               0  ...   \n",
       "1                       -0.266122                               1  ...   \n",
       "2                       -1.364513                               0  ...   \n",
       "3                        0.620670                               0  ...   \n",
       "4                       -0.266122                               1  ...   \n",
       "...                           ...                             ...  ...   \n",
       "39093                   -0.266122                               1  ...   \n",
       "39094                   -0.721995                               1  ...   \n",
       "39095                    2.637868                               1  ...   \n",
       "39096                   -0.266122                               1  ...   \n",
       "39097                    1.018914                               0  ...   \n",
       "\n",
       "       LOAN_GRADE_B  LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  \\\n",
       "0                 0             0             0             0             1   \n",
       "1                 0             1             0             0             0   \n",
       "2                 0             0             0             1             0   \n",
       "3                 0             0             0             0             0   \n",
       "4                 0             0             1             0             0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "39093             1             0             0             0             0   \n",
       "39094             0             0             0             0             0   \n",
       "39095             0             0             0             0             0   \n",
       "39096             0             0             1             0             0   \n",
       "39097             1             0             0             0             0   \n",
       "\n",
       "       LOAN_GRADE_G  CB_PERSON_CRED_HIST_LENGTH_11_17  \\\n",
       "0                 0                                 0   \n",
       "1                 0                                 0   \n",
       "2                 0                                 0   \n",
       "3                 0                                 0   \n",
       "4                 0                                 0   \n",
       "...             ...                               ...   \n",
       "39093             0                                 0   \n",
       "39094             0                                 0   \n",
       "39095             0                                 0   \n",
       "39096             0                                 0   \n",
       "39097             0                                 0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_18_above  CB_PERSON_CRED_HIST_LENGTH_5_10  \\\n",
       "0                                        0                                0   \n",
       "1                                        0                                0   \n",
       "2                                        0                                0   \n",
       "3                                        0                                1   \n",
       "4                                        0                                0   \n",
       "...                                    ...                              ...   \n",
       "39093                                    0                                0   \n",
       "39094                                    0                                0   \n",
       "39095                                    1                                0   \n",
       "39096                                    0                                0   \n",
       "39097                                    0                                1   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                       1  \n",
       "1                                       1  \n",
       "2                                       1  \n",
       "3                                       0  \n",
       "4                                       1  \n",
       "...                                   ...  \n",
       "39093                                   1  \n",
       "39094                                   1  \n",
       "39095                                   0  \n",
       "39096                                   1  \n",
       "39097                                   0  \n",
       "\n",
       "[39098 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import Testing Dataset\n",
    "loans_test_df = pd.read_csv('./outputs/cleaned_loans_test.csv')\n",
    "loans_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef20bbce-d2cf-477e-980f-2f3d8e68099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.755638</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>-0.117198</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>1.455666</td>\n",
       "      <td>2.189522</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.127233</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>-1.418731</td>\n",
       "      <td>0.403331</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>1.748450</td>\n",
       "      <td>-0.318861</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905387</td>\n",
       "      <td>-0.300610</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.259932</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>1.573370</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.332883</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-1.117500</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.389963</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-1.782989</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>3.895232</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>-1.043084</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0</td>\n",
       "      <td>2.637868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.019656</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>2.516703</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>0.573182</td>\n",
       "      <td>-0.531228</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>1.839088</td>\n",
       "      <td>-0.108264</td>\n",
       "      <td>3.062003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.018914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0       -0.755638       0.404383          -0.117198   2.836600       1.455666   \n",
       "1       -0.257331       1.127233           0.601227   0.140622       0.722635   \n",
       "2       -0.257331      -1.418731           0.403331  -0.937769       1.748450   \n",
       "3        0.905387      -0.300610           0.169270  -0.398573      -0.470628   \n",
       "4       -0.257331       1.259932           0.923860   1.039281       1.573370   \n",
       "...           ...            ...                ...        ...            ...   \n",
       "39093   -0.921741      -1.332883          -0.486519  -1.117500       0.044689   \n",
       "39094   -0.921741      -0.389963           0.601227  -0.398573      -1.782989   \n",
       "39095    3.895232       0.098465          -1.896898   1.039281      -1.043084   \n",
       "39096   -0.921741      -1.019656           0.169270   0.859550       1.425586   \n",
       "39097    0.573182      -0.531228           0.601227   1.839088      -0.108264   \n",
       "\n",
       "       loan_percent_income  cb_person_default_on_file  \\\n",
       "0                 2.189522                          0   \n",
       "1                -0.646041                          1   \n",
       "2                -0.318861                          1   \n",
       "3                -0.209801                          0   \n",
       "4                -0.100741                          1   \n",
       "...                    ...                        ...   \n",
       "39093            -0.646041                          0   \n",
       "39094            -0.100741                          0   \n",
       "39095             0.989861                          0   \n",
       "39096             2.516703                          1   \n",
       "39097             3.062003                          0   \n",
       "\n",
       "       cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  \\\n",
       "0                       -1.364513                               0   \n",
       "1                       -0.266122                               1   \n",
       "2                       -1.364513                               0   \n",
       "3                        0.620670                               0   \n",
       "4                       -0.266122                               1   \n",
       "...                           ...                             ...   \n",
       "39093                   -0.266122                               1   \n",
       "39094                   -0.721995                               1   \n",
       "39095                    2.637868                               1   \n",
       "39096                   -0.266122                               1   \n",
       "39097                    1.018914                               0   \n",
       "\n",
       "       PERSON_HOME_OWNERSHIP_OTHER  ...  LOAN_GRADE_B  LOAN_GRADE_C  \\\n",
       "0                                0  ...             0             0   \n",
       "1                                0  ...             0             1   \n",
       "2                                0  ...             0             0   \n",
       "3                                0  ...             0             0   \n",
       "4                                0  ...             0             0   \n",
       "...                            ...  ...           ...           ...   \n",
       "39093                            0  ...             1             0   \n",
       "39094                            0  ...             0             0   \n",
       "39095                            0  ...             0             0   \n",
       "39096                            0  ...             0             0   \n",
       "39097                            0  ...             1             0   \n",
       "\n",
       "       LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0                 0             0             1             0   \n",
       "1                 0             0             0             0   \n",
       "2                 0             1             0             0   \n",
       "3                 0             0             0             0   \n",
       "4                 1             0             0             0   \n",
       "...             ...           ...           ...           ...   \n",
       "39093             0             0             0             0   \n",
       "39094             0             0             0             0   \n",
       "39095             0             0             0             0   \n",
       "39096             1             0             0             0   \n",
       "39097             0             0             0             0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                     0                                    0   \n",
       "1                                     0                                    0   \n",
       "2                                     0                                    0   \n",
       "3                                     0                                    0   \n",
       "4                                     0                                    0   \n",
       "...                                 ...                                  ...   \n",
       "39093                                 0                                    0   \n",
       "39094                                 0                                    0   \n",
       "39095                                 0                                    1   \n",
       "39096                                 0                                    0   \n",
       "39097                                 0                                    0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                    0                                   1  \n",
       "1                                    0                                   1  \n",
       "2                                    0                                   1  \n",
       "3                                    1                                   0  \n",
       "4                                    0                                   1  \n",
       "...                                ...                                 ...  \n",
       "39093                                0                                   1  \n",
       "39094                                0                                   1  \n",
       "39095                                0                                   0  \n",
       "39096                                0                                   1  \n",
       "39097                                1                                   0  \n",
       "\n",
       "[39098 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = loans_test_df.loc[:, loans_test_df.columns != \"id\"]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685462f8-a0d1-4a29-ae1f-fb9d351b7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef0ac34-21ed-4dc8-a56f-845205a103bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_predictions_df = loans_test_df[\"id\"].copy(deep=True)\n",
    "loans_predictions_df = loans_predictions_df.to_frame()\n",
    "loans_predictions_df.insert(1, 'loan_status', y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f065c2d-5733-4ed5-824e-17156a5425b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>97738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>97739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>97740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>97741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>97742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  loan_status\n",
       "0      58645            1\n",
       "1      58646            0\n",
       "2      58647            1\n",
       "3      58648            0\n",
       "4      58649            0\n",
       "...      ...          ...\n",
       "39093  97738            0\n",
       "39094  97739            0\n",
       "39095  97740            0\n",
       "39096  97741            1\n",
       "39097  97742            1\n",
       "\n",
       "[39098 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab6e5ec4-fe69-4056-8aaa-c52f6825bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_predictions_df.to_csv('predictions/mlp_predictions.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8826cfb-accb-4747-bb93-e570e8593545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
