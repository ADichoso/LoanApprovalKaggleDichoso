{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b42ef0-d8b5-4504-b913-bc514293d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b701978-d1eb-49fe-90be-05b1af92ee93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.569797</td>\n",
       "      <td>-1.081318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.516442</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.619568</td>\n",
       "      <td>-0.973222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240977</td>\n",
       "      <td>-1.508084</td>\n",
       "      <td>0.587859</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.340882</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407079</td>\n",
       "      <td>0.435878</td>\n",
       "      <td>0.724529</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.493327</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>0.293930</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.167927</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101340</th>\n",
       "      <td>-0.622574</td>\n",
       "      <td>-0.580418</td>\n",
       "      <td>0.346125</td>\n",
       "      <td>-0.506511</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>-0.188108</td>\n",
       "      <td>0</td>\n",
       "      <td>0.451231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101341</th>\n",
       "      <td>-0.718924</td>\n",
       "      <td>-0.528345</td>\n",
       "      <td>0.563312</td>\n",
       "      <td>-0.386847</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>-0.076635</td>\n",
       "      <td>0</td>\n",
       "      <td>0.455258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101342</th>\n",
       "      <td>-0.888484</td>\n",
       "      <td>-0.791589</td>\n",
       "      <td>0.455777</td>\n",
       "      <td>-0.377021</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.160931</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434726</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101343</th>\n",
       "      <td>-0.621401</td>\n",
       "      <td>-0.580418</td>\n",
       "      <td>0.344272</td>\n",
       "      <td>-0.507146</td>\n",
       "      <td>0.742285</td>\n",
       "      <td>-0.188879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.451231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101344</th>\n",
       "      <td>-0.798828</td>\n",
       "      <td>-0.649073</td>\n",
       "      <td>0.523650</td>\n",
       "      <td>-0.415826</td>\n",
       "      <td>0.759936</td>\n",
       "      <td>-0.015667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101345 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        person_age  person_income  person_emp_length  loan_amnt  \\\n",
       "0         1.569797      -1.081318           0.000000  -0.578305   \n",
       "1        -0.921741      -0.052550           0.520621  -0.937769   \n",
       "2         0.240977      -1.508084           0.587859  -0.578305   \n",
       "3         0.407079       0.435878           0.724529   0.500086   \n",
       "4        -0.921741       0.098465           0.293930  -0.578305   \n",
       "...            ...            ...                ...        ...   \n",
       "101340   -0.622574      -0.580418           0.346125  -0.506511   \n",
       "101341   -0.718924      -0.528345           0.563312  -0.386847   \n",
       "101342   -0.888484      -0.791589           0.455777  -0.377021   \n",
       "101343   -0.621401      -0.580418           0.344272  -0.507146   \n",
       "101344   -0.798828      -0.649073           0.523650  -0.415826   \n",
       "\n",
       "        loan_int_rate  loan_percent_income  cb_person_default_on_file  \\\n",
       "0            0.516442             0.117380                          0   \n",
       "1            0.619568            -0.973222                          0   \n",
       "2            0.340882             0.553620                          0   \n",
       "3            0.493327             0.117380                          0   \n",
       "4            0.167927            -0.646041                          0   \n",
       "...               ...                  ...                        ...   \n",
       "101340       0.742300            -0.188108                          0   \n",
       "101341       0.733813            -0.076635                          0   \n",
       "101342       0.792969             0.160931                          0   \n",
       "101343       0.742285            -0.188879                          0   \n",
       "101344       0.759936            -0.015667                          0   \n",
       "\n",
       "        cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  \\\n",
       "0                         0.411879                               0   \n",
       "1                         0.491800                               0   \n",
       "2                         0.377667                               0   \n",
       "3                         0.528549                               0   \n",
       "4                         0.503242                               0   \n",
       "...                            ...                             ...   \n",
       "101340                    0.451231                               0   \n",
       "101341                    0.455258                               0   \n",
       "101342                    0.434726                               0   \n",
       "101343                    0.451231                               0   \n",
       "101344                    0.445865                               0   \n",
       "\n",
       "        PERSON_HOME_OWNERSHIP_OTHER  ...  LOAN_GRADE_C  LOAN_GRADE_D  \\\n",
       "0                                 0  ...             0             0   \n",
       "1                                 0  ...             1             0   \n",
       "2                                 0  ...             0             0   \n",
       "3                                 0  ...             0             0   \n",
       "4                                 0  ...             0             0   \n",
       "...                             ...  ...           ...           ...   \n",
       "101340                            0  ...             0             1   \n",
       "101341                            0  ...             0             1   \n",
       "101342                            0  ...             0             1   \n",
       "101343                            0  ...             0             1   \n",
       "101344                            0  ...             0             1   \n",
       "\n",
       "        LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0                  0             0             0   \n",
       "1                  0             0             0   \n",
       "2                  0             0             0   \n",
       "3                  0             0             0   \n",
       "4                  0             0             0   \n",
       "...              ...           ...           ...   \n",
       "101340             0             0             0   \n",
       "101341             0             0             0   \n",
       "101342             0             0             0   \n",
       "101343             0             0             0   \n",
       "101344             0             0             0   \n",
       "\n",
       "        CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                      1                                    0   \n",
       "1                                      0                                    0   \n",
       "2                                      0                                    0   \n",
       "3                                      0                                    0   \n",
       "4                                      0                                    0   \n",
       "...                                  ...                                  ...   \n",
       "101340                                 0                                    0   \n",
       "101341                                 0                                    0   \n",
       "101342                                 0                                    0   \n",
       "101343                                 0                                    0   \n",
       "101344                                 0                                    0   \n",
       "\n",
       "        CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \\\n",
       "0                                     0                                   0   \n",
       "1                                     0                                   1   \n",
       "2                                     1                                   0   \n",
       "3                                     1                                   0   \n",
       "4                                     0                                   1   \n",
       "...                                 ...                                 ...   \n",
       "101340                                0                                   1   \n",
       "101341                                0                                   1   \n",
       "101342                                0                                   1   \n",
       "101343                                0                                   1   \n",
       "101344                                0                                   1   \n",
       "\n",
       "        loan_status  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "101340            1  \n",
       "101341            1  \n",
       "101342            1  \n",
       "101343            1  \n",
       "101344            1  \n",
       "\n",
       "[101345 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import Training Dataset\n",
    "loans_train_df = pd.read_csv('./outputs/cleaned_loans_train.csv')\n",
    "loans_train_ada_df = pd.read_csv('./outputs/cleaned_loans_train.csv')\n",
    "loans_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f72267-aad4-49d9-9d6e-97b050e604f2",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062de0d9-9eb2-419e-bae5-0ab78148ef1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta0</th>\n",
       "      <th>tol</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>oversampling_method</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [loss, alpha, eta0, tol, learning_rate, oversampling_method, roc_auc]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyper_tuning = pd.DataFrame(columns=['loss', 'alpha', 'eta0', 'tol', 'learning_rate', 'oversampling_method', 'roc_auc'])\n",
    "df_hyper_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b5026b-be58-46b6-a95b-a6ba0dead741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08364950486033416\n",
      "Alpha: 648.6285423473726\n",
      "Eta0: 925.5750661239806\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7422939402914123\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.07394704287510731\n",
      "Alpha: 587.7001303535353\n",
      "Eta0: 167.91470420395706\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.47858382884937\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.02028770547596246\n",
      "Alpha: 435.7963131606108\n",
      "Eta0: 405.8598667328914\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.742482746242144\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0653803721124305\n",
      "Alpha: 142.78021024941773\n",
      "Eta0: 438.1989593514415\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5157701163398083\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.06910029489160224\n",
      "Alpha: 72.82615108911891\n",
      "Eta0: 848.2547817191397\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7441350355462047\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.00093486422396124\n",
      "Alpha: 435.1885715768862\n",
      "Eta0: 895.7668865279935\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.544220931003725\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.07994936169721144\n",
      "Alpha: 239.62544890069847\n",
      "Eta0: 366.05017418872336\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7435788192830816\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.03518524609084679\n",
      "Alpha: 287.4185737064418\n",
      "Eta0: 339.1628194548524\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7439744213765804\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.024637792468786064\n",
      "Alpha: 220.6679069160667\n",
      "Eta0: 430.71681048340173\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7182752438169869\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.06726349960136661\n",
      "Alpha: 708.2571616834668\n",
      "Eta0: 848.0910501123504\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7428625033410196\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0512483327330017\n",
      "Alpha: 0.0001\n",
      "Eta0: 0.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Invalid Config\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04525109508141455\n",
      "Alpha: 278.11764877325334\n",
      "Eta0: 348.7965528667536\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743905829503698\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0641072690126821\n",
      "Alpha: 104.07968505379641\n",
      "Eta0: 727.651619854272\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432657706050114\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 1000.0\n",
      "Eta0: 592.8383096761318\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7436058545182865\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 222.33125122411352\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5469285020424016\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 498.1864247181567\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5022751976620077\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 1000.0\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5623881976882936\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 0.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7431510282937754\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 423.1653762278326\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8566058703968843\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 1000.0\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5321649527448683\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 410.01997022272354\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5292967566671\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 656.5831404984175\n",
      "Eta0: 263.18032565873386\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5046874703281289\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.016714406407769816\n",
      "Eta0: 1000.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8396202275227166\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 565.0764687722454\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.6095927864958082\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.019535244012038003\n",
      "Alpha: 1000.0\n",
      "Eta0: 509.8258882900596\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431510773666478\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.045162444100322184\n",
      "Alpha: 743.9535878368362\n",
      "Eta0: 505.12541371094386\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7425152054572801\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 577.2893319380503\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5546452522821704\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 238.00990376712795\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7441257960804725\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 1000.0\n",
      "Eta0: 304.9017364719484\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5112080170725903\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 730.649460853501\n",
      "Eta0: 1000.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7418255161240149\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 716.6499672922164\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.6238428524734753\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 1000.0\n",
      "Eta0: 71.91950517102025\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5551409538263421\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.05046215113862859\n",
      "Alpha: 0.0001\n",
      "Eta0: 501.4039826284653\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574901048085314\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 813.2835294138438\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431510282937754\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 811.9100159749277\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5645551850753905\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.03297850711902121\n",
      "Alpha: 137.23822661525813\n",
      "Eta0: 501.20589080412446\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5560527519686708\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 201.4103596599677\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5465427865776015\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.086239475853356\n",
      "Alpha: 759.1648064560447\n",
      "Eta0: 504.07590727649796\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5500730002753037\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 567.7835657381451\n",
      "Eta0: 658.9507869502733\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.4730630725508009\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 1000.0\n",
      "Eta0: 0.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431828233658564\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 271.6465083262855\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428801035368129\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 1000.0\n",
      "Eta0: 720.3760849524408\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431147381945241\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 658.8732692387827\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7473953566026634\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 950.8666565204279\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8368767337637729\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 782.4640024980606\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8595042954703124\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 255.7934730574607\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7957588605073259\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 356.6193255731104\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8349795538714458\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 36.97353858191936\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7535623064072978\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.099999901565862\n",
      "Alpha: 1000.0\n",
      "Eta0: 498.44798359425045\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7403323241102996\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 0.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Invalid Config\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09178089774483214\n",
      "Alpha: 919.7629488827185\n",
      "Eta0: 249.43452177913156\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7419059037326777\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09183977419406458\n",
      "Alpha: 921.8183962149611\n",
      "Eta0: 249.44810021388605\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428245142041024\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0938901662494758\n",
      "Alpha: 1000.0\n",
      "Eta0: 249.65269493938277\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7424367790194122\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09780925974411853\n",
      "Alpha: 603.4674046525979\n",
      "Eta0: 250.81973234249645\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7424040397688269\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08998703311714766\n",
      "Alpha: 539.8313833045487\n",
      "Eta0: 259.80884696162246\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7437606885527339\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0979839444979923\n",
      "Alpha: 964.7674508361416\n",
      "Eta0: 248.95751765464954\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430930084197961\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09655567230148109\n",
      "Alpha: 940.3038909806291\n",
      "Eta0: 249.00561045878965\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431800009392068\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09937641537370599\n",
      "Alpha: 995.9835284180621\n",
      "Eta0: 248.6793163818097\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7437011836153529\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09578328400191635\n",
      "Alpha: 935.1055509265442\n",
      "Eta0: 248.96321834497255\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433609068444237\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.025846472673826525\n",
      "Alpha: 750.8289202006125\n",
      "Eta0: 334.3417201829485\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7435848406834569\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 951.8683918445853\n",
      "Eta0: 248.8412897822123\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436759446280145\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09482824737503856\n",
      "Alpha: 926.7345473317779\n",
      "Eta0: 248.69816081226938\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7417415274077089\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.004473854332004868\n",
      "Alpha: 1000.0\n",
      "Eta0: 320.07361143025514\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7439970905977299\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.074849498685171\n",
      "Alpha: 814.0748054166423\n",
      "Eta0: 248.09317639676567\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7414767392775282\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 763.2180012903939\n",
      "Eta0: 250.11763299984636\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7429875267888967\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09771338226704067\n",
      "Alpha: 947.0333595134346\n",
      "Eta0: 248.59295720768597\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431516341935657\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09818554198643774\n",
      "Alpha: 968.2763923946139\n",
      "Eta0: 248.46170823421758\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7440311531257843\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09694415837459631\n",
      "Alpha: 922.7474580842278\n",
      "Eta0: 248.8287824994455\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427701698384852\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09410693013903261\n",
      "Alpha: 932.256591768923\n",
      "Eta0: 248.50400984749274\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.74295543419918\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09844000229495918\n",
      "Alpha: 966.1669598023025\n",
      "Eta0: 248.32836222215124\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432200977245861\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.010820606555464185\n",
      "Alpha: 640.4278487033944\n",
      "Eta0: 307.4458944854508\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7437091120185855\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08165304448022562\n",
      "Alpha: 880.1660712719461\n",
      "Eta0: 250.18376616076483\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434969230754306\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08560326958864752\n",
      "Alpha: 605.6744859571717\n",
      "Eta0: 250.21764384166394\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7433034798101885\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 695.7394042773742\n",
      "Eta0: 249.81290873183042\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7432649360509416\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06708665610330443\n",
      "Alpha: 904.5168145609223\n",
      "Eta0: 248.64231568311632\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428058933355253\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.048619985206482995\n",
      "Alpha: 0.0001\n",
      "Eta0: 339.05405076165266\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.6463910038252987\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.020269857015177195\n",
      "Alpha: 995.9467141802196\n",
      "Eta0: 3.4899704979771387\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434291710416078\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09680814050089154\n",
      "Alpha: 965.0752838499185\n",
      "Eta0: 248.2566399723032\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433662681974228\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.03753844776734414\n",
      "Alpha: 243.96887562066996\n",
      "Eta0: 995.5377483196107\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7427046148083957\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0949353079442138\n",
      "Alpha: 984.0405577028181\n",
      "Eta0: 247.12095205709576\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438473041707979\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.024145695727348544\n",
      "Alpha: 250.9007777846847\n",
      "Eta0: 993.614052658057\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.09661481348243378\n",
      "Alpha: 26.190469523105957\n",
      "Eta0: 944.139089203539\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7424706274659391\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 806.4768022900099\n",
      "Eta0: 250.57816121728655\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7421436345965481\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.030467423045947527\n",
      "Alpha: 162.5887255465084\n",
      "Eta0: 304.8229183593653\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7438600642704699\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 249.70585296656645\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7431353124611392\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04468143126801281\n",
      "Alpha: 0.0001\n",
      "Eta0: 340.2120751439576\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7489840027865142\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07335619438264815\n",
      "Alpha: 650.667720960686\n",
      "Eta0: 250.18194839400186\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7420070033457494\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 786.6657806621564\n",
      "Eta0: 250.04576816238898\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438260595150156\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0852750109616934\n",
      "Alpha: 908.4114472463044\n",
      "Eta0: 248.30912294697697\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7440347413745266\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09992573507762717\n",
      "Alpha: 684.9908413550412\n",
      "Eta0: 250.09502850325345\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.742974045458343\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 775.9898284936661\n",
      "Eta0: 248.56712840777212\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7422459431100114\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07520230679041352\n",
      "Alpha: 867.626732540275\n",
      "Eta0: 248.55616684000685\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7419468124863026\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08782642572136388\n",
      "Alpha: 872.6410966403856\n",
      "Eta0: 248.43350082295532\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7419664164394749\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 527.9999637170708\n",
      "Eta0: 248.10114911530493\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7419462055650321\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08350634584475178\n",
      "Alpha: 1000.0\n",
      "Eta0: 247.97332829556944\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7421070658633858\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.08314195772695884\n",
      "Alpha: 173.51545178301595\n",
      "Eta0: 45.6858895181815\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.558859796427694\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04310966257568121\n",
      "Alpha: 823.4568503846393\n",
      "Eta0: 248.84284804565655\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7429455161947409\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 249.50096674180693\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7430039278913038\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 800.5463185421181\n",
      "Eta0: 248.739754778351\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7435347283282615\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09121003574182253\n",
      "Alpha: 926.5791140088157\n",
      "Eta0: 248.43550801831574\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7438369190120427\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 507.16535256412647\n",
      "Eta0: 247.90493588884098\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428256655301819\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07400966550581335\n",
      "Alpha: 774.1630221069805\n",
      "Eta0: 248.8433235322165\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7418878027708761\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09134171846826088\n",
      "Alpha: 843.2921308705875\n",
      "Eta0: 249.73669634072627\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.740217877486942\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 247.18345231675693\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.742852609346734\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0836040454602286\n",
      "Alpha: 826.1443027485623\n",
      "Eta0: 248.6052251852059\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431422923567036\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 847.3019850550481\n",
      "Eta0: 249.82951218687737\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7450716524087978\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07097532491155878\n",
      "Alpha: 726.1723805063244\n",
      "Eta0: 247.16996036461487\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427282343621616\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09741322289858101\n",
      "Alpha: 801.4262505006616\n",
      "Eta0: 247.92274751542826\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.744542112011971\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09607985049021525\n",
      "Alpha: 884.0039160325275\n",
      "Eta0: 248.02101245615154\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.742389196985696\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08975385046012563\n",
      "Alpha: 1000.0\n",
      "Eta0: 248.8439879382728\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432051457453793\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0366783799030136\n",
      "Alpha: 215.0151346111966\n",
      "Eta0: 294.83486005301273\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7436932836099137\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 958.060405178082\n",
      "Eta0: 249.2450581296721\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7432741475106478\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.03112490411363037\n",
      "Alpha: 185.13558428421584\n",
      "Eta0: 297.7088400536665\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.74339922062208\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09092771912099519\n",
      "Alpha: 776.8340785063515\n",
      "Eta0: 249.44236986479302\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433046813129328\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.047315446802336986\n",
      "Alpha: 486.62851109690337\n",
      "Eta0: 361.6830614754893\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434075130870647\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.03824808573419579\n",
      "Alpha: 162.808985641933\n",
      "Eta0: 294.72329222659783\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7438838627869421\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08034642490053093\n",
      "Alpha: 639.7241798215915\n",
      "Eta0: 247.7351907006891\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7415219179313429\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07236462268529015\n",
      "Alpha: 1000.0\n",
      "Eta0: 248.83180177036985\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7431301207855933\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05340194607139422\n",
      "Alpha: 1000.0\n",
      "Eta0: 247.5982834606796\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7411302320908367\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06980676200807932\n",
      "Alpha: 614.3728813894804\n",
      "Eta0: 247.68427324827988\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433010171226089\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08097122513211423\n",
      "Alpha: 949.9329075927327\n",
      "Eta0: 248.543673446319\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7424908722949833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.046207017006112745\n",
      "Alpha: 95.08225486716732\n",
      "Eta0: 293.36654338485897\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7439200065257562\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.043309876976626666\n",
      "Alpha: 152.10973791045845\n",
      "Eta0: 293.25153213163446\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7439191717985588\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07724582751120229\n",
      "Alpha: 804.1875695799304\n",
      "Eta0: 247.6309084816165\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.744102841870642\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09643008772252032\n",
      "Alpha: 759.2320671738128\n",
      "Eta0: 247.82892245453374\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7441839555300179\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08347561074543333\n",
      "Alpha: 996.7906159976601\n",
      "Eta0: 247.93061024244008\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7435009473845341\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07205757583055533\n",
      "Alpha: 136.4929410398296\n",
      "Eta0: 291.02629273827466\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7439606046878989\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.03713857563633844\n",
      "Alpha: 10.044175802377374\n",
      "Eta0: 292.66654515220347\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7511095142102953\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08902434097209745\n",
      "Alpha: 613.5820469021586\n",
      "Eta0: 247.51597966572004\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427285961016071\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.032627983442420044\n",
      "Alpha: 361.4646221173402\n",
      "Eta0: 294.26162684557266\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7434514668621551\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0986756047209814\n",
      "Alpha: 862.5008157473952\n",
      "Eta0: 246.39004311203522\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7426793204217818\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.03618986253866909\n",
      "Alpha: 310.14246141413963\n",
      "Eta0: 992.9367608705229\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7423602705054448\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0971473502115285\n",
      "Alpha: 835.6712682719954\n",
      "Eta0: 247.50320834999954\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427132065849489\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08167075328381407\n",
      "Alpha: 740.4017095913214\n",
      "Eta0: 248.48856426205847\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7426427114511057\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.043136935283635096\n",
      "Alpha: 308.36567046560845\n",
      "Eta0: 291.37182717539434\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7427332602317175\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 430.09372446335095\n",
      "Eta0: 654.9877256880081\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443351589113271\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09037477762288072\n",
      "Alpha: 904.2085488047945\n",
      "Eta0: 231.37379629827026\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5080920716681493\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 815.9878016377018\n",
      "Eta0: 247.82635318324603\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7423047211872872\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07747718196672729\n",
      "Alpha: 742.7621661397951\n",
      "Eta0: 7.792598110270241\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7431514571103106\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09975036097090074\n",
      "Alpha: 1000.0\n",
      "Eta0: 229.82173721416495\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5950467689881139\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.023426249311453626\n",
      "Alpha: 452.84482789112275\n",
      "Eta0: 292.6649957701423\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.742896628860661\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.04874040736915791\n",
      "Alpha: 0.0001\n",
      "Eta0: 990.168146826452\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7831063784521124\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 233.77389848314186\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7423053572107099\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0967676024425835\n",
      "Alpha: 945.9768061665337\n",
      "Eta0: 244.26858840222002\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7429763306518034\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0963956884811341\n",
      "Alpha: 929.3923039604499\n",
      "Eta0: 243.7133677539055\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7474822591766083\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09727623005171426\n",
      "Alpha: 1000.0\n",
      "Eta0: 230.78739211583297\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.541190620363826\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09830597835155476\n",
      "Alpha: 902.8993635767823\n",
      "Eta0: 243.63932901808374\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.742159610814659\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09347586817989913\n",
      "Alpha: 896.9151760359504\n",
      "Eta0: 243.1125301328745\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7443113516896905\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0936472241237463\n",
      "Alpha: 932.3650817726532\n",
      "Eta0: 243.77730489085823\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7447848625429851\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09251371790421406\n",
      "Alpha: 970.9836577224208\n",
      "Eta0: 243.52955073800075\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7441194842136444\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09305235512085043\n",
      "Alpha: 933.3465907130327\n",
      "Eta0: 243.58392253119442\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7447801369868235\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09679157261673374\n",
      "Alpha: 936.7271546359917\n",
      "Eta0: 243.5619906809347\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7416069971816416\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0978743162627526\n",
      "Alpha: 928.022787188572\n",
      "Eta0: 243.5737434839726\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.741085162363022\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09404651893719648\n",
      "Alpha: 939.5776536442614\n",
      "Eta0: 244.3131758645119\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7425471323046405\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09215185051988735\n",
      "Alpha: 848.3689324286352\n",
      "Eta0: 244.55298084825398\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434600239189723\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09074721865877562\n",
      "Alpha: 948.1097488850406\n",
      "Eta0: 244.46904391820934\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7419956036129832\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09515218552209989\n",
      "Alpha: 885.2671799169415\n",
      "Eta0: 243.66123938047392\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7466212190487965\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09666752625571236\n",
      "Alpha: 743.5659308337657\n",
      "Eta0: 245.0619278231379\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7429148143051415\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09731839197694321\n",
      "Alpha: 941.2392696600862\n",
      "Eta0: 244.44371348209341\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7429076186553196\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09096489270514761\n",
      "Alpha: 946.6629440354261\n",
      "Eta0: 244.480533481837\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7424159391382995\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04482188408756014\n",
      "Alpha: 135.96239491142413\n",
      "Eta0: 285.1338825770987\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7435778519972794\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09413842606786112\n",
      "Alpha: 943.9651962289938\n",
      "Eta0: 243.3586153415742\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7486604306640762\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09163965101727743\n",
      "Alpha: 946.251616611708\n",
      "Eta0: 243.60115077426943\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7438264337584645\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.045614304897827015\n",
      "Alpha: 180.70362155324406\n",
      "Eta0: 284.91667158729115\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438529475916825\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09624646701672344\n",
      "Alpha: 1000.0\n",
      "Eta0: 245.8651585228384\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7419439588857556\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09634132901189359\n",
      "Alpha: 914.7562859924798\n",
      "Eta0: 243.58483460506994\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7436708962766222\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08548363848949302\n",
      "Alpha: 720.2609810335449\n",
      "Eta0: 243.62477835222543\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7372563071230441\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 244.0607012893966\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7399275564670439\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06922794461225897\n",
      "Alpha: 974.5888005863602\n",
      "Eta0: 243.96190492961458\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7417900708975548\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08255404802674528\n",
      "Alpha: 924.9841321873196\n",
      "Eta0: 243.52751524071226\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7416545409747347\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09476184848184228\n",
      "Alpha: 920.3725995980232\n",
      "Eta0: 242.6097174027174\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7414358088644927\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 959.6766520924748\n",
      "Eta0: 243.92094741619198\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7431272007924514\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09855968782318168\n",
      "Alpha: 523.7156900593279\n",
      "Eta0: 243.40397073810948\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7443164505327783\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04791874865827649\n",
      "Alpha: 971.7484267458057\n",
      "Eta0: 243.33308569468412\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.741409819200026\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04662123733597915\n",
      "Alpha: 98.69729667590852\n",
      "Eta0: 283.5543395510657\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443502800529953\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.081644340404998\n",
      "Alpha: 515.881755616235\n",
      "Eta0: 243.45859506509916\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7409056231658203\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09079017656572548\n",
      "Alpha: 532.3666785958139\n",
      "Eta0: 243.98718996040932\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7421779445255826\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09257646202190423\n",
      "Alpha: 526.0613143407112\n",
      "Eta0: 243.45145718406928\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7413113513865347\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04217571986825379\n",
      "Alpha: 154.3554467624451\n",
      "Eta0: 283.24991356275916\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7439450192452428\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 245.82099213248344\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7417211875087215\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 242.98637553462635\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7436295955303528\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08330085591740491\n",
      "Alpha: 777.6267513957266\n",
      "Eta0: 244.32096160452718\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431478991909465\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07867312678017256\n",
      "Alpha: 733.6263009617786\n",
      "Eta0: 243.4817272475711\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7442005445974841\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08690229880501461\n",
      "Alpha: 913.1698721712344\n",
      "Eta0: 243.49401964743055\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7420760282515612\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06430586708508983\n",
      "Alpha: 890.0112262905755\n",
      "Eta0: 243.4138954949413\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7387739312889717\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09376097165724416\n",
      "Alpha: 956.6481315033898\n",
      "Eta0: 245.73042879883303\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436793147038018\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09000980392577407\n",
      "Alpha: 792.1132965742519\n",
      "Eta0: 244.4475434766171\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7421814677516259\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04402820561606125\n",
      "Alpha: 176.06523135714025\n",
      "Eta0: 282.2283357155955\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438203630072646\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04333870392121997\n",
      "Alpha: 160.24633564383282\n",
      "Eta0: 283.11287849665524\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436365453234727\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07986939467886847\n",
      "Alpha: 662.6530254611688\n",
      "Eta0: 243.47412998531735\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7448488599005869\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 242.3999743450407\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7391100602986035\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.043742746907809306\n",
      "Alpha: 203.9368044439558\n",
      "Eta0: 281.76831504684844\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7440658770893718\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08443309851570427\n",
      "Alpha: 774.3263707212205\n",
      "Eta0: 245.94478486446698\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427239474135799\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 243.19876539476255\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428870117229414\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0971262649727055\n",
      "Alpha: 963.6184702732346\n",
      "Eta0: 243.55649493432702\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7408344560636135\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07564374260587242\n",
      "Alpha: 887.0823215688666\n",
      "Eta0: 243.44779909229345\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7421364729491399\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0454722998723762\n",
      "Alpha: 140.97617479673212\n",
      "Eta0: 281.8973881755615\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433624388722917\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09747158654735798\n",
      "Alpha: 952.0860788411644\n",
      "Eta0: 245.34242735481718\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7427108177091478\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09270832516109911\n",
      "Alpha: 981.3130036188554\n",
      "Eta0: 244.39773185564434\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7429336061332497\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 921.3076058917471\n",
      "Eta0: 245.08189088603197\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7418954530994473\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09493648049032667\n",
      "Alpha: 987.009355145037\n",
      "Eta0: 244.52981051959668\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443299547413961\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.047929703757826736\n",
      "Alpha: 87.58115921829683\n",
      "Eta0: 281.7497466362732\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443367864093388\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0737972125379124\n",
      "Alpha: 948.994660851545\n",
      "Eta0: 244.09631496498542\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7452661106883971\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0775978295687953\n",
      "Alpha: 721.372273618424\n",
      "Eta0: 243.4466728963099\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7417720817432394\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.039550974838513435\n",
      "Alpha: 130.0688576953043\n",
      "Eta0: 281.74816002737106\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7439353163471574\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.046813865891611836\n",
      "Alpha: 131.03296331753822\n",
      "Eta0: 281.4576971395749\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436336405890414\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.018661138692516004\n",
      "Alpha: 153.4170067668699\n",
      "Eta0: 278.7995129462967\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433903416027032\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.04857313325845166\n",
      "Alpha: 124.83600950668891\n",
      "Eta0: 281.2486755033987\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438037350598442\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.023134024469246436\n",
      "Alpha: 1.4521043322594454\n",
      "Eta0: 297.6328122801797\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7763583653640821\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09852795461632376\n",
      "Alpha: 989.3769406839341\n",
      "Eta0: 244.29815484183544\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.742915147439375\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0548945436964852\n",
      "Alpha: 142.6690950898646\n",
      "Eta0: 279.52360392655504\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438507389179412\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.010466174434083927\n",
      "Alpha: 181.38046327278593\n",
      "Eta0: 298.81888646169404\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436320404403002\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08170994045740625\n",
      "Alpha: 914.5405197084433\n",
      "Eta0: 243.35895280720035\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7464796818023186\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08771683163184499\n",
      "Alpha: 508.3111459661059\n",
      "Eta0: 243.33665380214572\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7400102268747242\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07248723696297214\n",
      "Alpha: 144.12388270164968\n",
      "Eta0: 277.5780881315845\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438095037888864\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.060768386977144026\n",
      "Alpha: 907.1679781962589\n",
      "Eta0: 243.3087687388149\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7424248441603994\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.06351998120188962\n",
      "Alpha: 760.3499059962644\n",
      "Eta0: 276.1763816227973\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434619447491014\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09614370158784018\n",
      "Alpha: 703.2193546230029\n",
      "Eta0: 243.38285843272465\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.74255847871824\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0804845928912392\n",
      "Alpha: 210.37845387548413\n",
      "Eta0: 276.57606437001544\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434604695169597\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.053418971452787574\n",
      "Alpha: 121.49053205117454\n",
      "Eta0: 278.4191287602188\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436735122091763\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09668413868577562\n",
      "Alpha: 985.4646327210138\n",
      "Eta0: 245.95037014026965\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7425713847897182\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 296.41923248972887\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7169425857178324\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0922977632520786\n",
      "Alpha: 607.9748208007594\n",
      "Eta0: 243.2976514895996\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7442624576471277\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09652678097337607\n",
      "Alpha: 973.1905330351977\n",
      "Eta0: 243.47680222238924\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7428493426001325\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.054718196841022436\n",
      "Alpha: 14.059612678724546\n",
      "Eta0: 277.3389231361797\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7490354498628807\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.05876942841382965\n",
      "Alpha: 216.23444862440275\n",
      "Eta0: 277.5516903351403\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7437978161354231\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 243.12750579784486\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7411469384528999\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09872536629396246\n",
      "Alpha: 977.1976012697238\n",
      "Eta0: 246.06065566414398\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7441383244137513\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09592370133073252\n",
      "Alpha: 807.5832819446244\n",
      "Eta0: 246.05738825846612\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433046079548125\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.1\n",
      "Alpha: 549.3034435511894\n",
      "Eta0: 9.47812529459974\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.49208703568229745\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08525906333018492\n",
      "Alpha: 939.0720724329334\n",
      "Eta0: 243.46724283496263\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7416087779504158\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09162809536645335\n",
      "Alpha: 137.232558263525\n",
      "Eta0: 275.8704612948098\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7439754441738703\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09749080128925434\n",
      "Alpha: 923.7711877142915\n",
      "Eta0: 246.1278196942958\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7451465188858046\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.056451845558102336\n",
      "Alpha: 888.9915688519023\n",
      "Eta0: 243.19556133247346\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7447150670318402\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09845554780848621\n",
      "Alpha: 725.1125245475297\n",
      "Eta0: 243.29091902190345\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7443145424891346\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07598402596753864\n",
      "Alpha: 924.3985261300759\n",
      "Eta0: 245.1607971562766\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7426032379594667\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08687887094427497\n",
      "Alpha: 282.095220272064\n",
      "Eta0: 275.36566223510596\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432486702046246\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 294.7943047997888\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7374184777755666\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.058911497987764204\n",
      "Alpha: 91.61214938626767\n",
      "Eta0: 276.5223587890364\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7442269328590898\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0429293864483593\n",
      "Alpha: 597.5371755403534\n",
      "Eta0: 276.1933250374264\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743632504378863\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.06730131722897484\n",
      "Alpha: 127.32721745629689\n",
      "Eta0: 276.3675538417552\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7441224968031803\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 333.112178485021\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7329618248932296\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0432029728191406\n",
      "Alpha: 0.0001\n",
      "Eta0: 991.2803476848388\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8375046504689442\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.071958980695878\n",
      "Alpha: 662.6054807440204\n",
      "Eta0: 243.94489664976186\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433594425411313\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0058902827661823365\n",
      "Alpha: 37.93934547910026\n",
      "Eta0: 295.00935372592323\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7454971853548593\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 17.566693588772075\n",
      "Eta0: 28.984215627796445\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443028292027606\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08620247577991969\n",
      "Alpha: 984.645040304702\n",
      "Eta0: 243.22548536007483\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.742600012986154\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.08200975363661364\n",
      "Alpha: 490.8837984958456\n",
      "Eta0: 500.3341803332607\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7435680469952132\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08323694380389392\n",
      "Alpha: 203.83027792811097\n",
      "Eta0: 276.1378367132556\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434832132127763\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06264683470098414\n",
      "Alpha: 842.2841016216775\n",
      "Eta0: 243.20443443882712\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7383537924052334\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05407224481434311\n",
      "Alpha: 950.003635065383\n",
      "Eta0: 243.8228861565891\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430726567424215\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06404627167196551\n",
      "Alpha: 669.6930329972544\n",
      "Eta0: 243.24619525789024\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7420391961389473\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07647123474739144\n",
      "Alpha: 807.4934081820904\n",
      "Eta0: 243.26542500540722\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7392849038896602\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 1000.0\n",
      "Eta0: 358.1535059778129\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.744815786071784\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08822118210538472\n",
      "Alpha: 0.0001\n",
      "Eta0: 661.8511074989901\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571292436168205\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09126346883188703\n",
      "Alpha: 0.0001\n",
      "Eta0: 776.9748902329925\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.6623558397291341\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.055953241329148785\n",
      "Alpha: 129.20303786523058\n",
      "Eta0: 276.2927833228129\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7440195214934339\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07718396469236047\n",
      "Alpha: 975.1587710281846\n",
      "Eta0: 245.96829682929769\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7458345946109878\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0374669239637997\n",
      "Alpha: 0.0001\n",
      "Eta0: 706.9545329645308\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594987646442815\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07586369399938928\n",
      "Alpha: 222.14466349145803\n",
      "Eta0: 30.31770697489462\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6352994506360676\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05806211957130369\n",
      "Alpha: 941.4139052866715\n",
      "Eta0: 243.843881918051\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443358448514444\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.061481237934004826\n",
      "Alpha: 953.0787734620399\n",
      "Eta0: 243.85789637685642\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7442641096011454\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09544828259932922\n",
      "Alpha: 194.11504336576644\n",
      "Eta0: 274.97862145026124\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434834787929135\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0475310160519734\n",
      "Alpha: 417.1635490807917\n",
      "Eta0: 903.1385217778096\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7435228220077604\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07199125937388279\n",
      "Alpha: 900.2833176823156\n",
      "Eta0: 244.14092520733556\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7460459298615355\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.06048444704866638\n",
      "Alpha: 39.07949531206025\n",
      "Eta0: 500.5887091796434\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.745441857695277\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07878678366855539\n",
      "Alpha: 0.0001\n",
      "Eta0: 664.305184177507\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571207386322198\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.08831603680737787\n",
      "Alpha: 0.0001\n",
      "Eta0: 662.3632492345231\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8575080189915182\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0913173732260947\n",
      "Alpha: 982.3555920269744\n",
      "Eta0: 243.2309219436974\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7410289056291273\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09447458343524093\n",
      "Alpha: 106.27911501949049\n",
      "Eta0: 275.5761369841144\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7439503863140988\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0799168142872405\n",
      "Alpha: 127.76230221001936\n",
      "Eta0: 275.92682742611214\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743806524192272\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09303394815062704\n",
      "Alpha: 562.8390025958204\n",
      "Eta0: 243.29828021868178\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7441778031372706\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0014495303685858817\n",
      "Alpha: 276.97324019526314\n",
      "Eta0: 294.185088061647\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436653795410395\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 861.2584781888248\n",
      "Eta0: 363.44756827002146\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743508395333437\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09310013362389656\n",
      "Alpha: 809.7317162174595\n",
      "Eta0: 243.12340381935195\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.740918679324856\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 526.0758204333029\n",
      "Eta0: 356.2860190255389\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438623668960044\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09691905005296374\n",
      "Alpha: 116.59362789181888\n",
      "Eta0: 275.18993779687287\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7441155825423249\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08773731057054238\n",
      "Alpha: 849.8597897888593\n",
      "Eta0: 243.33129926263848\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7422175243759299\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08766535425196284\n",
      "Alpha: 886.3322569903777\n",
      "Eta0: 243.30438035129754\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7417829736773539\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09010613172458953\n",
      "Alpha: 911.3838474057579\n",
      "Eta0: 244.2584805187459\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427320829016604\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09145746777753876\n",
      "Alpha: 981.3707073722509\n",
      "Eta0: 245.23517148769986\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7431283682746607\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.049511926269270035\n",
      "Alpha: 889.596295929723\n",
      "Eta0: 355.86418445963255\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7419082545449559\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 824.3602500406002\n",
      "Eta0: 246.18407602920317\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428833621432843\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09351217122871322\n",
      "Alpha: 282.6978545947643\n",
      "Eta0: 405.1253587742631\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430844078197331\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 162.33699311731377\n",
      "Eta0: 356.27669809596586\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743172425789174\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.09357450526033004\n",
      "Alpha: 0.0001\n",
      "Eta0: 807.5721450136363\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594941278135386\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006169121771059143\n",
      "Alpha: 503.64327761349097\n",
      "Eta0: 499.85295571213715\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7436899870641197\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08281178716652936\n",
      "Alpha: 770.130624235167\n",
      "Eta0: 243.53591369042044\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7417101742493584\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09381937140930616\n",
      "Alpha: 75.78483881203762\n",
      "Eta0: 502.2307071347002\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432835489310493\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09028561799275975\n",
      "Alpha: 317.4385549185934\n",
      "Eta0: 364.02360858540226\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430717690043278\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 998.5964967018498\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.8426441247740563\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09109701524108813\n",
      "Alpha: 896.9917097458627\n",
      "Eta0: 244.41893902687124\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7442757273076724\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.02499185027450345\n",
      "Alpha: 47.31755665261662\n",
      "Eta0: 416.05881929935873\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431588064355562\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 308.5403998913025\n",
      "Eta0: 432.45031656199325\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7435217085927762\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.04328565992473496\n",
      "Alpha: 393.15332876159266\n",
      "Eta0: 350.02180264423424\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7381516790936953\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.026439497855200344\n",
      "Alpha: 301.99679673410435\n",
      "Eta0: 534.7960943995979\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7432043782003749\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0593043117730228\n",
      "Alpha: 0.0001\n",
      "Eta0: 588.2932350603497\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8571253459835404\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.06506196748381962\n",
      "Alpha: 82.37990985608369\n",
      "Eta0: 498.2219505463642\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430960435688109\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.038788788113265284\n",
      "Alpha: 50.226290134369826\n",
      "Eta0: 409.22143830132404\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7435228081121156\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06426343553295637\n",
      "Alpha: 893.7273131050396\n",
      "Eta0: 243.34954574893473\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7437818522440756\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09412633906391797\n",
      "Alpha: 996.2411416896671\n",
      "Eta0: 244.2571237364305\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427224160362643\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.07058349776046939\n",
      "Alpha: 2.5074771239256224\n",
      "Eta0: 506.70658394024395\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7474101479218941\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.08008026219917383\n",
      "Alpha: 57.360984092567456\n",
      "Eta0: 495.1611969612161\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433711611215554\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09964571228455629\n",
      "Alpha: 671.8273859328347\n",
      "Eta0: 243.4909427409392\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7405582921345019\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08271638548772085\n",
      "Alpha: 0.0001\n",
      "Eta0: 579.302555676983\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.857094825487263\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09686671978134284\n",
      "Alpha: 946.1769339556339\n",
      "Eta0: 245.46460552690357\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7426522921373958\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0993839464708519\n",
      "Alpha: 0.9121868186632721\n",
      "Eta0: 405.75119529912126\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7492842365335285\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.002863382840047508\n",
      "Alpha: 107.6852495789066\n",
      "Eta0: 293.49302859100027\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7441512902071871\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.08003850439928344\n",
      "Alpha: 76.75231293126691\n",
      "Eta0: 493.77397336941897\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431948715130695\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08680669081071746\n",
      "Alpha: 755.7846308424847\n",
      "Eta0: 243.4915248734578\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7391207003843677\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08111051610282342\n",
      "Alpha: 730.0519560529962\n",
      "Eta0: 243.44031041766357\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7418712612356284\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09721384691239526\n",
      "Alpha: 754.6815563574647\n",
      "Eta0: 244.2638981638352\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7421512868284766\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09492122217998811\n",
      "Alpha: 811.1507973607934\n",
      "Eta0: 244.34200233233145\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7445267489277881\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09556656032674692\n",
      "Alpha: 702.75677599024\n",
      "Eta0: 244.2130059256719\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7425325716193649\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.07176333250124811\n",
      "Alpha: 11.735579315004598\n",
      "Eta0: 495.10122960773793\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7441715562218415\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.06565705833103479\n",
      "Alpha: 58.62263672604845\n",
      "Eta0: 495.24340192742557\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432762366529343\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09417047861036199\n",
      "Alpha: 858.6785975281067\n",
      "Eta0: 243.4128322241092\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7439370832885203\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07375719001230813\n",
      "Alpha: 0.0001\n",
      "Eta0: 574.9813601183447\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571273430744638\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09460185708812223\n",
      "Alpha: 850.6738800740053\n",
      "Eta0: 246.51374189921466\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438521585563524\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.061612649025058225\n",
      "Alpha: 917.8375521155438\n",
      "Eta0: 243.38125570283427\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434002114002561\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08038090985207667\n",
      "Alpha: 951.192435763361\n",
      "Eta0: 243.4286528538008\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7428066489285272\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.04201766413221231\n",
      "Alpha: 456.1156202783146\n",
      "Eta0: 493.5611508791983\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743482267015148\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09898525972210546\n",
      "Alpha: 899.9555293454629\n",
      "Eta0: 245.50642377838494\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7427588348630811\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.014284288531281296\n",
      "Alpha: 1.2756117997622085\n",
      "Eta0: 494.377859797743\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7448272346794829\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0929687659740288\n",
      "Alpha: 795.3896447780868\n",
      "Eta0: 243.4005233937662\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7423216825436175\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04968428835589614\n",
      "Alpha: 202.78963288612687\n",
      "Eta0: 490.66337709902774\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431838999784862\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09650540860055211\n",
      "Alpha: 625.7935208169837\n",
      "Eta0: 243.41714100474888\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7427586661358082\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09232161655562773\n",
      "Alpha: 793.736946285262\n",
      "Eta0: 244.30747599608077\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7446203559408758\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08162566661079328\n",
      "Alpha: 699.7510792176685\n",
      "Eta0: 243.43815460684976\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7407240527701084\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.03633973765116932\n",
      "Alpha: 4.76197744013111\n",
      "Eta0: 407.9884533414849\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431832714746807\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06505681402897404\n",
      "Alpha: 124.67257372081285\n",
      "Eta0: 490.51788961670326\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.742819793325172\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09238348795380347\n",
      "Alpha: 920.7491614300061\n",
      "Eta0: 243.48552906823338\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.74381813280124\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07137446748435415\n",
      "Alpha: 291.149880805617\n",
      "Eta0: 488.63444023100567\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438142990419813\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0727021611485753\n",
      "Alpha: 355.8460295872322\n",
      "Eta0: 489.16446807520754\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7423443665326008\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.016909805374244892\n",
      "Alpha: 0.0001\n",
      "Eta0: 409.6855193416906\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570882765616643\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09195476332500054\n",
      "Alpha: 874.2250018419087\n",
      "Eta0: 243.3895790801553\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7437598889470761\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07906479201555391\n",
      "Alpha: 902.4679073752385\n",
      "Eta0: 244.18748581678204\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7426203919161295\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 107.97332838219698\n",
      "Eta0: 411.39975968377416\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430346899967346\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07586858340509034\n",
      "Alpha: 0.0001\n",
      "Eta0: 530.8904151873355\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8571272645309511\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04325933322242733\n",
      "Alpha: 92.63305111543453\n",
      "Eta0: 482.47116255000043\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7433261678393018\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.042642803604498195\n",
      "Alpha: 43.08847642879077\n",
      "Eta0: 529.7216795955982\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7434857086471074\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08069013196661735\n",
      "Alpha: 746.6156051839138\n",
      "Eta0: 244.79798397716905\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430345125691761\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07933627385285137\n",
      "Alpha: 885.3328698126791\n",
      "Eta0: 244.30062427535694\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7423352033867658\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08114086471325331\n",
      "Alpha: 844.9157599741701\n",
      "Eta0: 242.57390101087478\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7429873028143573\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08823078262263359\n",
      "Alpha: 840.060989903099\n",
      "Eta0: 245.5993563438084\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7424669677220095\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.06371649666522818\n",
      "Alpha: 0.0001\n",
      "Eta0: 708.3157580167899\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8595182360490945\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08880259774309898\n",
      "Alpha: 947.0383152901946\n",
      "Eta0: 245.49524123530438\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7410445889954733\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06404330646989823\n",
      "Alpha: 946.409901512507\n",
      "Eta0: 243.3902619792309\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7409471263551822\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.069689387487752\n",
      "Alpha: 804.7452987007265\n",
      "Eta0: 244.17610753932613\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7440917972137934\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08793318632141676\n",
      "Alpha: 965.5376906272276\n",
      "Eta0: 243.37134536888402\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7432801406116422\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09849746714896719\n",
      "Alpha: 0.0001\n",
      "Eta0: 662.200033703694\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8571259269799394\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0865999330029824\n",
      "Alpha: 136.8764783430023\n",
      "Eta0: 833.2680049514919\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7441123140494632\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08804040608573925\n",
      "Alpha: 760.5208141806055\n",
      "Eta0: 244.18630855155035\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7430902830325898\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08608243303360884\n",
      "Alpha: 0.0001\n",
      "Eta0: 803.3137501434537\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8533610600025138\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0376345162844573\n",
      "Alpha: 0.0001\n",
      "Eta0: 409.4609288160721\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571300644227945\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.03913930800124679\n",
      "Alpha: 0.0001\n",
      "Eta0: 989.2831077316293\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8535254119157342\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.060180576885068435\n",
      "Alpha: 54.908800175089034\n",
      "Eta0: 471.9524354042007\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432311050271494\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08423640996399866\n",
      "Alpha: 36.64776381439885\n",
      "Eta0: 467.11872735100377\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431762809298217\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09509438900539045\n",
      "Alpha: 916.684379704799\n",
      "Eta0: 244.40382808577547\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7425322583993273\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09680330841104348\n",
      "Alpha: 873.2418472969341\n",
      "Eta0: 244.21352239568768\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7411189739165883\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 460.6869433791741\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571281384314577\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05266704876718768\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.0883740300529\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571341611100078\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09541395909550303\n",
      "Alpha: 875.4165641920301\n",
      "Eta0: 244.37677215944888\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7424794132881329\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06455000707758539\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.4879655071738\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571211685796598\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09385225625016136\n",
      "Alpha: 982.1725773720813\n",
      "Eta0: 246.63955205571534\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7427814306631738\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07307714244404101\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.76741703790293\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571027838948763\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.011692255428446758\n",
      "Alpha: 175.9799150605699\n",
      "Eta0: 293.0704778162199\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7435388623576328\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08802665550056055\n",
      "Alpha: 0.0001\n",
      "Eta0: 662.4991286937725\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571335597475666\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0769261237634268\n",
      "Alpha: 70.01784786214637\n",
      "Eta0: 447.52910429640843\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7432095112636796\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09379314015010273\n",
      "Alpha: 0.0001\n",
      "Eta0: 662.3839142189754\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571268250332698\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06883815982421619\n",
      "Alpha: 770.9955793700269\n",
      "Eta0: 243.4236405060886\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7420129740346045\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.02604747576789826\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.82951115120704\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.857084438626366\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.004672964389132928\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.68715298262043\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570593785865398\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.6866980979655\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.857027737343007\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08930527972522609\n",
      "Alpha: 982.5342656570574\n",
      "Eta0: 244.32592495762\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.743599113957869\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04608813238629932\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.92302689588695\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570996989001184\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.7301776277477\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570343866730233\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0007425679850340121\n",
      "Alpha: 0.0001\n",
      "Eta0: 442.00790752884103\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570453361337074\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.16037144378515\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570119566436539\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.03666077507933425\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.1465398739422\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8595110731466512\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.08505142553607217\n",
      "Alpha: 901.6949037315006\n",
      "Eta0: 245.57328805653714\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7424908291936866\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09985083300273809\n",
      "Alpha: 0.0001\n",
      "Eta0: 663.5207640596747\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571218113823772\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.7089319746573\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570201587272871\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09812228363194189\n",
      "Alpha: 844.4327669874966\n",
      "Eta0: 243.50748485820247\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7417083907615619\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.8957935553353\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574555952007298\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.03425335484738711\n",
      "Alpha: 0.0001\n",
      "Eta0: 450.1031693131907\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571144834467451\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.80221806897725\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570271701497646\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04023411779597519\n",
      "Alpha: 0.0001\n",
      "Eta0: 442.50289224499284\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571213242266741\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07132354182392585\n",
      "Alpha: 0.0001\n",
      "Eta0: 457.4400441714215\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571065487918245\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09777154063433009\n",
      "Alpha: 996.5085208969184\n",
      "Eta0: 244.39277381627784\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7437986506419594\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07022368902113361\n",
      "Alpha: 957.2605073492751\n",
      "Eta0: 243.223097278787\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7407559324506021\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07677166020119261\n",
      "Alpha: 903.6575985015654\n",
      "Eta0: 244.3696104394397\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7438640947359157\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05688655205350188\n",
      "Alpha: 0.0001\n",
      "Eta0: 449.2860678195722\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571307877086247\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09197529410287077\n",
      "Alpha: 212.38253468511482\n",
      "Eta0: 449.67980144150994\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7431082506254887\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.00968063108405222\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.2583562772228\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570986999880152\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09848330910796063\n",
      "Alpha: 926.4415995009454\n",
      "Eta0: 243.4253392143055\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7436137301298201\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.0049027663334\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569984283714248\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09102645294855839\n",
      "Alpha: 868.4273996375689\n",
      "Eta0: 243.5366040775401\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7424412201056483\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.3711523788835\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570161087734941\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.33968606358707\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569803643032808\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09106961825294047\n",
      "Alpha: 895.3100332162005\n",
      "Eta0: 243.3480611934563\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7416027382801378\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06902559977600038\n",
      "Alpha: 961.0852250415901\n",
      "Eta0: 243.34965969995633\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7453137460844222\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09125210619285104\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.90184815018495\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571128816160044\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06313372206927358\n",
      "Alpha: 975.339132041151\n",
      "Eta0: 244.2451072196074\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.744499677639054\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04902500850740197\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.47389054327783\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571167378297085\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.043735707903382944\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.474037110337\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.857113386884095\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.06823399146967442\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.96609757937773\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571256613652327\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09729521847210656\n",
      "Alpha: 832.0288557330568\n",
      "Eta0: 243.41756577754754\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7442966999093655\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.75861497031934\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570172125567611\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09792874106441209\n",
      "Alpha: 787.1444756100439\n",
      "Eta0: 246.87700285297194\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7425770948507617\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 449.9372668049934\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570170577362145\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0767523086901435\n",
      "Alpha: 904.5695743091138\n",
      "Eta0: 244.36098660932097\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.744028317392082\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.9997006188943\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570327268292971\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.5811436315509\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570064303358751\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 441.014687133759\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594592093507218\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.01581221470807291\n",
      "Alpha: 0.0001\n",
      "Eta0: 452.7728282493585\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.857108243588284\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 449.05760171740155\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570343303379948\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.00967758181378474\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.35524199378244\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570618176823576\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0901376971843751\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.99982335301604\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571279868607463\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0036565939622852533\n",
      "Alpha: 0.0001\n",
      "Eta0: 443.35245769936546\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570939837615037\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 1000.0\n",
      "Eta0: 243.3811766012572\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7395123892014966\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.22310465808715\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569564039016973\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07395707683013579\n",
      "Alpha: 972.9722581008422\n",
      "Eta0: 243.3201371924781\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7407877151185557\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.036640510312996276\n",
      "Alpha: 0.0001\n",
      "Eta0: 449.77381512769324\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571048199583814\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0960538071939233\n",
      "Alpha: 924.9254321726906\n",
      "Eta0: 245.52722106913723\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7448431846552522\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0714737232212391\n",
      "Alpha: 0.0001\n",
      "Eta0: 967.5040813208482\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8524131006111663\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09305435618438564\n",
      "Alpha: 985.9163586941279\n",
      "Eta0: 245.47886875965673\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7465273672361024\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08654110466406688\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.5632210787405\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571291522017224\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05078121191230779\n",
      "Alpha: 0.0001\n",
      "Eta0: 442.2811919002999\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571228514119197\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.7942763589588\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570154023855799\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09806345801920002\n",
      "Alpha: 782.1364061776125\n",
      "Eta0: 245.28489997748233\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7428253943935852\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 443.89167106694583\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.857010090242826\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.028593825908204213\n",
      "Alpha: 88.86781132729945\n",
      "Eta0: 276.0100394518061\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7443509563064046\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 443.559483331407\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594590150129807\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09919086401968086\n",
      "Alpha: 931.1628764081586\n",
      "Eta0: 244.48356359998178\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7434131664066065\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.09667531334094141\n",
      "Alpha: 867.5959569695885\n",
      "Eta0: 245.20033620583158\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7452314113006248\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07790608223448935\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.6501875537632\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571253488511568\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.30288911223334\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594825957925227\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.01798255871341755\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.0412825148739\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571059002579168\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08496745487064526\n",
      "Alpha: 921.6403519870336\n",
      "Eta0: 243.91849095188064\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7414659753491964\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.03919462640842146\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.6932432868517\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571295683464237\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 443.03604268153117\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569810644441769\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08577541928659942\n",
      "Alpha: 975.714788107689\n",
      "Eta0: 244.1336158896446\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7445720027045227\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.07783585439165\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570107539469985\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.00384230487754\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569785243717285\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07462359109616012\n",
      "Alpha: 971.6474880883449\n",
      "Eta0: 244.13348931892628\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7415957014617197\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.58758566504497\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569906502502743\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07867922473076777\n",
      "Alpha: 996.4619115427223\n",
      "Eta0: 245.4405650781457\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7428389468569709\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 275.6312331511835\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7448914593369412\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 449.99458392130475\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.859468534654393\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07013598790420741\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.1251136949734\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571331479716866\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09203923388235699\n",
      "Alpha: 758.8104580748198\n",
      "Eta0: 243.27446459831657\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7394110377798717\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.9752942557845\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594226314080675\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.019265736253231483\n",
      "Alpha: 0.0001\n",
      "Eta0: 451.00268107847444\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570905519513952\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07948201458271952\n",
      "Alpha: 890.9932222265792\n",
      "Eta0: 243.17809361412546\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.740804655419958\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.73104483524446\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569800167598745\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0311903593991654\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.9805648004624\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570840394282987\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05470034413363285\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.11495962604664\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571047210046384\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.04089539203211745\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.8872547304176\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8595086185355685\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 441.1391711814214\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570054928030565\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.027149240170028996\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.29472233169673\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571159443166202\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.51806919262367\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569750372186595\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0190572086717688\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.3673453427391\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570791277373928\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09433246277764831\n",
      "Alpha: 820.7206283658203\n",
      "Eta0: 244.442314611458\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7437173610487728\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.02274276461354542\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.8226377962096\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570682561613054\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.5372647872789\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570411088983714\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08395462889948821\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.7480299511995\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571159957988508\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 448.58950193754845\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.857015923547809\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.8511343345192\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569882361789816\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 449.09636468826454\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8594719229950648\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 439.70885254207667\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574698751919186\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09485101836915442\n",
      "Alpha: 531.9785054774001\n",
      "Eta0: 243.2809700021999\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.744112074282543\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.003992195711698405\n",
      "Alpha: 0.0001\n",
      "Eta0: 446.5019100281885\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574958500259632\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.53677350791907\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569970112130445\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 441.1748214623862\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594258657013424\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.1757689621434\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570042269471929\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 438.5985480491492\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8594643760121278\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.07382158284712333\n",
      "Alpha: 0.0001\n",
      "Eta0: 450.86997707875173\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8575126357789165\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.013812977791786136\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.8038614349066\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570692454667824\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 452.2880498078798\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8574686438233514\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015312574909349959\n",
      "Alpha: 0.0001\n",
      "Eta0: 441.8776910990063\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8575023618493013\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.027743854484504497\n",
      "Alpha: 0.0001\n",
      "Eta0: 443.93355673879915\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571215306965846\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.08107728342816604\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.6946324631519\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8575134519499653\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.07407602491247599\n",
      "Alpha: 0.0001\n",
      "Eta0: 965.1375551590305\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.854182874122536\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07621267212120766\n",
      "Alpha: 146.90176528541107\n",
      "Eta0: 272.9157496836017\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7440372228404493\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 441.0965260655535\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574695895526157\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 447.60977844089547\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574754047200505\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08139981684928488\n",
      "Alpha: 0.0001\n",
      "Eta0: 445.37250271904344\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8571267872048351\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0030765119027557474\n",
      "Alpha: 0.0001\n",
      "Eta0: 443.7267754971851\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8595118672186404\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.020017864148397936\n",
      "Alpha: 0.0001\n",
      "Eta0: 440.62203147934287\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8595072765631837\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.4735890904057\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569940271289141\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09369278137033034\n",
      "Alpha: 756.4028021547891\n",
      "Eta0: 244.2471138279006\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7429398651395177\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09280402889982003\n",
      "Alpha: 885.3347076589541\n",
      "Eta0: 243.2654726791782\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7463269371967524\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.05671253049709952\n",
      "Alpha: 0.0001\n",
      "Eta0: 980.7920163411442\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8543850295916727\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.05765000596105042\n",
      "Alpha: 0.0001\n",
      "Eta0: 980.894172059294\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8425112248991898\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 440.58457527080037\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8572517323782303\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 8.107769790579711\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8550682613232742\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 438.9745722482636\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8569123512322457\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.3837720671115\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8570179982519287\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 444.17082876744524\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8574667153096778\n",
      "================\n",
      "Best parameters: ['modified_huber', 0.0001, 708.3157580167899, 0.06371649666522818, 'adaptive', 'none']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "import sys\n",
    "# Define the search space\n",
    "search_space = [\n",
    "    Categorical(['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron'], name='loss'),\n",
    "    Real(0.0001, 1000, name='alpha'),\n",
    "    Real(0.0, 1000.0, name='eta0'),\n",
    "    Real(0.0001, 0.1, name='tol'),\n",
    "    Categorical(['constant', 'optimal', 'invscaling', 'adaptive'], name='learning_rate'),\n",
    "    Categorical(['none', 'ada'], name='oversampling_method')\n",
    "]\n",
    "\n",
    "# Define your objective function (e.g., maximizing accuracy)\n",
    "@use_named_args(search_space)\n",
    "def objective_function(loss, alpha, eta0, tol, learning_rate, oversampling_method):\n",
    "    print(\"================\")\n",
    "    print(\"Configuration:\")\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Tolerance:\", tol)\n",
    "    print(\"Alpha:\", alpha)\n",
    "    print(\"Eta0:\", eta0)\n",
    "    print(\"Learning Rate:\", learning_rate)\n",
    "    print(\"Oversampling Method:\", oversampling_method)\n",
    "    print(\"----------------\")\n",
    "    try:\n",
    "        if oversampling_method == 'none':\n",
    "            X = loans_train_df.loc[:, loans_train_df.columns != \"loan_status\"]\n",
    "            y = loans_train_df[\"loan_status\"]\n",
    "        elif oversampling_method == 'ada':\n",
    "            X = loans_train_ada_df.loc[:, loans_train_ada_df.columns != \"loan_status\"]\n",
    "            y = loans_train_ada_df[\"loan_status\"]\n",
    "            \n",
    "        model = SGDClassifier(class_weight='balanced', loss=loss, alpha=alpha, eta0=eta0, max_iter=200, tol=tol, learning_rate=learning_rate)\n",
    "        roc_auc = cross_val_score(model, X, y, cv=3, scoring='roc_auc').mean()\n",
    "\n",
    "        print(\"Results:\", -roc_auc)\n",
    "        print(\"================\")\n",
    "        df_hyper_tuning.loc[len(df_hyper_tuning.index)] = [loss, alpha, eta0, tol, learning_rate, oversampling_method, roc_auc] \n",
    "        return -roc_auc\n",
    "    except:\n",
    "        print(\"Invalid Config\")\n",
    "        return 100000\n",
    "        \n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "res = gp_minimize(objective_function, search_space, n_calls=500)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3add7e-74c4-4f8f-bada-596f5a363ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta0</th>\n",
       "      <th>tol</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>oversampling_method</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>708.315758</td>\n",
       "      <td>0.063716</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.859518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>443.726775</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.859512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>447.146540</td>\n",
       "      <td>0.036661</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.859511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>445.887255</td>\n",
       "      <td>0.040895</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.859509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>440.622031</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.859507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>498.186425</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>constant</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.502275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>hinge</td>\n",
       "      <td>250.900778</td>\n",
       "      <td>993.614053</td>\n",
       "      <td>0.024146</td>\n",
       "      <td>constant</td>\n",
       "      <td>none</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>perceptron</td>\n",
       "      <td>549.303444</td>\n",
       "      <td>9.478125</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>constant</td>\n",
       "      <td>none</td>\n",
       "      <td>0.492087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perceptron</td>\n",
       "      <td>587.700130</td>\n",
       "      <td>167.914704</td>\n",
       "      <td>0.073947</td>\n",
       "      <td>constant</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.478584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>567.783566</td>\n",
       "      <td>658.950787</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>constant</td>\n",
       "      <td>none</td>\n",
       "      <td>0.473063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss        alpha        eta0       tol learning_rate  \\\n",
       "343  modified_huber     0.000100  708.315758  0.063716      adaptive   \n",
       "486  modified_huber     0.000100  443.726775  0.003077      adaptive   \n",
       "377  modified_huber     0.000100  447.146540  0.036661      adaptive   \n",
       "456  modified_huber     0.000100  445.887255  0.040895      adaptive   \n",
       "487  modified_huber     0.000100  440.622031  0.020018      adaptive   \n",
       "..              ...          ...         ...       ...           ...   \n",
       "14         log_loss  1000.000000  498.186425  0.100000      constant   \n",
       "78            hinge   250.900778  993.614053  0.024146      constant   \n",
       "227      perceptron   549.303444    9.478125  0.100000      constant   \n",
       "1        perceptron   587.700130  167.914704  0.073947      constant   \n",
       "37         log_loss   567.783566  658.950787  0.000100      constant   \n",
       "\n",
       "    oversampling_method   roc_auc  \n",
       "343                none  0.859518  \n",
       "486                none  0.859512  \n",
       "377                none  0.859511  \n",
       "456                none  0.859509  \n",
       "487                none  0.859507  \n",
       "..                  ...       ...  \n",
       "14                  ada  0.502275  \n",
       "78                 none  0.500000  \n",
       "227                none  0.492087  \n",
       "1                   ada  0.478584  \n",
       "37                 none  0.473063  \n",
       "\n",
       "[498 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyper_tuning.sort_values(by=['roc_auc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3257b27f-4806-488b-80ba-cdeca8d548bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_tuning.to_csv('hyper_tuning/sgd_hyper_tuning.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3effbdf-f86a-44c6-9d28-a036d9b123fc",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01569abe-8638-489d-974e-1160305ef72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(class_weight='balanced', loss=res.x[0], alpha=res.x[1], eta0=res.x[2], max_iter=200, tol=res.x[3], learning_rate=res.x[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4aeb4b-d63c-4a1b-9e81-f837471ae270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.8595204701587827\n"
     ]
    }
   ],
   "source": [
    "if res.x[5] == 'none':\n",
    "    X = loans_train_df.loc[:, loans_train_df.columns != \"loan_status\"]\n",
    "    y = loans_train_df[\"loan_status\"]\n",
    "elif res.x[5] == 'ada':\n",
    "    X = loans_train_ada_df.loc[:, loans_train_ada_df.columns != \"loan_status\"]\n",
    "    y = loans_train_ada_df[\"loan_status\"]\n",
    "    \n",
    "clf.fit(X,y)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = cross_val_score(clf, X, y, cv=3, scoring='roc_auc').mean()\n",
    "print(\"Validation AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9e1611-5a3e-401c-8f66-ecf6babb66cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./outputs/sgd_model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "clf.fit(X,y)\n",
    "dump(clf, './outputs/sgd_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9036f3e-f336-4314-b62b-b4d1d8543855",
   "metadata": {},
   "source": [
    "# Fitting into Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca2e220-4773-4e7e-9ff4-d06e285bfa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58645</td>\n",
       "      <td>-0.755638</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>0.370898</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>0.733635</td>\n",
       "      <td>2.189522</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.080800</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58646</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.127233</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.584177</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58647</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>-1.418731</td>\n",
       "      <td>0.479379</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.793331</td>\n",
       "      <td>-0.318861</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.080800</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58648</td>\n",
       "      <td>0.905387</td>\n",
       "      <td>-0.300610</td>\n",
       "      <td>0.430599</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>0.340882</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>0</td>\n",
       "      <td>1.197778</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58649</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.259932</td>\n",
       "      <td>0.587859</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>0.757634</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>97738</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.332883</td>\n",
       "      <td>0.293930</td>\n",
       "      <td>-1.117500</td>\n",
       "      <td>0.445950</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>97739</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.389963</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>0.073304</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.343323</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>97740</td>\n",
       "      <td>3.895232</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>0.224164</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0</td>\n",
       "      <td>3.513102</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>97741</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.019656</td>\n",
       "      <td>0.430599</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>0.727502</td>\n",
       "      <td>2.516703</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>97742</td>\n",
       "      <td>0.573182</td>\n",
       "      <td>-0.531228</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>1.839088</td>\n",
       "      <td>0.414765</td>\n",
       "      <td>3.062003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.654880</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  person_age  person_income  person_emp_length  loan_amnt  \\\n",
       "0      58645   -0.755638       0.404383           0.370898   2.836600   \n",
       "1      58646   -0.257331       1.127233           0.520621   0.140622   \n",
       "2      58647   -0.257331      -1.418731           0.479379  -0.937769   \n",
       "3      58648    0.905387      -0.300610           0.430599  -0.398573   \n",
       "4      58649   -0.257331       1.259932           0.587859   1.039281   \n",
       "...      ...         ...            ...                ...        ...   \n",
       "39093  97738   -0.921741      -1.332883           0.293930  -1.117500   \n",
       "39094  97739   -0.921741      -0.389963           0.520621  -0.398573   \n",
       "39095  97740    3.895232       0.098465           0.000000   1.039281   \n",
       "39096  97741   -0.921741      -1.019656           0.430599   0.859550   \n",
       "39097  97742    0.573182      -0.531228           0.520621   1.839088   \n",
       "\n",
       "       loan_int_rate  loan_percent_income  cb_person_default_on_file  \\\n",
       "0           0.733635             2.189522                          0   \n",
       "1           0.584177            -0.646041                          1   \n",
       "2           0.793331            -0.318861                          1   \n",
       "3           0.340882            -0.209801                          0   \n",
       "4           0.757634            -0.100741                          1   \n",
       "...              ...                  ...                        ...   \n",
       "39093       0.445950            -0.646041                          0   \n",
       "39094       0.073304            -0.100741                          0   \n",
       "39095       0.224164             0.989861                          0   \n",
       "39096       0.727502             2.516703                          1   \n",
       "39097       0.414765             3.062003                          0   \n",
       "\n",
       "       cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  ...  \\\n",
       "0                       -1.080800                               0  ...   \n",
       "1                        0.179926                               1  ...   \n",
       "2                       -1.080800                               0  ...   \n",
       "3                        1.197778                               0  ...   \n",
       "4                        0.179926                               1  ...   \n",
       "...                           ...                             ...  ...   \n",
       "39093                    0.179926                               1  ...   \n",
       "39094                   -0.343323                               1  ...   \n",
       "39095                    3.513102                               1  ...   \n",
       "39096                    0.179926                               1  ...   \n",
       "39097                    1.654880                               0  ...   \n",
       "\n",
       "       LOAN_GRADE_B  LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  \\\n",
       "0                 0             0             0             0             1   \n",
       "1                 0             1             0             0             0   \n",
       "2                 0             0             0             1             0   \n",
       "3                 0             0             0             0             0   \n",
       "4                 0             0             1             0             0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "39093             1             0             0             0             0   \n",
       "39094             0             0             0             0             0   \n",
       "39095             0             0             0             0             0   \n",
       "39096             0             0             1             0             0   \n",
       "39097             1             0             0             0             0   \n",
       "\n",
       "       LOAN_GRADE_G  CB_PERSON_CRED_HIST_LENGTH_11_17  \\\n",
       "0                 0                                 0   \n",
       "1                 0                                 0   \n",
       "2                 0                                 0   \n",
       "3                 0                                 0   \n",
       "4                 0                                 0   \n",
       "...             ...                               ...   \n",
       "39093             0                                 0   \n",
       "39094             0                                 0   \n",
       "39095             0                                 0   \n",
       "39096             0                                 0   \n",
       "39097             0                                 0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_18_above  CB_PERSON_CRED_HIST_LENGTH_5_10  \\\n",
       "0                                        0                                0   \n",
       "1                                        0                                0   \n",
       "2                                        0                                0   \n",
       "3                                        0                                1   \n",
       "4                                        0                                0   \n",
       "...                                    ...                              ...   \n",
       "39093                                    0                                0   \n",
       "39094                                    0                                0   \n",
       "39095                                    1                                0   \n",
       "39096                                    0                                0   \n",
       "39097                                    0                                1   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                       1  \n",
       "1                                       1  \n",
       "2                                       1  \n",
       "3                                       0  \n",
       "4                                       1  \n",
       "...                                   ...  \n",
       "39093                                   1  \n",
       "39094                                   1  \n",
       "39095                                   0  \n",
       "39096                                   1  \n",
       "39097                                   0  \n",
       "\n",
       "[39098 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import Testing Dataset\n",
    "loans_test_df = pd.read_csv('./outputs/cleaned_loans_test.csv')\n",
    "loans_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4c7777-94b9-42ab-8e73-95876e9bffa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.755638</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>0.370898</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>0.733635</td>\n",
       "      <td>2.189522</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.080800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.127233</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.584177</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>-1.418731</td>\n",
       "      <td>0.479379</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.793331</td>\n",
       "      <td>-0.318861</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.080800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905387</td>\n",
       "      <td>-0.300610</td>\n",
       "      <td>0.430599</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>0.340882</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>0</td>\n",
       "      <td>1.197778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.259932</td>\n",
       "      <td>0.587859</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>0.757634</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.332883</td>\n",
       "      <td>0.293930</td>\n",
       "      <td>-1.117500</td>\n",
       "      <td>0.445950</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.389963</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>0.073304</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.343323</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>3.895232</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>0.224164</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0</td>\n",
       "      <td>3.513102</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.019656</td>\n",
       "      <td>0.430599</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>0.727502</td>\n",
       "      <td>2.516703</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>0.573182</td>\n",
       "      <td>-0.531228</td>\n",
       "      <td>0.520621</td>\n",
       "      <td>1.839088</td>\n",
       "      <td>0.414765</td>\n",
       "      <td>3.062003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.654880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0       -0.755638       0.404383           0.370898   2.836600       0.733635   \n",
       "1       -0.257331       1.127233           0.520621   0.140622       0.584177   \n",
       "2       -0.257331      -1.418731           0.479379  -0.937769       0.793331   \n",
       "3        0.905387      -0.300610           0.430599  -0.398573       0.340882   \n",
       "4       -0.257331       1.259932           0.587859   1.039281       0.757634   \n",
       "...           ...            ...                ...        ...            ...   \n",
       "39093   -0.921741      -1.332883           0.293930  -1.117500       0.445950   \n",
       "39094   -0.921741      -0.389963           0.520621  -0.398573       0.073304   \n",
       "39095    3.895232       0.098465           0.000000   1.039281       0.224164   \n",
       "39096   -0.921741      -1.019656           0.430599   0.859550       0.727502   \n",
       "39097    0.573182      -0.531228           0.520621   1.839088       0.414765   \n",
       "\n",
       "       loan_percent_income  cb_person_default_on_file  \\\n",
       "0                 2.189522                          0   \n",
       "1                -0.646041                          1   \n",
       "2                -0.318861                          1   \n",
       "3                -0.209801                          0   \n",
       "4                -0.100741                          1   \n",
       "...                    ...                        ...   \n",
       "39093            -0.646041                          0   \n",
       "39094            -0.100741                          0   \n",
       "39095             0.989861                          0   \n",
       "39096             2.516703                          1   \n",
       "39097             3.062003                          0   \n",
       "\n",
       "       cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  \\\n",
       "0                       -1.080800                               0   \n",
       "1                        0.179926                               1   \n",
       "2                       -1.080800                               0   \n",
       "3                        1.197778                               0   \n",
       "4                        0.179926                               1   \n",
       "...                           ...                             ...   \n",
       "39093                    0.179926                               1   \n",
       "39094                   -0.343323                               1   \n",
       "39095                    3.513102                               1   \n",
       "39096                    0.179926                               1   \n",
       "39097                    1.654880                               0   \n",
       "\n",
       "       PERSON_HOME_OWNERSHIP_OTHER  ...  LOAN_GRADE_B  LOAN_GRADE_C  \\\n",
       "0                                0  ...             0             0   \n",
       "1                                0  ...             0             1   \n",
       "2                                0  ...             0             0   \n",
       "3                                0  ...             0             0   \n",
       "4                                0  ...             0             0   \n",
       "...                            ...  ...           ...           ...   \n",
       "39093                            0  ...             1             0   \n",
       "39094                            0  ...             0             0   \n",
       "39095                            0  ...             0             0   \n",
       "39096                            0  ...             0             0   \n",
       "39097                            0  ...             1             0   \n",
       "\n",
       "       LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0                 0             0             1             0   \n",
       "1                 0             0             0             0   \n",
       "2                 0             1             0             0   \n",
       "3                 0             0             0             0   \n",
       "4                 1             0             0             0   \n",
       "...             ...           ...           ...           ...   \n",
       "39093             0             0             0             0   \n",
       "39094             0             0             0             0   \n",
       "39095             0             0             0             0   \n",
       "39096             1             0             0             0   \n",
       "39097             0             0             0             0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                     0                                    0   \n",
       "1                                     0                                    0   \n",
       "2                                     0                                    0   \n",
       "3                                     0                                    0   \n",
       "4                                     0                                    0   \n",
       "...                                 ...                                  ...   \n",
       "39093                                 0                                    0   \n",
       "39094                                 0                                    0   \n",
       "39095                                 0                                    1   \n",
       "39096                                 0                                    0   \n",
       "39097                                 0                                    0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                    0                                   1  \n",
       "1                                    0                                   1  \n",
       "2                                    0                                   1  \n",
       "3                                    1                                   0  \n",
       "4                                    0                                   1  \n",
       "...                                ...                                 ...  \n",
       "39093                                0                                   1  \n",
       "39094                                0                                   1  \n",
       "39095                                0                                   0  \n",
       "39096                                0                                   1  \n",
       "39097                                1                                   0  \n",
       "\n",
       "[39098 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = loans_test_df.loc[:, loans_test_df.columns != \"id\"]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eecb097-ae24-4014-8547-264c923b6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee2dd873-9810-4164-a2f9-c7724d5e99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_predictions_df = loans_test_df[\"id\"].copy(deep=True)\n",
    "loans_predictions_df = loans_predictions_df.to_frame()\n",
    "loans_predictions_df.insert(1, 'loan_status', y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5457c53-557c-4cfa-aed4-18d016e14eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>97738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>97739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>97740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>97741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>97742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  loan_status\n",
       "0      58645            1\n",
       "1      58646            0\n",
       "2      58647            1\n",
       "3      58648            0\n",
       "4      58649            1\n",
       "...      ...          ...\n",
       "39093  97738            0\n",
       "39094  97739            0\n",
       "39095  97740            0\n",
       "39096  97741            1\n",
       "39097  97742            0\n",
       "\n",
       "[39098 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66e27851-d8eb-4444-afd2-60e7fa05de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_predictions_df.to_csv('predictions/sgd_predictions.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37dbc58-6dc2-4d5b-ab92-f0b1d72df582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
