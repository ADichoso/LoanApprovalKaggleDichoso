{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a00a707-a051-48b1-b59c-79f5b3789eee",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction Kaggle Competition\n",
    "## October 28, 2024\n",
    "DICHOSO, Aaron Gabrielle C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212790f-9e8e-4785-ab1a-9c9cb8fccb99",
   "metadata": {},
   "source": [
    "This Notebook is part of a series of notebooks that will contain documentation and methods used for training a SGDClassifier used in the <a href=\"https://www.kaggle.com/competitions/playground-series-s4e10/\"><b>2024 Loan Approval Prediction Kaggle Playground Series</b></a>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc6c1f-8bde-4084-9341-83d859c5564c",
   "metadata": {},
   "source": [
    "For this notebook, I will focus on the methods that I utilized for exploratory data analysis, data cleaning, and feature engineering for model training and hyperparameter training later on.\n",
    "\n",
    "To view the data cleaning itself, feel free to visit the following notebook: \n",
    "\n",
    "<ul>\n",
    "    <li>1. Data Exploration, Cleaning, and Transformations</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c89654-0500-4432-a7cc-4034ab471a3d",
   "metadata": {},
   "source": [
    "An SGDClassifier is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c92792-b5b5-4460-8776-e248deeed9e1",
   "metadata": {},
   "source": [
    "## 1. Import Cleaned Datasets\n",
    "\n",
    "We first import the cleaned datasets from the previous notebook first. In this repository, the cleaned datasets are saved in the <b>./output</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b42ef0-d8b5-4504-b913-bc514293d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ad63b-89f9-48d7-b12f-ba5be511a046",
   "metadata": {},
   "source": [
    "As observed, there are two kinds of training datasets used, the train set without oversampling, and the dataset that underwent ADASYN oversampling. I wish to test the performance of the model comparing these two methods during the hyperparameter tuning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b701978-d1eb-49fe-90be-05b1af92ee93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.569797</td>\n",
       "      <td>-1.081318</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.390423</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>1.719062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>-0.973222</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240977</td>\n",
       "      <td>-1.508084</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.185873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407079</td>\n",
       "      <td>0.435878</td>\n",
       "      <td>1.579649</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.277050</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-1.318902</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0    1.569797      -1.081318          -1.896898  -0.578305       0.390423   \n",
       "1   -0.921741      -0.052550           0.601227  -0.937769       0.896212   \n",
       "2    0.240977      -1.508084           0.923860  -0.578305      -0.470628   \n",
       "3    0.407079       0.435878           1.579649   0.500086       0.277050   \n",
       "4   -0.921741       0.098465          -0.486519  -0.578305      -1.318902   \n",
       "\n",
       "   loan_percent_income  cb_person_default_on_file  cb_person_cred_hist_length  \\\n",
       "0             0.117380                          0                    1.719062   \n",
       "1            -0.973222                          0                   -1.364513   \n",
       "2             0.553620                          0                    1.185873   \n",
       "3             0.117380                          0                    0.087481   \n",
       "4            -0.646041                          0                   -0.721995   \n",
       "\n",
       "   loan_status  PERSON_HOME_OWNERSHIP_MORTGAGE  ...  LOAN_GRADE_B  \\\n",
       "0            0                               0  ...             1   \n",
       "1            0                               0  ...             0   \n",
       "2            0                               0  ...             0   \n",
       "3            0                               0  ...             1   \n",
       "4            0                               0  ...             0   \n",
       "\n",
       "   LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0             0             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                 1                                    0   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                0                                   0  \n",
       "1                                0                                   1  \n",
       "2                                1                                   0  \n",
       "3                                1                                   0  \n",
       "4                                0                                   1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import Training Dataset\n",
    "loans_train_df = pd.read_csv('./outputs/cleaned_loans_train.csv')\n",
    "loans_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca99473-5083-4a17-ab53-a86c29d293fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.569797</td>\n",
       "      <td>-1.081318</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>0.390423</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>1.719062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.052550</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>0.896212</td>\n",
       "      <td>-0.973222</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.240977</td>\n",
       "      <td>-1.508084</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.185873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407079</td>\n",
       "      <td>0.435878</td>\n",
       "      <td>1.579649</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.277050</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-0.578305</td>\n",
       "      <td>-1.318902</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0    1.569797      -1.081318          -1.896898  -0.578305       0.390423   \n",
       "1   -0.921741      -0.052550           0.601227  -0.937769       0.896212   \n",
       "2    0.240977      -1.508084           0.923860  -0.578305      -0.470628   \n",
       "3    0.407079       0.435878           1.579649   0.500086       0.277050   \n",
       "4   -0.921741       0.098465          -0.486519  -0.578305      -1.318902   \n",
       "\n",
       "   loan_percent_income  cb_person_default_on_file  cb_person_cred_hist_length  \\\n",
       "0             0.117380                          0                    1.719062   \n",
       "1            -0.973222                          0                   -1.364513   \n",
       "2             0.553620                          0                    1.185873   \n",
       "3             0.117380                          0                    0.087481   \n",
       "4            -0.646041                          0                   -0.721995   \n",
       "\n",
       "   PERSON_HOME_OWNERSHIP_MORTGAGE  PERSON_HOME_OWNERSHIP_OTHER  ...  \\\n",
       "0                               0                            0  ...   \n",
       "1                               0                            0  ...   \n",
       "2                               0                            0  ...   \n",
       "3                               0                            0  ...   \n",
       "4                               0                            0  ...   \n",
       "\n",
       "   LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0             0             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                 1                                    0   \n",
       "1                                 0                                    0   \n",
       "2                                 0                                    0   \n",
       "3                                 0                                    0   \n",
       "4                                 0                                    0   \n",
       "\n",
       "   CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \\\n",
       "0                                0                                   0   \n",
       "1                                0                                   1   \n",
       "2                                1                                   0   \n",
       "3                                1                                   0   \n",
       "4                                0                                   1   \n",
       "\n",
       "   loan_status  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_train_ada_df = pd.read_csv('./outputs/cleaned_loans_train_ada.csv')\n",
    "loans_train_ada_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f72267-aad4-49d9-9d6e-97b050e604f2",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55827d9f-f7d1-4c1c-bd86-cc71ef2eeb97",
   "metadata": {},
   "source": [
    "The decision tree classifier (DTC) has several hyperparameters that should be tuned to maximize its performance.\n",
    "\n",
    "In this notebook, I will focus on tuning the following <a href=\"https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">hyperparameters of the DTC</a> for their explainability, namely:\n",
    "\n",
    "<ol>\n",
    "    <li><b>criterion:</b> the formula used for determining the quality of the split performed at a node</li>\n",
    "    <li><b>splitter:</b> the method used to split each node</li>\n",
    "    <li><b>max_depth:</b> the maximum depth of the decision tree</li>\n",
    "    <li><b>min_samples_split:</b> the minimum number of samples required for a node to be able to split</li>\n",
    "    <li><b>max_features:</b> the maximum number of features to consider when looking for the best way to split a node</li>\n",
    "    <li><b>oversampling_method:</b> The type of oversampling done in the dataset used.</li>\n",
    "</ol>\n",
    "\n",
    "The range of follows I chose for these hyperparameters are as follows:\n",
    "<ol>\n",
    "    <li><b>criterion:</b> gini impurity or entropy.</li>\n",
    "    <li><b>splitter:</b> \"best\" refers to using the feature that splits the node the best according to the criterion, while \"random\" refers to the best random split.</li>\n",
    "    <li><b>max_depth:</b> A range from 1 to 100.</li>\n",
    "    <li><b>min_samples_split:</b> A fraction, referring to the percentage of samples in the dataset for the minimum number of samples required, with a minimum value of 1e-8.</li>\n",
    "    <li><b>max_features:</b>\"sqrt\" refers to using sqrt(number of features) to consider for a split, \"log2\" refers to using log2(number of features) to consider for a split, and \"none\" refers to using all features to consider for a split.</li>\n",
    "        <li><b>oversampling_method:</b> Either using the ADASYN oversampled data set or the unbalanced labels dataset.</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062de0d9-9eb2-419e-bae5-0ab78148ef1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta0</th>\n",
       "      <th>tol</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>oversampling_method</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [loss, alpha, eta0, tol, learning_rate, oversampling_method, roc_auc]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyper_tuning = pd.DataFrame(columns=['loss', 'alpha', 'eta0', 'tol', 'learning_rate', 'oversampling_method', 'roc_auc'])\n",
    "df_hyper_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de632dd3-1674-484d-b25d-b39f6c355408",
   "metadata": {},
   "source": [
    "For the specific method of hyperparameter tuning, I chose to perform bayesian optimization, which is a hyperparameter tuning method that involves observing the past iterations of the tuning process to influence the configurations to test later on. I chose this method over GridSearch because of the numerous possible configurations that I would need to search through not being a feasible method for my system. Additionally, I chose it over RandomSearch because bayesian optimization would be able to utilize my system resources more effectively by searching in areas with higher probabilities of giving me high performances as opposed to randomly testing configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed41ad-fd86-40bf-8945-e5fd1231ab7e",
   "metadata": {},
   "source": [
    "To perform bayesian optimization, I utilized the <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html\">gp_minimize()</a> function provided by the scikit-optimize library.\n",
    "\n",
    "Following the instructions found in the documentations, I first initialized the search space to be used in the optimization process. This involved creating an array that pertains to the hyperparameters to tune, the data type of the hyperparameters, and the range of possible values to test in the hyperparameter tuning process.\n",
    "\n",
    "Afterwards, I created the objective function that the gp_minimize() function will execute. The objective function will use the search space defined earlier and test different values for the hyperparameters. It will then return the negative value of the Area Under the ROC (AUC), as this value will be minimized by the gp_minimize() function. Invalid configurations during hyperparameter tuning process will be given a positive value, allowing the bayesian optimization process to avoid such configurations.\n",
    "\n",
    "I also limit the number of calls performed by the function due to my limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b5026b-be58-46b6-a95b-a6ba0dead741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.05412956686808547\n",
      "Alpha: 84.74747382102345\n",
      "Eta0: 77.96390028256833\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5217856009318392\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.039649019886471154\n",
      "Alpha: 64.41220646667635\n",
      "Eta0: 39.32627781516281\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5503153537364779\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.023449328206767754\n",
      "Alpha: 45.96927560370874\n",
      "Eta0: 75.48336816805381\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.595869505490533\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.05056476131422954\n",
      "Alpha: 52.30544704291901\n",
      "Eta0: 14.63490172247202\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7919216452048573\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.021058851618259957\n",
      "Alpha: 1.8102984788305057\n",
      "Eta0: 65.79344412146754\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8043622696994893\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.041229346351993504\n",
      "Alpha: 5.570307754654888\n",
      "Eta0: 33.662874630504774\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.6305419919111612\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.015092819770383504\n",
      "Alpha: 49.76705122808478\n",
      "Eta0: 67.693717885402\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09195465629416474\n",
      "Alpha: 59.43568765547818\n",
      "Eta0: 53.2899458036037\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7910640337424818\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.08024782689625436\n",
      "Alpha: 80.5878521960227\n",
      "Eta0: 56.84623550530744\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07572660006964896\n",
      "Alpha: 19.2702301376014\n",
      "Eta0: 79.30597841619942\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5930354446302227\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8792762848077634\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.7880911176336429\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['hinge', 0.0001, 100.0, 0.1, 'optimal', 'ada'] before, using random point ['squared_hinge', 90.50282028158718, 6.654634705424524, 0.078463123744636, 'optimal', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.078463123744636\n",
      "Alpha: 90.50282028158718\n",
      "Eta0: 6.654634705424524\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8609697341185649\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09980305527632294\n",
      "Alpha: 99.34845790365219\n",
      "Eta0: 14.615092840557523\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8614667588091574\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.04536313958257223\n",
      "Alpha: 6.2001535312486284\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616211004867323\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.07865158248633465\n",
      "Alpha: 88.49190039725987\n",
      "Eta0: 10.208303992631096\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8621446368388171\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8835768289171858\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 9.578658984201558\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9027980207402737\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 96.49809436563169\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9039402324942912\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040844786834586\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['log_loss', 44.784077832825105, 78.08743796595049, 0.09440841686684166, 'invscaling', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09440841686684166\n",
      "Alpha: 44.784077832825105\n",
      "Eta0: 78.08743796595049\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5970939079556626\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['modified_huber', 73.76078982695252, 55.370495849561905, 0.09772442178501022, 'invscaling', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.09772442178501022\n",
      "Alpha: 73.76078982695252\n",
      "Eta0: 55.370495849561905\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7428637131913467\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8845799205142163\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['hinge', 79.56007281574153, 1.749369243229904, 0.035727394117202674, 'constant', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.035727394117202674\n",
      "Alpha: 79.56007281574153\n",
      "Eta0: 1.749369243229904\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5310817901124255\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['log_loss', 79.16575219549003, 80.65781474850814, 0.03861768165063565, 'invscaling', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.03861768165063565\n",
      "Alpha: 79.16575219549003\n",
      "Eta0: 80.65781474850814\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5142437585231904\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['squared_hinge', 10.943659583631915, 53.899532351619115, 0.022982447347402447, 'invscaling', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.022982447347402447\n",
      "Alpha: 10.943659583631915\n",
      "Eta0: 53.899532351619115\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5476953760941595\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['modified_huber', 24.288342845504218, 78.39569410277477, 0.09268951714375327, 'constant', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.09268951714375327\n",
      "Alpha: 24.288342845504218\n",
      "Eta0: 78.39569410277477\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5545838118313217\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['perceptron', 86.55149212265135, 17.25933242078595, 0.05944382205975051, 'adaptive', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.05944382205975051\n",
      "Alpha: 86.55149212265135\n",
      "Eta0: 17.25933242078595\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.802797126465831\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8528628656435302\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['perceptron', 66.1784991476255, 66.8040032470372, 0.057573736923642246, 'optimal', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.057573736923642246\n",
      "Alpha: 66.1784991476255\n",
      "Eta0: 66.8040032470372\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8565025169495067\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 13.539877683994483\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8626591996210183\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0005292701899918859\n",
      "Alpha: 99.50758189658117\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8614850013408821\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.06254958505813381\n",
      "Alpha: 97.57932429068147\n",
      "Eta0: 60.6287091818959\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8520665654538218\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0058788919127716315\n",
      "Alpha: 100.0\n",
      "Eta0: 80.24026911101888\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8625743286322995\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 9.428198415497553e-05\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8621836531884887\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 93.01800057742187\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8626236939376484\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 100.0\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7913141704075678\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 29.843190144191144\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9042477331214268\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.00010764470283446973\n",
      "Alpha: 93.48339892799757\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.86287585429617\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 80.48563144733774\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8868629704989247\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8869962599381317\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040854795632022\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 17.04171514193433\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8657938697590812\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 66.45395819960736\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8621517413667955\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 0.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8619956685433862\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 0.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7903843220498642\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 100.0, 0.0, 0.0001, 'optimal', 'none'] before, using random point ['squared_hinge', 52.97300312923799, 23.558935051992275, 0.06809264772404269, 'adaptive', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.06809264772404269\n",
      "Alpha: 52.97300312923799\n",
      "Eta0: 23.558935051992275\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8630034430385948\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['squared_hinge', 44.89309110285409, 36.12229164858618, 0.00543949038019718, 'adaptive', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.00543949038019718\n",
      "Alpha: 44.89309110285409\n",
      "Eta0: 36.12229164858618\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7919123491699444\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['modified_huber', 61.691218168337514, 37.343892672707454, 0.08665024359305702, 'adaptive', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.08665024359305702\n",
      "Alpha: 61.691218168337514\n",
      "Eta0: 37.343892672707454\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8627453809263387\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 100.0, 0.0, 0.0001, 'optimal', 'none'] before, using random point ['hinge', 27.120041713146698, 67.46402420901224, 0.07510351248959285, 'optimal', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07510351248959285\n",
      "Alpha: 27.120041713146698\n",
      "Eta0: 67.46402420901224\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.7910637638522063\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 100.0, 0.0, 0.0001, 'optimal', 'none'] before, using random point ['squared_hinge', 90.93319396878985, 56.92363230735309, 0.07995286292358053, 'invscaling', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07995286292358053\n",
      "Alpha: 90.93319396878985\n",
      "Eta0: 56.92363230735309\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5446445152644906\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 99.44071685949797\n",
      "Eta0: 0.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8614786348944324\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 3.557044860097338\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8614704498888849\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8614424644647194\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 100.0\n",
      "Eta0: 12.306182974973542\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.861440879581712\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 100.0, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['perceptron', 71.80876909453598, 56.431717579884165, 0.06763601903856743, 'optimal', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.06763601903856743\n",
      "Alpha: 71.80876909453598\n",
      "Eta0: 56.431717579884165\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8497241663158901\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 99.73139895650904\n",
      "Eta0: 82.68619646712084\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8614364721673436\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 88.8091703403351\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8609542465713839\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['modified_huber', 27.10772430149118, 62.14635193281342, 0.07074205635924263, 'optimal', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.07074205635924263\n",
      "Alpha: 27.10772430149118\n",
      "Eta0: 62.14635193281342\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8631838207213378\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.901946344898238\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['modified_huber', 45.48644005530842, 29.856531952717074, 0.002759855059128887, 'constant', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.002759855059128887\n",
      "Alpha: 45.48644005530842\n",
      "Eta0: 29.856531952717074\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.7299621071626937\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 94.51852927247108\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040918789208977\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 96.52658067819037\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040822426773699\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['hinge', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['modified_huber', 88.04948705147582, 83.83257904284446, 0.05651538590388674, 'constant', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.05651538590388674\n",
      "Alpha: 88.04948705147582\n",
      "Eta0: 83.83257904284446\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['hinge', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['log_loss', 25.279356784625413, 94.35429955152946, 0.09797349740219594, 'adaptive', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09797349740219594\n",
      "Alpha: 25.279356784625413\n",
      "Eta0: 94.35429955152946\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8620772426153976\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 14.475581268792832\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616347567298922\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904633909620903\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['modified_huber', 43.190277855695655, 51.720587173189514, 0.028409546997040343, 'constant', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.028409546997040343\n",
      "Alpha: 43.190277855695655\n",
      "Eta0: 51.720587173189514\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.4175326375782571\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 77.03797643415137\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8328147311794791\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01330992318840209\n",
      "Alpha: 0.0001\n",
      "Eta0: 97.29356463687314\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046286594999832\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.03827487538316947\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040716072645439\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.02752451152810618\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046370825330046\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.005082843076252805\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046453606846082\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.002411774287248661\n",
      "Alpha: 0.0001\n",
      "Eta0: 90.68173544617592\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046249492665498\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.008559566787569639\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046295083963455\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006840945946941428\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046289302143494\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.007398502960674447\n",
      "Alpha: 0.0001\n",
      "Eta0: 48.278880630186485\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046448614327591\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.002453555155522101\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046150192105703\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0019823751104693543\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046235499544087\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0075471365474696515\n",
      "Alpha: 0.0001\n",
      "Eta0: 97.46951470756721\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046161065774271\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.007440237466823685\n",
      "Alpha: 0.0001\n",
      "Eta0: 50.93764029043611\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046446604196521\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.007330787171261497\n",
      "Alpha: 0.0001\n",
      "Eta0: 91.62715315608916\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904639090374434\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0010941306345415148\n",
      "Alpha: 0.0001\n",
      "Eta0: 89.63383034947893\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046302667683906\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0066841194115532096\n",
      "Alpha: 0.0001\n",
      "Eta0: 7.172438040171884\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046371252427972\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006490729910918645\n",
      "Alpha: 0.0001\n",
      "Eta0: 67.23905131924685\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046262939301567\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006152567476107103\n",
      "Alpha: 0.0001\n",
      "Eta0: 76.94490776326323\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046162575566274\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.00605397346075334\n",
      "Alpha: 0.0001\n",
      "Eta0: 78.03145025771076\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046300804826991\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0059014736131582516\n",
      "Alpha: 0.0001\n",
      "Eta0: 46.59975693945398\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046223163467433\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.005545397573874586\n",
      "Alpha: 0.0001\n",
      "Eta0: 14.638474752262146\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046415539318046\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 46.12457133007669\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046224801571464\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01795741817155295\n",
      "Alpha: 0.0001\n",
      "Eta0: 50.39652781889284\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046280218283319\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0032459802796223625\n",
      "Alpha: 0.0001\n",
      "Eta0: 28.745863653668568\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046234877634604\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0026502407987918897\n",
      "Alpha: 0.0001\n",
      "Eta0: 74.76843467704659\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046238072211962\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.002140361787340934\n",
      "Alpha: 0.0001\n",
      "Eta0: 79.3172326797197\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040880323989292\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 28.735983867189148\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046311509391414\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 73.98024185619055\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046338447161876\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 99.54106182205575\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046292151275033\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 98.59125388729643\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046287513469408\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 23.55519808500843\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046238013545548\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 44.17378998277145\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040808571723606\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 82.56643986092814\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046313382178833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0020854904129543055\n",
      "Alpha: 0.0001\n",
      "Eta0: 69.3743911520985\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904636416908939\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 75.86529136044896\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046300236554465\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.8173386901753288\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0012152344686190385\n",
      "Alpha: 0.0001\n",
      "Eta0: 82.77661878539719\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046348174697317\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0003139335717886879\n",
      "Alpha: 0.0001\n",
      "Eta0: 43.65802253291381\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040808640341927\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.009237749939532688\n",
      "Alpha: 0.0001\n",
      "Eta0: 51.85690752705104\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046319809578832\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 91.67978914962576\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046268083567219\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.00014588168268514837\n",
      "Alpha: 0.0001\n",
      "Eta0: 72.5255016471863\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040760838215807\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0073611984486392995\n",
      "Alpha: 0.0001\n",
      "Eta0: 88.64821535910995\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904638218583305\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006425100697408047\n",
      "Alpha: 0.0001\n",
      "Eta0: 63.66881003384892\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046309805922158\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 67.89489235591208\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904632587351257\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0005024680696702034\n",
      "Alpha: 0.0001\n",
      "Eta0: 77.18749130894224\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046353372578523\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 36.84483203716877\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046344682339953\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.008188924429129059\n",
      "Alpha: 0.0001\n",
      "Eta0: 17.90157858774924\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046363957841136\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.003305103580634822\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046381527369038\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.003641330355838903\n",
      "Alpha: 0.0001\n",
      "Eta0: 95.99125185717006\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904623828756939\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004036691027092978\n",
      "Alpha: 0.0001\n",
      "Eta0: 36.078762290296936\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046568124288953\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006574548239653331\n",
      "Alpha: 0.0001\n",
      "Eta0: 35.71749037231465\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046230814564892\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006709300354576583\n",
      "Alpha: 0.0001\n",
      "Eta0: 87.4374435316281\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046308229523619\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.006671605209330139\n",
      "Alpha: 0.0001\n",
      "Eta0: 71.66265965141456\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046228379005248\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0023040985717228866\n",
      "Alpha: 0.0001\n",
      "Eta0: 51.006975166552735\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046221781345399\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0007669110488657952\n",
      "Alpha: 0.0001\n",
      "Eta0: 73.23886085746794\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046295730634992\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 90.83756389353037\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046392507348427\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.002672587825106492\n",
      "Alpha: 0.0001\n",
      "Eta0: 69.44791617874048\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046433840765117\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 39.29095032139301\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046315801087763\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.00010492271516769188\n",
      "Alpha: 0.0001\n",
      "Eta0: 86.6442651513725\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904630173613774\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 67.27984887331323\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904626542466722\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004946322106037419\n",
      "Alpha: 0.0001\n",
      "Eta0: 98.69011036262197\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046279309808453\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.007377368182659262\n",
      "Alpha: 0.0001\n",
      "Eta0: 57.50564902640857\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046240710822914\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0072846830962644766\n",
      "Alpha: 0.0001\n",
      "Eta0: 57.054187374244314\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046386189037837\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 59.98198897542492\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046308936680735\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.003127901221389328\n",
      "Alpha: 0.0001\n",
      "Eta0: 66.85400395315318\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046336294957321\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 93.29507743352853\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046282790630165\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 98.91220885325127\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046328448684475\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 79.31595390947545\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046349173599091\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['perceptron', 79.92387646095324, 23.595318663012694, 0.01161843652817615, 'invscaling', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.01161843652817615\n",
      "Alpha: 79.92387646095324\n",
      "Eta0: 23.595318663012694\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5446017675365954\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 67.46548372206918\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904637338659068\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0048376819965610055\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904636052284467\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004788453535830111\n",
      "Alpha: 0.0001\n",
      "Eta0: 68.11188412121521\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046393616302697\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 79.35048516367462\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046284096857216\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.005197326398208104\n",
      "Alpha: 0.0001\n",
      "Eta0: 32.02303913654268\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046244369437962\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004643657042497927\n",
      "Alpha: 0.0001\n",
      "Eta0: 39.63428357956715\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046306586176108\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004936956792154248\n",
      "Alpha: 0.0001\n",
      "Eta0: 73.96556952708931\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046238720039206\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004827819932582008\n",
      "Alpha: 0.0001\n",
      "Eta0: 86.63605874028505\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046345946683387\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004401769194038376\n",
      "Alpha: 0.0001\n",
      "Eta0: 99.0070839695006\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046184580276533\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 28.6947496612596\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046269439792892\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 53.836274310763734\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046371664886964\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 77.74269055587598\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046325811015207\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 67.83216940070558\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046286670359766\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 83.77785168415836\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046174005672089\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.004039287882782739\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046360512165096\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0037283275187838826\n",
      "Alpha: 0.0001\n",
      "Eta0: 66.88768566937104\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046410893957529\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0017127537336857806\n",
      "Alpha: 0.0001\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046420464352517\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 49.949173480965555\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046287442368461\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 31.949129276537512\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046286369262457\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.000126403584279229\n",
      "Alpha: 0.0001\n",
      "Eta0: 85.14018458864436\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046245872809381\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 96.33021063706437\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046354362470077\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 65.05281346132368\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046277448428272\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 87.31491575474085\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046435024975983\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 50.47806235670809\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046238793280347\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 43.656144058456384\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046405446518581\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['log_loss', 95.7155372582, 27.522421479067678, 0.05786727461447874, 'invscaling', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.05786727461447874\n",
      "Alpha: 95.7155372582\n",
      "Eta0: 27.522421479067678\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6141074685781666\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 59.71147177767316\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046253088761781\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 97.2662161269348\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046301014234678\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 76.08207940836463\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046398311619033\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 44.86518155384958\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046353024521667\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 59.46659126156055\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046302888841264\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 46.9322682702352\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046399467169018\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 57.489479039455894\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046288215211833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.002164394520603513\n",
      "Alpha: 0.0001\n",
      "Eta0: 89.47581296225358\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046239292399503\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 67.80804112116382\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046274445880217\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 36.60660991050072\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046306675532918\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 78.0089297947223\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046347794901369\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 93.00043790704035\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046384881676485\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 62.31906496931955\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046368019862153\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 63.535481403432705\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046188780711298\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 63.87861333822023\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904630987421945\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['log_loss', 89.86077400115252, 81.84668612322838, 0.04666413063325838, 'constant', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.04666413063325838\n",
      "Alpha: 89.86077400115252\n",
      "Eta0: 81.84668612322838\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.32098762522791285\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.022166575015964994\n",
      "Alpha: 0.0001\n",
      "Eta0: 78.16382866283001\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040927073815777\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0006832334921429543\n",
      "Alpha: 0.0001\n",
      "Eta0: 91.98228196577834\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046282022259802\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 85.88082121700933\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046274019317341\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.022259731878947524\n",
      "Alpha: 0.0001\n",
      "Eta0: 60.22139142264595\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046307182677805\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.010291984970083663\n",
      "Alpha: 0.0001\n",
      "Eta0: 65.28077831535202\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046383904626102\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.011898673012847097\n",
      "Alpha: 0.0001\n",
      "Eta0: 37.38603969811687\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046074711425871\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0008671966559997061\n",
      "Alpha: 0.0001\n",
      "Eta0: 73.78925366364926\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046419115275093\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.003109947852093086\n",
      "Alpha: 0.0001\n",
      "Eta0: 47.42643971477036\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.904633831546834\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.00010544012430207686\n",
      "Alpha: 0.0001\n",
      "Eta0: 72.2470911164892\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046234944048525\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0015226231317257814\n",
      "Alpha: 0.0001\n",
      "Eta0: 94.04408604432922\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046320658445599\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0007287589733667601\n",
      "Alpha: 0.0001\n",
      "Eta0: 48.31324896349912\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046372832272226\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0007536757283195525\n",
      "Alpha: 0.0001\n",
      "Eta0: 38.07785821823688\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046326382498027\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.00017434665254468566\n",
      "Alpha: 0.0001\n",
      "Eta0: 76.84497034359042\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046276379367236\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 66.10102644778222\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046344953816963\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001077915558286828\n",
      "Alpha: 0.0001\n",
      "Eta0: 80.50931272887011\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046359398370316\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0002519810554176388\n",
      "Alpha: 0.0001\n",
      "Eta0: 31.292667537608782\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046239720332107\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0005434662592979117\n",
      "Alpha: 0.0001\n",
      "Eta0: 29.57735815089585\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046330535052958\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 94.26555858428881\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046246573246286\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 85.3609752168809\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046283867074872\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 70.45572906287046\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.90462525762022\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 89.85019295362376\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046331965611274\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 54.3872331463063\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046278744981642\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 94.31716542685933\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046265374604389\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 54.87809689613253\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046208939557788\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 99.06457616750713\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046274806756841\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 77.37371920177823\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9046230363304167\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'adaptive', 'none'] before, using random point ['hinge', 77.33255188269172, 87.55481702004381, 0.050349274188254516, 'constant', 'none']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.050349274188254516\n",
      "Alpha: 77.33255188269172\n",
      "Eta0: 87.55481702004381\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.58200986440154\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point ['log_loss', 0.0001, 100.0, 0.0001, 'optimal', 'none'] before, using random point ['squared_hinge', 47.66060914633564, 63.42334634734921, 0.008016924546043322, 'constant', 'ada']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.008016924546043322\n",
      "Alpha: 47.66060914633564\n",
      "Eta0: 63.42334634734921\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.061085069793886955\n",
      "Alpha: 2.804993587496848\n",
      "Eta0: 81.6332310025716\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.862327704354712\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.055365206013973094\n",
      "Alpha: 1.7833655545930158\n",
      "Eta0: 64.88863968787861\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8664025735051513\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: squared_hinge\n",
      "Tolerance: 0.07933086612972681\n",
      "Alpha: 57.707035826049996\n",
      "Eta0: 50.414394215772894\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8629726024611158\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.008481869207872925\n",
      "Alpha: 0.0001\n",
      "Eta0: 91.79000526529029\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.9040678981632736\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0001\n",
      "Alpha: 0.0001\n",
      "Eta0: 0.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Invalid Config\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09999999423448412\n",
      "Alpha: 0.0001\n",
      "Eta0: 1.3436413286446398e-05\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8617384811644175\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.09999682945012903\n",
      "Alpha: 0.0001\n",
      "Eta0: 99.9955469334388\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8854712882741724\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: perceptron\n",
      "Tolerance: 0.00011402768466133671\n",
      "Alpha: 99.8763130965306\n",
      "Eta0: 0.008996842149674299\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.8085747717711179\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.00010436800083682954\n",
      "Alpha: 99.98101626243898\n",
      "Eta0: 0.0\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616176292331597\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.09997977088113871\n",
      "Alpha: 0.0001\n",
      "Eta0: 0.0\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Invalid Config\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0999998527262475\n",
      "Alpha: 6.661149566552336\n",
      "Eta0: 48.467480199496826\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5500049981844403\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.1\n",
      "Alpha: 100.0\n",
      "Eta0: 100.0\n",
      "Learning Rate: invscaling\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.48887322747796547\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.1\n",
      "Alpha: 0.0001\n",
      "Eta0: 0.0\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Invalid Config\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.011425851184840564\n",
      "Alpha: 80.52621575621679\n",
      "Eta0: 100.0\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5543830933610677\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.011687140213119914\n",
      "Alpha: 78.8754981970756\n",
      "Eta0: 100.0\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.518528743451316\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.011785028405483055\n",
      "Alpha: 78.38376384355456\n",
      "Eta0: 99.84139705504154\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5100228308824891\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.011885164694847895\n",
      "Alpha: 78.06957538524834\n",
      "Eta0: 99.60969638614314\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5716383447433514\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012006362328244563\n",
      "Alpha: 77.94169572950773\n",
      "Eta0: 99.46883593666873\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6160926393302333\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0803942162452034\n",
      "Alpha: 15.94239282587691\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616502957142137\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01208206343917531\n",
      "Alpha: 77.70356062052032\n",
      "Eta0: 99.2714100958905\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.45524855019689775\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01212345665643102\n",
      "Alpha: 77.57666075427309\n",
      "Eta0: 99.18999062712346\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6188686490441099\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07969385716491402\n",
      "Alpha: 15.966553193095093\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616167258894415\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012259429504503939\n",
      "Alpha: 77.46155201408851\n",
      "Eta0: 99.04790979664855\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5543277909297796\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012249213627872995\n",
      "Alpha: 77.34509979702095\n",
      "Eta0: 98.9860871822277\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5826812218429543\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07926346914369929\n",
      "Alpha: 15.576587187880328\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616400801244241\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012369081905146585\n",
      "Alpha: 77.25117738324616\n",
      "Eta0: 98.86661748227098\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.596378385812231\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012403789306995833\n",
      "Alpha: 77.18730451848846\n",
      "Eta0: 98.81197649813829\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07740405961015323\n",
      "Alpha: 17.22534529274596\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616081771804397\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012422134445045752\n",
      "Alpha: 77.02405853212264\n",
      "Eta0: 98.70775637776231\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5504210098065726\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012493916706935088\n",
      "Alpha: 77.00953934680754\n",
      "Eta0: 98.65752815046643\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4813219593798032\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012523477785223424\n",
      "Alpha: 76.9541688380046\n",
      "Eta0: 98.61015563927128\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5729413194106833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012550500612254763\n",
      "Alpha: 76.90092560282402\n",
      "Eta0: 98.5644179503692\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5399152315119262\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08182138696271112\n",
      "Alpha: 15.148656124464873\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615756681787251\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012606490946525004\n",
      "Alpha: 76.79331489203548\n",
      "Eta0: 98.46924906316114\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5375913359883014\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07875909199274506\n",
      "Alpha: 15.149242736468716\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616330048715005\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012910358535761963\n",
      "Alpha: 76.61185561324994\n",
      "Eta0: 98.37872517285592\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07910862142135874\n",
      "Alpha: 15.422504087606958\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615672608790109\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01276507104948904\n",
      "Alpha: 76.56865637255133\n",
      "Eta0: 98.29332577449053\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5969609363008411\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07953822299397739\n",
      "Alpha: 14.953583302144022\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616024028786179\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012692419725011124\n",
      "Alpha: 76.49343391111589\n",
      "Eta0: 98.20999732601578\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5713282748121961\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07857085924518542\n",
      "Alpha: 14.866363523130017\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616282552091322\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012755838864248071\n",
      "Alpha: 76.35353081822997\n",
      "Eta0: 98.12865619818645\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4117204292039472\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012774231903800498\n",
      "Alpha: 76.33529621056441\n",
      "Eta0: 98.09076412783321\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5508812216514204\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01285843709665524\n",
      "Alpha: 76.31563727779701\n",
      "Eta0: 98.05379185271794\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5109017180610494\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012843638250116331\n",
      "Alpha: 76.220804885622\n",
      "Eta0: 98.0165745489011\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6098328934500075\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012908036098760987\n",
      "Alpha: 76.2192283384781\n",
      "Eta0: 97.97846781767025\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5823944413986434\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012857056534990868\n",
      "Alpha: 76.1221481741785\n",
      "Eta0: 97.9434399655298\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5114896139479324\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.012912735226357724\n",
      "Alpha: 76.10090678395731\n",
      "Eta0: 97.90785114344926\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4745706045838853\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01373583899411182\n",
      "Alpha: 76.1596742280848\n",
      "Eta0: 97.87243293989954\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.49399016435018267\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07854673567364479\n",
      "Alpha: 14.579990556760407\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616727580198719\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013023776499191293\n",
      "Alpha: 75.78141845145794\n",
      "Eta0: 97.79971816379879\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5366701335918606\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07954019042156373\n",
      "Alpha: 17.21030679653856\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615878427638458\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0812289171183432\n",
      "Alpha: 9.17263722064964\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616101310603376\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013067492043336027\n",
      "Alpha: 75.92687548076336\n",
      "Eta0: 97.69265667882527\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5594196992065407\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013095963270169084\n",
      "Alpha: 75.8592731028524\n",
      "Eta0: 97.66040450069713\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4207321221943297\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013162046729884127\n",
      "Alpha: 75.81466972003372\n",
      "Eta0: 97.63104569407305\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5302137635338543\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013134832760023588\n",
      "Alpha: 75.78743497796657\n",
      "Eta0: 97.59645460855985\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5551870293173956\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013892874947985958\n",
      "Alpha: 75.75660086739062\n",
      "Eta0: 97.56451926102243\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6062381095774777\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013206762446990207\n",
      "Alpha: 75.59930577394312\n",
      "Eta0: 97.53343836488992\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5022063325937483\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013263066080133157\n",
      "Alpha: 75.67337240123307\n",
      "Eta0: 97.50408506396155\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5640133769861025\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013212246825475038\n",
      "Alpha: 75.62575535939435\n",
      "Eta0: 97.47276201483301\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5365454549082082\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07871729863574044\n",
      "Alpha: 14.976021532869483\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615954955524265\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0132012241814624\n",
      "Alpha: 75.3839175543619\n",
      "Eta0: 97.40968207580445\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.46134164558060303\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01327504189839908\n",
      "Alpha: 75.54797989468494\n",
      "Eta0: 97.38018413119848\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.48546705868332457\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013301607767378407\n",
      "Alpha: 75.50478873070463\n",
      "Eta0: 97.35197434480206\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5066232287811637\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013303869129918959\n",
      "Alpha: 75.47693191291769\n",
      "Eta0: 97.32220943857844\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5423552623597656\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.076436079515825\n",
      "Alpha: 14.280820175592323\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616386512372339\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013296542323636638\n",
      "Alpha: 75.53238575136056\n",
      "Eta0: 97.26320861805132\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5949335951643565\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013298666254633267\n",
      "Alpha: 75.33249434446014\n",
      "Eta0: 97.2362016341232\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5655167402857187\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01322913075991352\n",
      "Alpha: 75.47790553008772\n",
      "Eta0: 97.20854353365354\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.48490561367708335\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013231630755163419\n",
      "Alpha: 74.90127038908072\n",
      "Eta0: 97.18071829901625\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6141278234551647\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07724775842031016\n",
      "Alpha: 14.029370885265584\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615709043967569\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07000149948756805\n",
      "Alpha: 84.73993423789682\n",
      "Eta0: 68.43480578819987\n",
      "Learning Rate: optimal\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8528448750143888\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07823480130370268\n",
      "Alpha: 12.167134069773837\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616177496317864\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08341175666175342\n",
      "Alpha: 12.23134518428855\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.861631144854257\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013455232126612371\n",
      "Alpha: 75.21935701609787\n",
      "Eta0: 97.0895857954463\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5305765307236022\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013793063564424488\n",
      "Alpha: 75.18890817758712\n",
      "Eta0: 97.0639855607995\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4858960802183079\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013471121863825306\n",
      "Alpha: 75.1771001435449\n",
      "Eta0: 97.0387251415357\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5709163421439135\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013511208307599526\n",
      "Alpha: 75.13756934266716\n",
      "Eta0: 97.01294330975286\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6174849278789525\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01367894561570878\n",
      "Alpha: 74.87906629166956\n",
      "Eta0: 96.98984585639116\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.45733222969372117\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07751399380037842\n",
      "Alpha: 9.877867222409416\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616938704562349\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013697339489494174\n",
      "Alpha: 74.2404743057516\n",
      "Eta0: 96.9382849756314\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5279295987650562\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013574210627936087\n",
      "Alpha: 75.0302246364827\n",
      "Eta0: 96.91267887329174\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5599938215097496\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013328607487810853\n",
      "Alpha: 75.55622364562703\n",
      "Eta0: 96.89148007388512\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4482136344575503\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01359464264616136\n",
      "Alpha: 74.9822579079289\n",
      "Eta0: 96.86791901343602\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6112357810119539\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013609917893165117\n",
      "Alpha: 74.95739480368947\n",
      "Eta0: 96.84552064405607\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.44926640318029376\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014631289664890924\n",
      "Alpha: 75.37394677212235\n",
      "Eta0: 96.8259021643376\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5762171254987453\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07740604679201919\n",
      "Alpha: 11.532313827292763\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616652709503811\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013688756697888292\n",
      "Alpha: 74.8589067704222\n",
      "Eta0: 96.77515575978393\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5053825653385348\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013668512243386615\n",
      "Alpha: 74.86107238854271\n",
      "Eta0: 96.75527158094447\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.49991734789294623\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07721086961505111\n",
      "Alpha: 11.814413291620617\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616025250097635\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01381086815160222\n",
      "Alpha: 74.77738322980707\n",
      "Eta0: 96.70988759531008\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5674426566684324\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013717237201187234\n",
      "Alpha: 74.78398889124634\n",
      "Eta0: 96.6881634685393\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4973890871056235\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013727551979377245\n",
      "Alpha: 74.76268413355865\n",
      "Eta0: 96.66740911955219\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5449623152213731\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07661518839714232\n",
      "Alpha: 11.384008251240502\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615864857223903\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013639936846651534\n",
      "Alpha: 74.61205477945623\n",
      "Eta0: 96.62420881280711\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5445051887591625\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013622626792803053\n",
      "Alpha: 74.79218269210176\n",
      "Eta0: 96.60431142773277\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5017753499161369\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0137819743144044\n",
      "Alpha: 74.68310579966516\n",
      "Eta0: 96.58461009499665\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5095249091678858\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013800332129938727\n",
      "Alpha: 74.66271727179509\n",
      "Eta0: 96.56513770713275\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5518944190360282\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013956349380801484\n",
      "Alpha: 74.72638571254113\n",
      "Eta0: 96.54661226111885\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5619871705239371\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013823874202142013\n",
      "Alpha: 74.61278170323271\n",
      "Eta0: 96.52603872777439\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5268982809276095\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013629634356721055\n",
      "Alpha: 74.75373196329926\n",
      "Eta0: 96.70504113352966\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5791211070505917\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013710279189324022\n",
      "Alpha: 74.76972518203189\n",
      "Eta0: 96.68384130788664\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5832540167168568\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08482144256205508\n",
      "Alpha: 17.2099500944261\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616480655725641\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013788679819587342\n",
      "Alpha: 74.5502375304636\n",
      "Eta0: 96.44909771449235\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5102209024123113\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013831167299428819\n",
      "Alpha: 74.3638295576568\n",
      "Eta0: 96.43030495196865\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5909858092166111\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01389456330714438\n",
      "Alpha: 74.50407248025981\n",
      "Eta0: 96.41287277336966\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5566031263115713\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013915022156184606\n",
      "Alpha: 74.48086626408471\n",
      "Eta0: 96.39573025529596\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5507495141240472\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013927673690696852\n",
      "Alpha: 74.46185871626962\n",
      "Eta0: 96.3778043910819\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5059281749877621\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01380178367158849\n",
      "Alpha: 74.62664363480938\n",
      "Eta0: 96.5467438746566\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6064226007787016\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015491073656818522\n",
      "Alpha: 75.03529196687879\n",
      "Eta0: 96.34771437031712\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5098862665433911\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013963012215275749\n",
      "Alpha: 74.40931913224205\n",
      "Eta0: 96.32659206437508\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.41676105782305545\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08466697479986894\n",
      "Alpha: 2.9329023155926\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8620489659805234\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013784359666695934\n",
      "Alpha: 73.7609836599938\n",
      "Eta0: 96.28984907409138\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5278169804996601\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013945620071452539\n",
      "Alpha: 74.32293089943849\n",
      "Eta0: 96.27507368295181\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07717163833574793\n",
      "Alpha: 11.031658554142787\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.861630463012833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013930893381512916\n",
      "Alpha: 74.2926967732869\n",
      "Eta0: 96.24089928775798\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4997594171258462\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01394333610400863\n",
      "Alpha: 74.20588045125022\n",
      "Eta0: 96.22618205998198\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5240532264267826\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014026401525457526\n",
      "Alpha: 74.61273312259272\n",
      "Eta0: 96.20907830629702\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5376255620848064\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014069092667534679\n",
      "Alpha: 74.28935293600018\n",
      "Eta0: 96.19409545709134\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4980575704409003\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013988536614389887\n",
      "Alpha: 74.16222388988837\n",
      "Eta0: 96.17859541417627\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.590098493252417\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014080418277405788\n",
      "Alpha: 74.24535250950146\n",
      "Eta0: 96.16339076461676\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5551581121008698\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014044883296405495\n",
      "Alpha: 74.23260117511384\n",
      "Eta0: 96.14954025330336\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5961980756479487\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013965274699950717\n",
      "Alpha: 74.32924677714374\n",
      "Eta0: 96.1322262267864\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6115703051438921\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014129450021494926\n",
      "Alpha: 74.20212015894123\n",
      "Eta0: 96.11730133201203\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5134984669754944\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01413929981895915\n",
      "Alpha: 74.19338588669932\n",
      "Eta0: 96.10441071025917\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5551168220811659\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014145006594912277\n",
      "Alpha: 74.17916280285922\n",
      "Eta0: 96.08990040269096\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4748074482671108\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08024844019770623\n",
      "Alpha: 15.421510705799426\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615951717882515\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014159682055297733\n",
      "Alpha: 74.1461500565337\n",
      "Eta0: 96.06019912799455\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6008906387523025\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014167032782820178\n",
      "Alpha: 74.1444271306179\n",
      "Eta0: 96.04623540070541\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.500333371312064\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014176433180772089\n",
      "Alpha: 74.11771037279429\n",
      "Eta0: 96.0328206879525\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014185688981141222\n",
      "Alpha: 74.1032029462133\n",
      "Eta0: 96.01920076820386\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5343361014167808\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01400919132453146\n",
      "Alpha: 73.91853922084235\n",
      "Eta0: 96.00495829252323\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5373028657596292\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014205142618473228\n",
      "Alpha: 74.07709800923516\n",
      "Eta0: 95.9930570706077\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5987893986127139\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.03607525962425858\n",
      "Alpha: 34.91301965169031\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616614610343695\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014226417028681911\n",
      "Alpha: 74.04469364910095\n",
      "Eta0: 95.9641821956346\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4944551978079097\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01423346049225571\n",
      "Alpha: 74.03604377045743\n",
      "Eta0: 95.95144069434221\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5208532785431649\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014103796432721546\n",
      "Alpha: 73.90061950882634\n",
      "Eta0: 95.93846432320363\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014193916205281617\n",
      "Alpha: 73.26757563890769\n",
      "Eta0: 95.9262983520227\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5221037442429529\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01445126898945553\n",
      "Alpha: 73.96078984550003\n",
      "Eta0: 95.91320293142329\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6055014900988792\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01428276177777175\n",
      "Alpha: 73.9819820592144\n",
      "Eta0: 95.90060157819381\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.577425221299862\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014261176414451768\n",
      "Alpha: 73.82471914986235\n",
      "Eta0: 95.89110290474319\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5219762114163137\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01429202188565095\n",
      "Alpha: 73.96593735298158\n",
      "Eta0: 95.87599003520637\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5241940252753955\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014547929676066263\n",
      "Alpha: 73.492773604275\n",
      "Eta0: 95.86505263020112\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5580630028278837\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01437367660626074\n",
      "Alpha: 73.63447892948834\n",
      "Eta0: 95.85785559853257\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5374314492513124\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014318942313104351\n",
      "Alpha: 73.93564332687421\n",
      "Eta0: 95.8435423176406\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5661741947020077\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014368042439144045\n",
      "Alpha: 73.20984456646899\n",
      "Eta0: 95.84562329504412\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.511119101561681\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014197018381640388\n",
      "Alpha: 73.84043395668498\n",
      "Eta0: 95.81717481550213\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.48451631099647613\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014338328789625881\n",
      "Alpha: 73.94089131081395\n",
      "Eta0: 95.80538149695758\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.43062928485861657\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01441371566724717\n",
      "Alpha: 74.32596764632467\n",
      "Eta0: 95.79364799352449\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5277324397193449\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014390676434406308\n",
      "Alpha: 73.14660106437525\n",
      "Eta0: 95.77382359276329\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6143564898884799\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014373518642005433\n",
      "Alpha: 73.86404133601835\n",
      "Eta0: 95.77139866739462\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5161956450108719\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014181378676655363\n",
      "Alpha: 73.90375808364993\n",
      "Eta0: 95.75984913015522\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4533638542748868\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014418594676368633\n",
      "Alpha: 73.83614146677405\n",
      "Eta0: 95.73544035955037\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4722239600824378\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01439695998311282\n",
      "Alpha: 73.8116017642845\n",
      "Eta0: 95.73858452957921\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5240356782847112\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015494172254103974\n",
      "Alpha: 73.30410982441471\n",
      "Eta0: 95.72384193663044\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.592996237984169\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01406499858093202\n",
      "Alpha: 73.73683850396161\n",
      "Eta0: 95.71719439872408\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5287058555889468\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014361404159381738\n",
      "Alpha: 73.79581614188778\n",
      "Eta0: 95.70647147480808\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6070659363852647\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014253554789894035\n",
      "Alpha: 73.89901724200313\n",
      "Eta0: 95.69575670882233\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6387825605495755\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013077303164780353\n",
      "Alpha: 73.59968297581194\n",
      "Eta0: 95.68461619570708\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4483852071644155\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01452255874590901\n",
      "Alpha: 73.77837614375883\n",
      "Eta0: 95.67543267599649\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5549539859524945\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014380095659295631\n",
      "Alpha: 73.32787603917791\n",
      "Eta0: 95.66472337135664\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.601618647780259\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01446178756233746\n",
      "Alpha: 73.75555247889497\n",
      "Eta0: 95.65488374395042\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4695837096250197\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014454017900692244\n",
      "Alpha: 73.74454702483844\n",
      "Eta0: 95.6447223268573\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6203015306089775\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014489236504316935\n",
      "Alpha: 73.78392331624185\n",
      "Eta0: 95.63478528095753\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.581369188872393\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014484670758827008\n",
      "Alpha: 73.29709178037359\n",
      "Eta0: 95.62492828322678\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5245032014138302\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.052070743279072816\n",
      "Alpha: 15.713386410914008\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615992227665337\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014519331162803479\n",
      "Alpha: 73.71167359178831\n",
      "Eta0: 95.60443087226038\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6237123704839063\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.016056802618902258\n",
      "Alpha: 73.7044379390303\n",
      "Eta0: 95.59373913814169\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5637555299211688\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014219953425303326\n",
      "Alpha: 73.70189498264973\n",
      "Eta0: 95.58656833942904\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4796339111075533\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 62.312241864129774\n",
      "Eta0: 45.89710461824087\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.4829511529121919\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 62.225161406032235\n",
      "Eta0: 45.860485239275796\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.6103235642281123\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014621146282375177\n",
      "Alpha: 73.86304658655762\n",
      "Eta0: 95.57082397808485\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6408265079952495\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014516143381415901\n",
      "Alpha: 73.66161456360031\n",
      "Eta0: 95.56465323479901\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6163933587276055\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014542200187564466\n",
      "Alpha: 73.679857188382\n",
      "Eta0: 95.55547654700064\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5697950609354533\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01461622080938132\n",
      "Alpha: 73.952533335639\n",
      "Eta0: 95.54827293334466\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5068719396213895\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014479960445482045\n",
      "Alpha: 73.63562028739798\n",
      "Eta0: 95.5380694569991\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6240897953725523\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0753213389661822\n",
      "Alpha: 10.840365025732513\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8617041306395558\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.0669914812862461\n",
      "Alpha: 10.358484597352692\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.861542360478983\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01457603458273159\n",
      "Alpha: 73.61959354404061\n",
      "Eta0: 95.50919988534052\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5205094189225895\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013872706283379131\n",
      "Alpha: 74.94370527785834\n",
      "Eta0: 95.5010103340247\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5363396662284262\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014609093698697713\n",
      "Alpha: 73.60925140752941\n",
      "Eta0: 95.49188652277427\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5578619634752543\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.07299333629800746\n",
      "Alpha: 10.201530931090566\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8615626638976653\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014619611957296157\n",
      "Alpha: 73.66546399230621\n",
      "Eta0: 95.47417992742271\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5962980754201768\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014645022138827173\n",
      "Alpha: 73.57896481104835\n",
      "Eta0: 95.4651993791192\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5297453124724788\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014624164077957014\n",
      "Alpha: 73.55947786601538\n",
      "Eta0: 95.46593168236316\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.43922998452580897\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014553108257018483\n",
      "Alpha: 73.63921455720171\n",
      "Eta0: 95.56266377715531\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.661240564059248\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014634123497074257\n",
      "Alpha: 73.55663547915478\n",
      "Eta0: 95.44094886981992\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5666661903506823\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014654278739787207\n",
      "Alpha: 73.57072004460144\n",
      "Eta0: 95.43256308988599\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014646970323910375\n",
      "Alpha: 73.54164520813416\n",
      "Eta0: 95.4244776993031\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.46882342949817685\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014648683575162254\n",
      "Alpha: 73.5357285224516\n",
      "Eta0: 95.41669365648158\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5106535976841249\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01404801474226919\n",
      "Alpha: 73.32196896609864\n",
      "Eta0: 95.40795409827015\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5545871989820963\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014193117468607234\n",
      "Alpha: 73.80453214671128\n",
      "Eta0: 95.39021099872951\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6282971480170672\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01466609498100215\n",
      "Alpha: 73.51420462733157\n",
      "Eta0: 95.39353387125249\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5470647512771908\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014738828804931362\n",
      "Alpha: 73.51122683108936\n",
      "Eta0: 95.38477709147395\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5330859153989471\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014028892587918515\n",
      "Alpha: 73.25708048348876\n",
      "Eta0: 95.37670269179704\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5189362272521071\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014685733114383913\n",
      "Alpha: 73.4927754522016\n",
      "Eta0: 95.36973991158071\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5005209971742733\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014698629936511083\n",
      "Alpha: 73.53066279046179\n",
      "Eta0: 95.36298968975696\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.43553760111479384\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014740445805722781\n",
      "Alpha: 73.51011871026425\n",
      "Eta0: 95.35485750872189\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5308101985193917\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01473360536174219\n",
      "Alpha: 73.49389886232981\n",
      "Eta0: 95.34749883841326\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5706277849732846\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014666439789595464\n",
      "Alpha: 74.29932648371471\n",
      "Eta0: 95.338847583015\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6610367886363749\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01472237269944235\n",
      "Alpha: 73.45801306273388\n",
      "Eta0: 95.33268966668014\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5904342192507719\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01472436098163092\n",
      "Alpha: 73.45136467312338\n",
      "Eta0: 95.32378069094092\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4836768630472094\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014722656961768015\n",
      "Alpha: 73.47814136738926\n",
      "Eta0: 95.31863393168635\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014737059016773489\n",
      "Alpha: 73.44395437712417\n",
      "Eta0: 95.31068508629895\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5667874015629705\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014353550934942895\n",
      "Alpha: 73.50615049362453\n",
      "Eta0: 95.40733295865074\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5028003104339978\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014802282430899\n",
      "Alpha: 73.47773338471522\n",
      "Eta0: 95.2968985718609\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5775295392453645\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014749588727531076\n",
      "Alpha: 73.420315734069\n",
      "Eta0: 95.28991724310555\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6245142757513064\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.013717879102426318\n",
      "Alpha: 74.60758821530565\n",
      "Eta0: 95.27907788394108\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5689321694166833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014762229030940162\n",
      "Alpha: 73.46309376465372\n",
      "Eta0: 95.2755807260195\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5522552284038612\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014781206054918889\n",
      "Alpha: 73.4239901605196\n",
      "Eta0: 95.26734699927644\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5621843722368526\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014772087297136673\n",
      "Alpha: 73.39845868871595\n",
      "Eta0: 95.26207380142453\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4253278209320439\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014822292797085067\n",
      "Alpha: 73.39236449654153\n",
      "Eta0: 95.25562898221797\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.46586361262501996\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014806607194802441\n",
      "Alpha: 73.40185998716665\n",
      "Eta0: 95.24915564609842\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5037081367476741\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014853994844797973\n",
      "Alpha: 73.53073587844126\n",
      "Eta0: 95.34067766260901\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5386328716521532\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014790163548034863\n",
      "Alpha: 73.37335312496998\n",
      "Eta0: 95.23504138656197\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5967038533842043\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014772516047975666\n",
      "Alpha: 73.32617128930038\n",
      "Eta0: 95.22956386345177\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5307268401839909\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014801371022709784\n",
      "Alpha: 73.37067496823941\n",
      "Eta0: 95.22297744765218\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5458891835854952\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014808023438532584\n",
      "Alpha: 73.3532327440716\n",
      "Eta0: 95.21618477276098\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.553510718251336\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014865702085466882\n",
      "Alpha: 73.33274070428523\n",
      "Eta0: 95.21126785511417\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5533354038950146\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014756507033895425\n",
      "Alpha: 73.17770369996805\n",
      "Eta0: 95.20283446805091\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5842985057894025\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014823391340831169\n",
      "Alpha: 73.33930518109938\n",
      "Eta0: 95.19677312241298\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.535824951748268\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015723447217572233\n",
      "Alpha: 70.46609028812452\n",
      "Eta0: 95.11549266495315\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014844388408692498\n",
      "Alpha: 73.33055336236119\n",
      "Eta0: 95.18446414852674\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5944875910832416\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01441231784623093\n",
      "Alpha: 72.8795242172571\n",
      "Eta0: 95.17695372173966\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01492702684190956\n",
      "Alpha: 73.62449277527439\n",
      "Eta0: 95.17105072550963\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4976552381605146\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01476362160801022\n",
      "Alpha: 73.37740883793792\n",
      "Eta0: 95.25872952966347\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5671490343423283\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014609506351032028\n",
      "Alpha: 72.47075308164638\n",
      "Eta0: 95.15793425307987\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5947152082458537\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014857801501970483\n",
      "Alpha: 73.29772470062741\n",
      "Eta0: 95.15372967407744\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4929559155566345\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014781466736756147\n",
      "Alpha: 73.36910723347562\n",
      "Eta0: 95.23949820795917\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014874695513117505\n",
      "Alpha: 73.26948219791053\n",
      "Eta0: 95.14104150106843\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.504578233315138\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014867238222863961\n",
      "Alpha: 73.287366783666\n",
      "Eta0: 95.13574155476627\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5270277940152054\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014804325622041314\n",
      "Alpha: 73.34751601322145\n",
      "Eta0: 95.22104146838349\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5602521723794323\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014870215329411546\n",
      "Alpha: 73.2669969958208\n",
      "Eta0: 95.12453862157206\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6140711966778364\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014819848651877252\n",
      "Alpha: 73.35696769796137\n",
      "Eta0: 95.20754002939175\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4589059086039924\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015081857519394809\n",
      "Alpha: 73.14859026784649\n",
      "Eta0: 95.20069080095894\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5445220349372132\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014814439970890991\n",
      "Alpha: 73.32031701061922\n",
      "Eta0: 95.19614660027248\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5591809216791844\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014891030061568835\n",
      "Alpha: 73.25078190997985\n",
      "Eta0: 95.10101043909506\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6211775006145531\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: hinge\n",
      "Tolerance: 0.08181304063778223\n",
      "Alpha: 4.201739115450653\n",
      "Eta0: 100.0\n",
      "Learning Rate: adaptive\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.8616470593801849\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014910726331938549\n",
      "Alpha: 73.23838601052776\n",
      "Eta0: 95.08954142385751\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5628101378448833\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014863336512096148\n",
      "Alpha: 73.26154237484837\n",
      "Eta0: 95.12896976202717\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4976187609653222\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014983862653988812\n",
      "Alpha: 73.22158386234936\n",
      "Eta0: 95.0793275803613\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5882628477529904\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014498655228233447\n",
      "Alpha: 74.5608302265677\n",
      "Eta0: 95.03839944763072\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015381472967684634\n",
      "Alpha: 74.3709459550023\n",
      "Eta0: 95.03546140359987\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5472786018578254\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.016532220509456387\n",
      "Alpha: 71.99651096265347\n",
      "Eta0: 94.99302700051786\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4054494159623472\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014034588642142194\n",
      "Alpha: 73.19468986472323\n",
      "Eta0: 95.02635300010655\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5139179734777573\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014902172992365967\n",
      "Alpha: 73.11863147024484\n",
      "Eta0: 95.05078913584705\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.523010082111388\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014901166042804616\n",
      "Alpha: 73.1897559633393\n",
      "Eta0: 95.04606843048545\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.49924634443373966\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014907421843276446\n",
      "Alpha: 73.15751050507524\n",
      "Eta0: 95.04059748559807\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4626797607593814\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014950639818282955\n",
      "Alpha: 73.1925354479066\n",
      "Eta0: 95.03528662950261\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5543972366326068\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014985013526162874\n",
      "Alpha: 73.24286670375042\n",
      "Eta0: 95.02982169373705\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5468769317682894\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014901785103576558\n",
      "Alpha: 73.22801569835889\n",
      "Eta0: 95.10764111369885\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4277576051330707\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01488852581462291\n",
      "Alpha: 73.23274560075082\n",
      "Eta0: 95.10218534363818\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.4619686583612253\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014921457838135959\n",
      "Alpha: 73.24166899854187\n",
      "Eta0: 95.01392588991071\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5960957199782849\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.543350213721695\n",
      "Eta0: 44.556739943371426\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.6115719875801403\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.39167661180235\n",
      "Eta0: 44.82098493345734\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5734864997609364\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.47649488187342\n",
      "Eta0: 44.49575414252025\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.40773946626054\n",
      "Eta0: 44.43450651272942\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5775872406699145\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01490872066276882\n",
      "Alpha: 73.22058921154877\n",
      "Eta0: 95.0686051174359\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014438777395835314\n",
      "Alpha: 73.28611232859964\n",
      "Eta0: 94.99192483654092\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.49435414813338\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014996765523358817\n",
      "Alpha: 73.13480849450526\n",
      "Eta0: 94.98734016896326\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.47192759191497263\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.0149994685483204\n",
      "Alpha: 73.14506370644803\n",
      "Eta0: 94.98306807072868\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6318246691614123\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01500131534017319\n",
      "Alpha: 73.14518413514328\n",
      "Eta0: 94.97746689237171\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5613002929786967\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015000944918440505\n",
      "Alpha: 73.12969619309449\n",
      "Eta0: 94.97262508664721\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.529372795903053\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.105514238856316\n",
      "Eta0: 44.59212795453428\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5038951599962521\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.072729689548105\n",
      "Eta0: 44.57079572990569\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5698800937776999\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.05352015619097\n",
      "Eta0: 44.550341618023594\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.42551808862386725\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015029480577235743\n",
      "Alpha: 73.10096339901516\n",
      "Eta0: 94.95583586167167\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6132795243896704\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014983303640950539\n",
      "Alpha: 73.10193092388144\n",
      "Eta0: 94.95119827915292\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5614904986688077\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.94197077289713\n",
      "Eta0: 44.509397873275624\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5862517157229131\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 61.10868583381891\n",
      "Eta0: 44.493007798645024\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.47734626572785777\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.98607044419419\n",
      "Eta0: 44.475785962608896\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.49815344230091685\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.97782992211073\n",
      "Eta0: 44.460342669399225\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.4263157936738076\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.94294669324092\n",
      "Eta0: 44.44522891073007\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.4686472254055866\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.88978973922856\n",
      "Eta0: 44.430959894374034\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5485747536689126\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.91068573336077\n",
      "Eta0: 44.415782007465026\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5658318257887519\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.88130951182866\n",
      "Eta0: 44.40276862142717\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.48528277331647546\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.97722455463599\n",
      "Eta0: 44.388751759285924\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015054349628854014\n",
      "Alpha: 73.0586516421233\n",
      "Eta0: 94.90745758230244\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.49682516453250997\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015035400972625219\n",
      "Alpha: 73.0883082134884\n",
      "Eta0: 94.90289485859219\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6166732543882633\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014870487928658024\n",
      "Alpha: 73.0715541950992\n",
      "Eta0: 94.89868603566597\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5928544409728188\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015065883235755018\n",
      "Alpha: 73.0622311236062\n",
      "Eta0: 94.8940757986952\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.6197266028924464\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.85298060963519\n",
      "Eta0: 44.33163687188147\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5367140784578717\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.90063084113342\n",
      "Eta0: 44.31765559995477\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.525894116006202\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015087499101962215\n",
      "Alpha: 73.04697350244709\n",
      "Eta0: 94.881603666378\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.45892639638906835\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.8127916569393\n",
      "Eta0: 44.2953980258417\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5714450212663719\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.64063269361295\n",
      "Eta0: 44.28297822913804\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.896992580283495\n",
      "Eta0: 44.2786007968758\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.44558327499586325\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.014761321896590369\n",
      "Alpha: 73.02450514908433\n",
      "Eta0: 94.93197883619285\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5248437540226863\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: modified_huber\n",
      "Tolerance: 0.0001\n",
      "Alpha: 60.78329385702965\n",
      "Eta0: 44.24834897293714\n",
      "Learning Rate: constant\n",
      "Oversampling Method: none\n",
      "----------------\n",
      "Results: -0.4723891282382153\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015008550837256357\n",
      "Alpha: 72.9851566444867\n",
      "Eta0: 94.85860132084993\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5162244630631235\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.015098577862412973\n",
      "Alpha: 73.02307392390298\n",
      "Eta0: 94.85214807642701\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5153041131920348\n",
      "================\n",
      "================\n",
      "Configuration:\n",
      "Loss: log_loss\n",
      "Tolerance: 0.01510281597666182\n",
      "Alpha: 73.02149197427147\n",
      "Eta0: 94.84828897201369\n",
      "Learning Rate: constant\n",
      "Oversampling Method: ada\n",
      "----------------\n",
      "Results: -0.5\n",
      "================\n",
      "Best parameters: ['log_loss', 0.0001, 36.078762290296936, 0.004036691027092978, 'adaptive', 'none']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "import sys\n",
    "# Define the search space\n",
    "search_space = [\n",
    "    Categorical(['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron'], name='loss'),\n",
    "    Real(0.0001, 100, name='alpha'),\n",
    "    Real(0.0, 100.0, name='eta0'),\n",
    "    Real(0.0001, 0.1, name='tol'),\n",
    "    Categorical(['constant', 'optimal', 'invscaling', 'adaptive'], name='learning_rate'),\n",
    "    Categorical(['none', 'ada'], name='oversampling_method')\n",
    "]\n",
    "\n",
    "# Define your objective function (e.g., maximizing accuracy)\n",
    "@use_named_args(search_space)\n",
    "def objective_function(loss, alpha, eta0, tol, learning_rate, oversampling_method):\n",
    "    print(\"================\")\n",
    "    print(\"Configuration:\")\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Tolerance:\", tol)\n",
    "    print(\"Alpha:\", alpha)\n",
    "    print(\"Eta0:\", eta0)\n",
    "    print(\"Learning Rate:\", learning_rate)\n",
    "    print(\"Oversampling Method:\", oversampling_method)\n",
    "    print(\"----------------\")\n",
    "    try:\n",
    "        if oversampling_method == 'none':\n",
    "            X = loans_train_df.loc[:, loans_train_df.columns != \"loan_status\"]\n",
    "            y = loans_train_df[\"loan_status\"]\n",
    "        elif oversampling_method == 'ada':\n",
    "            X = loans_train_ada_df.loc[:, loans_train_ada_df.columns != \"loan_status\"]\n",
    "            y = loans_train_ada_df[\"loan_status\"]\n",
    "            \n",
    "        model = SGDClassifier(class_weight='balanced', loss=loss, alpha=alpha, eta0=eta0, max_iter=200, tol=tol, learning_rate=learning_rate)\n",
    "        roc_auc = cross_val_score(model, X, y, cv=3, scoring='roc_auc').mean()\n",
    "\n",
    "        print(\"Results:\", -roc_auc)\n",
    "        print(\"================\")\n",
    "        df_hyper_tuning.loc[len(df_hyper_tuning.index)] = [loss, alpha, eta0, tol, learning_rate, oversampling_method, roc_auc] \n",
    "        return -roc_auc\n",
    "    except:\n",
    "        print(\"Invalid Config\")\n",
    "        return 100000\n",
    "        \n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "res = gp_minimize(objective_function, search_space, n_calls=500)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", res.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814afa5-f76d-4bff-885e-6dfc91a4074f",
   "metadata": {},
   "source": [
    "These tuning results are saved in a dataframe. The hyperparameter tuning results for all models can be viewed in the <b>./hyper_tuning</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3add7e-74c4-4f8f-bada-596f5a363ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta0</th>\n",
       "      <th>tol</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>oversampling_method</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>36.078762</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.904657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.904645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>48.278881</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.904645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>50.937640</td>\n",
       "      <td>0.007440</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.904645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>87.314916</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>none</td>\n",
       "      <td>0.904644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>modified_huber</td>\n",
       "      <td>43.190278</td>\n",
       "      <td>51.720587</td>\n",
       "      <td>0.028410</td>\n",
       "      <td>constant</td>\n",
       "      <td>none</td>\n",
       "      <td>0.417533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>74.409319</td>\n",
       "      <td>96.326592</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>constant</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.416761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>76.353531</td>\n",
       "      <td>98.128656</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>constant</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.411720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>71.996511</td>\n",
       "      <td>94.993027</td>\n",
       "      <td>0.016532</td>\n",
       "      <td>constant</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.405449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>89.860774</td>\n",
       "      <td>81.846686</td>\n",
       "      <td>0.046664</td>\n",
       "      <td>constant</td>\n",
       "      <td>none</td>\n",
       "      <td>0.320988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss      alpha        eta0       tol learning_rate  \\\n",
       "117        log_loss   0.000100   36.078762  0.004037      adaptive   \n",
       "72         log_loss   0.000100  100.000000  0.005083      adaptive   \n",
       "76         log_loss   0.000100   48.278881  0.007399      adaptive   \n",
       "80         log_loss   0.000100   50.937640  0.007440      adaptive   \n",
       "159        log_loss   0.000100   87.314916  0.000100      adaptive   \n",
       "..              ...        ...         ...       ...           ...   \n",
       "67   modified_huber  43.190278   51.720587  0.028410      constant   \n",
       "315        log_loss  74.409319   96.326592  0.013963      constant   \n",
       "245        log_loss  76.353531   98.128656  0.012756      constant   \n",
       "448        log_loss  71.996511   94.993027  0.016532      constant   \n",
       "178        log_loss  89.860774   81.846686  0.046664      constant   \n",
       "\n",
       "    oversampling_method   roc_auc  \n",
       "117                none  0.904657  \n",
       "72                 none  0.904645  \n",
       "76                 none  0.904645  \n",
       "80                 none  0.904645  \n",
       "159                none  0.904644  \n",
       "..                  ...       ...  \n",
       "67                 none  0.417533  \n",
       "315                 ada  0.416761  \n",
       "245                 ada  0.411720  \n",
       "448                 ada  0.405449  \n",
       "178                none  0.320988  \n",
       "\n",
       "[497 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyper_tuning.sort_values(by=['roc_auc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3257b27f-4806-488b-80ba-cdeca8d548bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_tuning.to_csv('hyper_tuning/sgd_hyper_tuning.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f522d-e7c4-486d-a9f0-016b8e0b5597",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1136aa-68e4-455e-8a6b-2efb86d31f58",
   "metadata": {},
   "source": [
    "The results showed that the following configuration produced the best results:\n",
    "\n",
    "<ol>\n",
    "    <li><b>criterion: </b>entropy</li>\n",
    "    <li><b>splitter: </b>best</li>\n",
    "    <li><b>max_depth: </b>42</li>\n",
    "    <li><b>min_samples_split: </b>0.042438 = 4.2438%</li>\n",
    "    <li><b>max_features: </b>log2</li>\n",
    "    <li><b>oversampling_method: </b>none</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3effbdf-f86a-44c6-9d28-a036d9b123fc",
   "metadata": {},
   "source": [
    "# 4. Exporting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b948ae-dfbb-4b8e-83ec-b9a64b1e51b1",
   "metadata": {},
   "source": [
    "The model with the best configuration found during the hyperparameter tuning process is saved in the <b>./outputs</b> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01569abe-8638-489d-974e-1160305ef72c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(class_weight='balanced', loss=res.x[0], alpha=res.x[1], eta0=res.x[2], max_iter=200, tol=res.x[3], learning_rate=res.x[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4aeb4b-d63c-4a1b-9e81-f837471ae270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9046316593278497\n"
     ]
    }
   ],
   "source": [
    "if res.x[5] == 'none':\n",
    "    X = loans_train_df.loc[:, loans_train_df.columns != \"loan_status\"]\n",
    "    y = loans_train_df[\"loan_status\"]\n",
    "elif res.x[5] == 'ada':\n",
    "    X = loans_train_ada_df.loc[:, loans_train_ada_df.columns != \"loan_status\"]\n",
    "    y = loans_train_ada_df[\"loan_status\"]\n",
    "    \n",
    "clf.fit(X,y)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = cross_val_score(clf, X, y, cv=3, scoring='roc_auc').mean()\n",
    "print(\"Validation AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9e1611-5a3e-401c-8f66-ecf6babb66cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./outputs/sgd_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "clf.fit(X,y)\n",
    "dump(clf, './outputs/sgd_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9036f3e-f336-4314-b62b-b4d1d8543855",
   "metadata": {},
   "source": [
    "# 5. Fitting into Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201091e-1908-4486-bdaa-f908c34de7d3",
   "metadata": {},
   "source": [
    "Finally, we can now generate the predictions made by the DTC on the test data. This is done by isolating the features of the test samples, forwarding it to the DTC for prediction, and appending the predicted class labels with the corresponding IDs of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fca2e220-4773-4e7e-9ff4-d06e285bfa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58645</td>\n",
       "      <td>-0.755638</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>-0.117198</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>1.455666</td>\n",
       "      <td>2.189522</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58646</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.127233</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58647</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>-1.418731</td>\n",
       "      <td>0.403331</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>1.748450</td>\n",
       "      <td>-0.318861</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58648</td>\n",
       "      <td>0.905387</td>\n",
       "      <td>-0.300610</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620670</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58649</td>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.259932</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>1.573370</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>97738</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.332883</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-1.117500</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>97739</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.389963</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-1.782989</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>97740</td>\n",
       "      <td>3.895232</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>-1.043084</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0</td>\n",
       "      <td>2.637868</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>97741</td>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.019656</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>2.516703</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>97742</td>\n",
       "      <td>0.573182</td>\n",
       "      <td>-0.531228</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>1.839088</td>\n",
       "      <td>-0.108264</td>\n",
       "      <td>3.062003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.018914</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  person_age  person_income  person_emp_length  loan_amnt  \\\n",
       "0      58645   -0.755638       0.404383          -0.117198   2.836600   \n",
       "1      58646   -0.257331       1.127233           0.601227   0.140622   \n",
       "2      58647   -0.257331      -1.418731           0.403331  -0.937769   \n",
       "3      58648    0.905387      -0.300610           0.169270  -0.398573   \n",
       "4      58649   -0.257331       1.259932           0.923860   1.039281   \n",
       "...      ...         ...            ...                ...        ...   \n",
       "39093  97738   -0.921741      -1.332883          -0.486519  -1.117500   \n",
       "39094  97739   -0.921741      -0.389963           0.601227  -0.398573   \n",
       "39095  97740    3.895232       0.098465          -1.896898   1.039281   \n",
       "39096  97741   -0.921741      -1.019656           0.169270   0.859550   \n",
       "39097  97742    0.573182      -0.531228           0.601227   1.839088   \n",
       "\n",
       "       loan_int_rate  loan_percent_income  cb_person_default_on_file  \\\n",
       "0           1.455666             2.189522                          0   \n",
       "1           0.722635            -0.646041                          1   \n",
       "2           1.748450            -0.318861                          1   \n",
       "3          -0.470628            -0.209801                          0   \n",
       "4           1.573370            -0.100741                          1   \n",
       "...              ...                  ...                        ...   \n",
       "39093       0.044689            -0.646041                          0   \n",
       "39094      -1.782989            -0.100741                          0   \n",
       "39095      -1.043084             0.989861                          0   \n",
       "39096       1.425586             2.516703                          1   \n",
       "39097      -0.108264             3.062003                          0   \n",
       "\n",
       "       cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  ...  \\\n",
       "0                       -1.364513                               0  ...   \n",
       "1                       -0.266122                               1  ...   \n",
       "2                       -1.364513                               0  ...   \n",
       "3                        0.620670                               0  ...   \n",
       "4                       -0.266122                               1  ...   \n",
       "...                           ...                             ...  ...   \n",
       "39093                   -0.266122                               1  ...   \n",
       "39094                   -0.721995                               1  ...   \n",
       "39095                    2.637868                               1  ...   \n",
       "39096                   -0.266122                               1  ...   \n",
       "39097                    1.018914                               0  ...   \n",
       "\n",
       "       LOAN_GRADE_B  LOAN_GRADE_C  LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  \\\n",
       "0                 0             0             0             0             1   \n",
       "1                 0             1             0             0             0   \n",
       "2                 0             0             0             1             0   \n",
       "3                 0             0             0             0             0   \n",
       "4                 0             0             1             0             0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "39093             1             0             0             0             0   \n",
       "39094             0             0             0             0             0   \n",
       "39095             0             0             0             0             0   \n",
       "39096             0             0             1             0             0   \n",
       "39097             1             0             0             0             0   \n",
       "\n",
       "       LOAN_GRADE_G  CB_PERSON_CRED_HIST_LENGTH_11_17  \\\n",
       "0                 0                                 0   \n",
       "1                 0                                 0   \n",
       "2                 0                                 0   \n",
       "3                 0                                 0   \n",
       "4                 0                                 0   \n",
       "...             ...                               ...   \n",
       "39093             0                                 0   \n",
       "39094             0                                 0   \n",
       "39095             0                                 0   \n",
       "39096             0                                 0   \n",
       "39097             0                                 0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_18_above  CB_PERSON_CRED_HIST_LENGTH_5_10  \\\n",
       "0                                        0                                0   \n",
       "1                                        0                                0   \n",
       "2                                        0                                0   \n",
       "3                                        0                                1   \n",
       "4                                        0                                0   \n",
       "...                                    ...                              ...   \n",
       "39093                                    0                                0   \n",
       "39094                                    0                                0   \n",
       "39095                                    1                                0   \n",
       "39096                                    0                                0   \n",
       "39097                                    0                                1   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                       1  \n",
       "1                                       1  \n",
       "2                                       1  \n",
       "3                                       0  \n",
       "4                                       1  \n",
       "...                                   ...  \n",
       "39093                                   1  \n",
       "39094                                   1  \n",
       "39095                                   0  \n",
       "39096                                   1  \n",
       "39097                                   0  \n",
       "\n",
       "[39098 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import Testing Dataset\n",
    "loans_test_df = pd.read_csv('./outputs/cleaned_loans_test.csv')\n",
    "loans_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4c7777-94b9-42ab-8e73-95876e9bffa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_MORTGAGE</th>\n",
       "      <th>PERSON_HOME_OWNERSHIP_OTHER</th>\n",
       "      <th>...</th>\n",
       "      <th>LOAN_GRADE_B</th>\n",
       "      <th>LOAN_GRADE_C</th>\n",
       "      <th>LOAN_GRADE_D</th>\n",
       "      <th>LOAN_GRADE_E</th>\n",
       "      <th>LOAN_GRADE_F</th>\n",
       "      <th>LOAN_GRADE_G</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_11_17</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_18_above</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_10</th>\n",
       "      <th>CB_PERSON_CRED_HIST_LENGTH_5_below</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.755638</td>\n",
       "      <td>0.404383</td>\n",
       "      <td>-0.117198</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>1.455666</td>\n",
       "      <td>2.189522</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.127233</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>-1.418731</td>\n",
       "      <td>0.403331</td>\n",
       "      <td>-0.937769</td>\n",
       "      <td>1.748450</td>\n",
       "      <td>-0.318861</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.364513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905387</td>\n",
       "      <td>-0.300610</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-0.470628</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.257331</td>\n",
       "      <td>1.259932</td>\n",
       "      <td>0.923860</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>1.573370</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.332883</td>\n",
       "      <td>-0.486519</td>\n",
       "      <td>-1.117500</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.646041</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-0.389963</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>-0.398573</td>\n",
       "      <td>-1.782989</td>\n",
       "      <td>-0.100741</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.721995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>3.895232</td>\n",
       "      <td>0.098465</td>\n",
       "      <td>-1.896898</td>\n",
       "      <td>1.039281</td>\n",
       "      <td>-1.043084</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0</td>\n",
       "      <td>2.637868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>-0.921741</td>\n",
       "      <td>-1.019656</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>2.516703</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.266122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>0.573182</td>\n",
       "      <td>-0.531228</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>1.839088</td>\n",
       "      <td>-0.108264</td>\n",
       "      <td>3.062003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.018914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       person_age  person_income  person_emp_length  loan_amnt  loan_int_rate  \\\n",
       "0       -0.755638       0.404383          -0.117198   2.836600       1.455666   \n",
       "1       -0.257331       1.127233           0.601227   0.140622       0.722635   \n",
       "2       -0.257331      -1.418731           0.403331  -0.937769       1.748450   \n",
       "3        0.905387      -0.300610           0.169270  -0.398573      -0.470628   \n",
       "4       -0.257331       1.259932           0.923860   1.039281       1.573370   \n",
       "...           ...            ...                ...        ...            ...   \n",
       "39093   -0.921741      -1.332883          -0.486519  -1.117500       0.044689   \n",
       "39094   -0.921741      -0.389963           0.601227  -0.398573      -1.782989   \n",
       "39095    3.895232       0.098465          -1.896898   1.039281      -1.043084   \n",
       "39096   -0.921741      -1.019656           0.169270   0.859550       1.425586   \n",
       "39097    0.573182      -0.531228           0.601227   1.839088      -0.108264   \n",
       "\n",
       "       loan_percent_income  cb_person_default_on_file  \\\n",
       "0                 2.189522                          0   \n",
       "1                -0.646041                          1   \n",
       "2                -0.318861                          1   \n",
       "3                -0.209801                          0   \n",
       "4                -0.100741                          1   \n",
       "...                    ...                        ...   \n",
       "39093            -0.646041                          0   \n",
       "39094            -0.100741                          0   \n",
       "39095             0.989861                          0   \n",
       "39096             2.516703                          1   \n",
       "39097             3.062003                          0   \n",
       "\n",
       "       cb_person_cred_hist_length  PERSON_HOME_OWNERSHIP_MORTGAGE  \\\n",
       "0                       -1.364513                               0   \n",
       "1                       -0.266122                               1   \n",
       "2                       -1.364513                               0   \n",
       "3                        0.620670                               0   \n",
       "4                       -0.266122                               1   \n",
       "...                           ...                             ...   \n",
       "39093                   -0.266122                               1   \n",
       "39094                   -0.721995                               1   \n",
       "39095                    2.637868                               1   \n",
       "39096                   -0.266122                               1   \n",
       "39097                    1.018914                               0   \n",
       "\n",
       "       PERSON_HOME_OWNERSHIP_OTHER  ...  LOAN_GRADE_B  LOAN_GRADE_C  \\\n",
       "0                                0  ...             0             0   \n",
       "1                                0  ...             0             1   \n",
       "2                                0  ...             0             0   \n",
       "3                                0  ...             0             0   \n",
       "4                                0  ...             0             0   \n",
       "...                            ...  ...           ...           ...   \n",
       "39093                            0  ...             1             0   \n",
       "39094                            0  ...             0             0   \n",
       "39095                            0  ...             0             0   \n",
       "39096                            0  ...             0             0   \n",
       "39097                            0  ...             1             0   \n",
       "\n",
       "       LOAN_GRADE_D  LOAN_GRADE_E  LOAN_GRADE_F  LOAN_GRADE_G  \\\n",
       "0                 0             0             1             0   \n",
       "1                 0             0             0             0   \n",
       "2                 0             1             0             0   \n",
       "3                 0             0             0             0   \n",
       "4                 1             0             0             0   \n",
       "...             ...           ...           ...           ...   \n",
       "39093             0             0             0             0   \n",
       "39094             0             0             0             0   \n",
       "39095             0             0             0             0   \n",
       "39096             1             0             0             0   \n",
       "39097             0             0             0             0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_11_17  CB_PERSON_CRED_HIST_LENGTH_18_above  \\\n",
       "0                                     0                                    0   \n",
       "1                                     0                                    0   \n",
       "2                                     0                                    0   \n",
       "3                                     0                                    0   \n",
       "4                                     0                                    0   \n",
       "...                                 ...                                  ...   \n",
       "39093                                 0                                    0   \n",
       "39094                                 0                                    0   \n",
       "39095                                 0                                    1   \n",
       "39096                                 0                                    0   \n",
       "39097                                 0                                    0   \n",
       "\n",
       "       CB_PERSON_CRED_HIST_LENGTH_5_10  CB_PERSON_CRED_HIST_LENGTH_5_below  \n",
       "0                                    0                                   1  \n",
       "1                                    0                                   1  \n",
       "2                                    0                                   1  \n",
       "3                                    1                                   0  \n",
       "4                                    0                                   1  \n",
       "...                                ...                                 ...  \n",
       "39093                                0                                   1  \n",
       "39094                                0                                   1  \n",
       "39095                                0                                   0  \n",
       "39096                                0                                   1  \n",
       "39097                                1                                   0  \n",
       "\n",
       "[39098 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = loans_test_df.loc[:, loans_test_df.columns != \"id\"]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eecb097-ae24-4014-8547-264c923b6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee2dd873-9810-4164-a2f9-c7724d5e99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_predictions_df = loans_test_df[\"id\"].copy(deep=True)\n",
    "loans_predictions_df = loans_predictions_df.to_frame()\n",
    "loans_predictions_df.insert(1, 'loan_status', y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5457c53-557c-4cfa-aed4-18d016e14eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39093</th>\n",
       "      <td>97738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39094</th>\n",
       "      <td>97739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39095</th>\n",
       "      <td>97740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39096</th>\n",
       "      <td>97741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>97742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39098 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  loan_status\n",
       "0      58645            1\n",
       "1      58646            0\n",
       "2      58647            1\n",
       "3      58648            0\n",
       "4      58649            1\n",
       "...      ...          ...\n",
       "39093  97738            0\n",
       "39094  97739            0\n",
       "39095  97740            0\n",
       "39096  97741            1\n",
       "39097  97742            1\n",
       "\n",
       "[39098 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e27851-d8eb-4444-afd2-60e7fa05de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_predictions_df.to_csv('predictions/sgd_predictions.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37dbc58-6dc2-4d5b-ab92-f0b1d72df582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
